<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何撰写高质量科技论文]]></title>
    <url>%2Fblog%2F2021%2F06%2F28%2F%E5%A6%82%E4%BD%95%E6%92%B0%E5%86%99%E9%AB%98%E8%B4%A8%E9%87%8F%E7%A7%91%E6%8A%80%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[根据刘洋教授做的报告整理 概述 确定方向→确定问题→确定思路→确定方法→实验验证→撰写论文→发表文章 图中左边是论文写作的pipeline，右边是对应的一般评审标准，包括思路新颖（novelty）、影响重大（impact）、表达清晰（clarity）等。 信息是表象，内部逻辑是骨架，传递的思想是灵魂。 写作和阅读可以看成是两个互逆过程，写作是先有一个思想，通过逻辑来组织，最后通过信息来呈现。 观念的一个转变：以作者为核心整理工作→以读者为核心阐述工作。 摘要的写作技巧几句话概括工作【起到广告作用】，避免要把所有细节都说清楚、用很专业的术语来描述、出现数学符号。 一个原则：用语要简单，让外行能看懂。 介绍的写作技巧Introduction比题目和摘要更进一步，用几段话说清楚你的工作。 要点是充分论证所作工作的必要性、重要性和紧迫性，让审稿人认同并迫不及待想往下看。 行文逻辑严密，论证充分。 常见的逻辑 说明问题是什么 简单罗列相关工作，说明当前工作的挑战 描述我们的工作，我们的工作对当前的一些挑战的处理，效果能更好 需要说清楚这个方向上的state-of-art，还没有解决的问题是什么，然后我们的工作解决这个问题。 每一句都是很重要的，不要存在废话。 在这个范例中，红色标出的是中心句论点，表达一个观点。然后分为两个方面，包括计算生物学和计算语言学，然后分别提供著名学者提供的论据，是经典的总分结构。 这是另一个范例，首先是陈述句给出观点，认为什么是重要的，然后从两个方面去论证，On one hand，On the other hand，给出一些学者的观点和数据来支撑。 此外，有时候也会加入一些衔接句，上一段和下一段有很好的连贯性。 一般写的流程 首先几句话列出这个领域很重要，这个问题很重要； 然后是在这个问题上有哪些主要的方法，当前最好效果的一些方法； 接着是这些方法有哪些挑战； 再然后是我们要解决这些挑战； 最后是我们的贡献。 大概可以分为五段，每一段中心句，然后列出响应的论据来支持中心句，段落之间需要有连贯性。 中心句（支撑句）论证严密 文中红色部分提出一个论点，下面跟出一些论据。论据基本分为两类，一类是前人的观点，第二类是实验数据。文中两类都给出了，非常有说服力的一些论据。 小技巧1-首页加图表 首先需要了解的是信息元素理解的难易程度，一般图最好理解，直观，证明最难看懂。 读者潜意识里优先选择易理解度高的信息元素。需要精心对信息元素进行布局，引导一个接收信息时走一条“舒服”路径。 文章在Introduction的右上放一个图表，讲清楚自己做的工作，这样就会产生一个信息流的变化： 看完标题，先看图表，直接能读懂自己的工作，加速审稿人对本文工作的了解。 在应用类的论文中，图和表的重要性很高，争取能让读者按照顺序看图和表就能理解论文的主要思想，而不用通过看正文才能懂。 小技巧2-直接列出自己的贡献 以前都是paper organization，讲诉文章内容下面是怎么组织的。现在很多都在讲contribution，在这里面加上节标号，相当于是一个快捷方式，便于读者找到相应的内容。 全局连贯性 提出的问题和方法需要有对应的实验验证，不要有落空，每一段都要环环相扣。 方法的写作技巧 可以先介绍背景知识（baseline）。 在第2节可以给出一个Background，在上面给出一个例子，这一块主要是在讲目前最好的方法是怎么做的。 在第3节给出我们自己的方法，同时也在上面给出一个例子，这两个例子进行对比。双面打印时，第2-3页是可以同时看到对比。 英语不好说清楚，用例子！ 看懂例子，再看我们的方法，可以更容易理解我们的方法。 这个文章中，用了图和文生成描述，使用了attention方法，发现之前的关联性。需要精心设计例子。 方法描述的逻辑顺序一个错误的描述顺序：形式化描述+解释数学符号的意义。 每个公式都有语言学意义，都来自你的直觉和想法，直接告诉审稿人，不要让去揣摩。直觉是一个high level的idea，公式是一个technique detail，先说清楚自己的想法motivation，然后再给出公式，这个顺序不能颠倒。 首先是用一个例子阐述我们的核心idea，通过这个例子，Intuitively（从直觉上）我们这么做应该会有更好的效果，最后是Formally，对于整个思路做出一个形式化的描述，给出严格的数学上的表达。 描述的准确性和形式化的能力 图中右边是修改版本，数学符号怎么使用会有很多区别，小写字母$a$是一个数，$\pmb {a}$是一个向量，$\pmb A$是一个矩阵，空心体$\mathbb{E}$表示期望。 多参考领域相关文章的形式化写法，参考模仿。 实验的写作技巧 一个注意的是：要使用公认的标准数据和最好的（state-of -the-art）系统，应用类的研究，实验很重要。 maximum是实验做的很充分，审稿人想到的和没想到的都做了实验。证明方法的优越性，需要统计性显著检验。 一般是先进行辅助实验，然后进行主实验。在测试集上，不要调参，严格来说，测试集上只能跑一次实验。 用表技巧在实验部分，会大量使用到表格。 一般是将baseline放在上面，我们的方法放在最下面。 把多个指标中，最重要的放在最右面。 对于复杂的表格，利用单双线等进行区分。 左边两列是讲实验的设置，右边列是实验结果。单线分开，MT02是在验证集上的结果，后面都是在测试集上的结果。最后给一个总体的结果，即看最后一列的结果即可。把一些好的结果加粗显示。 用图技巧 曲线上下摆放位置和右上角示意图顺序不一致。 此外，尽量在图的Caption中包含充分信息，最好能直接看懂图，不用再去看正文。 相关工作的写作技巧Related work这一部分常见问题是漏掉重要的文章，攻击前人的工作，用词注意。 正确的思路：向审稿人显示对本领域具有全面深刻的把握和梳理。 我们会说这个工作受到了哪些研究的启发，对前人的研究思想做了扩展，从哪里借鉴了一些idea。 但是一定要说明difference，每一个related work最后要讲的是我们的工作和前人的不同之处在哪。 英文写作进阶教授最后推荐了一本英文写作方面的书，面向的是英语母语者，会给出列子和改写的版本。 必须掌握的工具教授给出了一些写作方面要掌握的工具。 推荐使用MetaPost编程绘图。 平时练习写论文 研读和剖析公认的经典范文，学习写作技巧，“模拟写作”； 多做研究笔记，动笔写； 认真做好组会报告，练习和提高表达能力； 投稿过程多听取导师、同学和审稿人的意见。 对于每段话，每个图和表都找到参考例子，撰写的更加professional。 总结 写论文的本质是分享信息，呈现信息。 信息的呈现符合读者的认知惯性。让堵住快速的了解你所做的工作。 降低阅读难度，提高愉悦感。 细节决定成败。 不要本末倒置：创新至上，技法为辅。 机器翻译学术论文写作方法和技巧-刘洋教授]]></content>
      <categories>
        <category>会议报告</category>
      </categories>
      <tags>
        <tag>学术会议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础（一）——算法时间复杂度理解]]></title>
    <url>%2Fblog%2F2021%2F06%2F02%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[概述算法是独立存在的一种解决问题的方法和思路，是计算机处理信息的本质，通过一个算法来告诉计算机确切的步骤来执行一个指定的任务。 程序=数据结构+算法 算法是为解决实际问题设计；数据结构是算法需要处理问题的载体，是指数据对象中数据元素之间的关系。 以Python为例，内置的数据结构（如：list，tuple，dictionary）是不用我们自己定义的；需要我们自己去定义实现这些数据的组织方式，这些称为Python的扩展数据结构，如栈、队列。 算法特性输入：0个或多个输入 输出：至少1个或多个输出 有穷性：在有限步骤之后会自动结束而不会无限循环，并且每一个步骤可以在可接受时间内完成 确定性：算法的每一个步骤有确定的含义，不会出现二义性 可行性：算法每一步都是可行的，即每一步都能够执行有限的次数完成 时间复杂度程序的执行时间=基本的运算总数*基本的运算单位时间 基本运行单位时间：与运行的计算机环境有关 基本的运算总数是衡量程序执行时间标准 算法时间复杂度可以理解为算法的运行时间，如果算法运行时间太长，这个算法就无法使用。 时间复杂度：考虑的是基本运算总数，即假设存在函数$g$，使得算法$A$处理规模为$n$的问题所用的时间$T(n) = g(n)=O(g(n))$，则$O(g(n))$为算法$A$的渐进时间复杂度，简称为时间复杂度，记为$T(n)$。 $g(n)$表示每行代码执行次数之和，$O$表示正比例关系。 时间复杂度计算规则 1、基本操作。只有常数项，时间复杂度为$O(1)$ 2、顺序结构。时间复杂度按加法进行计算 3、循环结构。时间复杂度按乘法进行计算 4、分支结构。时间复杂度取最大值 5、判断算法的效率，往往只需要关注操作数量的最高次项 6、无特殊说明下，分析算法的时间复杂度都是指最坏时间复杂度 常见的时间复杂度量级有： 常数阶O(1) 对数阶$O(\log N)$ 线性阶O(n) 线性对数阶O(nlogN) 平方阶O(n²) 立方阶O(n³) K次方阶O(n^k) 指数阶(2^n) $$O(1)\lt O(\log n)\lt O(n) \lt O(n\log n) \lt O(n^2) \lt O(n^3) \lt O(2^n) \lt O(n!) \lt O(n^n)$$ 举例如下： 123456789int i = 1;int j = 2;++i;j++;int m = i + j;/*在执行的时候，它消耗的时候并不随着某个变量的增长而增长，那么无论这类代码有多长，即使有几万几十万行，都可以用O(1)来表示它的时间复杂度*/ 12345for(i=1;i&lt;=n;++i) // 1(n?)&#123; j=i; // n j++; // n&#125; 假设每行代码执行时间一样，都是1颗粒时间，上述代码：$T(n) = (1+2n)$ 算法的耗时是随着$n$的变化而变化，算法的时间复杂度可表示为：$T(n)=O(n)$ 123456789int i = 1;while(i&lt;n)&#123; i = i * 2;&#125;/*假设循环x次之后，i 就大于 2 了，此时这个循环就退出了，也就是说 2 的 x 次方等于 n，那么 x = log2(n),也就是说当循环 log2(n) 次以后，这个代码就结束了,代码的时间复杂度为：O(logn)*/ 1234567891011for(m=1; m&lt;n; m++)&#123; i = 1; while(i&lt;n) &#123; i = i * 2; &#125;&#125;/*将时间复杂度为O(logn)的代码循环N遍的话，那么它的时间复杂度就是 n * O(logN)，也就是了O(nlogN)。*/ 12345678910111213141516171819202122232425for(x=1; x&lt;=n; x++)&#123; for(i=1; i&lt;=n; i++) &#123; j = i; j++; &#125;&#125;/*嵌套了2层n循环，它的时间复杂度就是 O(n*n)，即 O(n²)*/for(x=1; x&lt;=m; x++)&#123; for(i=1; i&lt;=n; i++) &#123; j = i; j++; &#125;&#125;/*时间复杂度就变成了 O(m*n)O(n³)相当于三层n循环*/ 空间复杂度定义为该算法消耗的存储空间，也是问题规模为$n$的函数，如今的设备完全可以忽略空间复杂度的影响。 算法时间与空间复杂度 算法复杂度理解]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>时间复杂度</tag>
        <tag>算法基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[核能科学与技术导论]]></title>
    <url>%2Fblog%2F2021%2F03%2F31%2F%E6%A0%B8%E8%83%BD%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AF%BC%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[参考资料: ①《资源革命：如何抓住一百年来最大的商机》（美）斯蒂芬·赫克（Stefen Heck）等 1、序言1.1 能量与能源1.2 能源与人类社会1.3 什么是核能2、核能物理基础2.1 原子核性质基础2.2 核能物理基础]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>核能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（一）人工智能与专家系统概述]]></title>
    <url>%2Fblog%2F2021%2F03%2F24%2F%EF%BC%88%E4%B8%80%EF%BC%89%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%B8%93%E5%AE%B6%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[参考资料： ①《人工智能及其应用》（第五版）蔡自兴等著 ②《人工智能——一种现代的方法》（第三版）Stuart J·Russell &amp; Peter Norvig著 ③《高级专家系统——原理、设计及应用》（第二版）蔡自兴 [美]约翰·德尔金 龚涛著 ④《专家系统——原理与编程》（第四版）[美]Joseph C·Giarratano &amp; Gary D·Riley著 分为两大部分，第一部分包括逻辑、概率、数据结构、人工智能概念和其他形式的专家系统理论的内容；第二部分介绍CLIPS专家系统工具。 对AI的概述人工智能（artificial intelligence，AI）是最新兴的科学与工程领域之一，自1956年创造了“人工智能”一词算起，经过60十多年的发展，AI目前包含了大量各种各样的子领域，范围从通用领域，如学习和感知，到专门领域，如下棋，证明数学定理，诊断疾病，自动驾驶等。成为了一门广泛的交叉和前沿科学。 没有一种人工智能技术能成功解决所有问题，一些组合的方法会更有效一些。 人工智能尚无一个统一定义，人类的许多活动，如解题、讨论、编制计划、编程、驾驶等，都需要“智能”。如果机器能执行这样的任务，可以认为机器已具有某种性质的“人工智能”，它所包含的“智能”是人为制造的或由机器或计算机表现出来的一种智能（区别于自然智能）。 智能（intelligence）：人的智能是人类理解和学习事物的能力，智能是思考和理解的能力而不是本能的做事的能力。 另一种定义：智能是一种应用知识处理环境的能力或由目标准则衡量的抽象思考能力。 智能机器（intelligent machine）：一种能够呈现出人类智能行为的机器，而这种智能行为是人类用大脑考虑问题或创造思想。 另一种定义：智能机器是一种能够在不确定环境中执行各种拟人任务（anthropomorphic tasks）达到预取目标的机器。 人工智能（学科）；人工智能（学科）是计算机科学中涉及研究、设计和应用智能机器的一个分支，近期目标是在于研究用机器来模仿和执行人脑的某些智力功能，并开发相关理论和技术。（是计算机科学还是智能科学？？？有待探讨和实践） 人工智能（能力）：智能机器所执行的通常与人类智能有关的智能行为，这些智能行为涉及学习、思考、感知、理解、识别、判断、推理、证明、通信、设计、规划和问题求解等活动。 人工智能的定义人工智能一个最早的定义之一：“使计算机像人类一样思考”。该定义来源于“图灵测试（1950）”。 “图灵测试（Turing test）”：一个人试图判断和其通过键盘交谈的对象是人类还是计算机程序，如果计算机能在交谈过程中顺利回答问题而不被发现其是计算机，则说明计算机已经具有强人工智能（strong AI）。 强人工智能：基于严格逻辑基础。 弱人工智能：基于ANN、遗传算法、进化方法等。 定义1. 人工智能是一种使计算机能够思维，使机器具有智力的激动人心的尝试，字面意思：有头脑的机器（Haugeland 1985） 定义2. 与人类思维相关的活动，诸如决策、问题求解、学习等活动的自动化（Bellman 1978） “像人一样思考”——认知建模的途径 定义3. 创造执行一些功能的技艺，当由人来执行这些功能时需要智能（Kurzweil 1990） 定义4. 研究如何使计算机做那些目前人比计算机更擅长的事情（Rich &amp; Knight 1991） “像人一样行动”——图灵测试的途径 定义5. 通过使用计算机模型来研究智力（Charniak &amp; McDermott 1985） 定义6. 使感知、推理和行动成为可能的计算的研究（Winston 1992） “合理的思考”——思维法则的途径 定义7. 计算智能研究智能Agent的设计（Poole等 1998） 定义8. AI…关心人工制品的智能行为（Nilsson 1998） “合理的行动”——合理Agent的途径 Agent就是能够行动的某种东西，计算机程序在做某些事情，但期望计算机Agent做更多的事：自主的操作、感知环境、长期持续、适应变化并能创建与追求目标。合理的Agent就是为了实现最佳效果。 计算机尚需具有一下功能： 自然语言处理（natural language processing）使之能成功的交流 知识表示（knowledge representation）以存储它知道的或听到的信息 自动推理（automated reasoning）运用存储的信息来回答问题并推出新结论 机器学习（machine learning）适应新情况并检测和预测模式 计算机视觉（computer vision）感知物体 机器人学（robotics）操纵和移动对象。 人工智能起源与发展1、孕育时期（1956前） 人工智能开拓者们在数理逻辑、计算本质、控制论、信息论、自动化理论、神经网络模型和电子计算机方面的创造性贡献，奠定了AI发展的理论基础。 2、形成时期（1956-1970年） 1956年，达特茅斯会议，提出“人工智能”这一术语； 巴贝奇、图灵、冯·诺伊曼等研制的计算机本身，在机器应用成为可行之后，编程解决智力问题、数学定理和其他命题的自动证明； 1965年，“专家系统和知识工程之父”的费根鲍曼（Feigenbaum）开始研制专家系统； 1968年，第一个专家系统DENDRAL用于分析有机化合物分子结构； 1969年，召开了第一届国际人工智能联合会议（IJCAI） 3、暗淡时期（1966-1974年） 主要是存在以下三个局限性： ①知识局限性。缺乏足够的专业知识和常识 ②解法局限性。设计的程序实际上无法得到问题的解答，或者得到简单问题的解答。 ③结构局限性。用于产生智能行为的人工智能系统或程序存在一些基本结构上的严重局限。 4、知识应用时期（1970-1988年） 一些专家系统的出现，如DENDRAL、MYCIN、PROSPECTOR、CASNET、MACSYMA、ELAS等。费根鲍姆于1977年第五届国际人工智能联合会议上，正式提出了知识工程（knowledge engineering）。 在开发专家系统过程中，也认识到：人工智能系统是一个知识处理系统，而知识表示、知识利用和知识获取是AI的三个基本问题。 5、集成发展时期（1986年至今） 已有的专家系统存在缺乏常识知识，应用领域狭窄、知识获取困难、推理机制单一、未能分布处理等问题。人工智能和知识工程的一些根本问题，如交互问题、扩展问题和体系问题等，都没有得到很好解决。 1980年后期以来，机器学习、计算智能、人工神经网络和行为主义等研究深入。区别于符号主义的连接主义和行为主义的人工智能学派获得新的发展。 人工智能三大学派： （1）符号主义（symbolicism）：也称为逻辑主义（logicism）、心理学派（psycologism）或计算机学派（computerism），其原理主要为物理符号系统假设和有限合理性原理。认为认知的过程是符号操作过程，认为人是一个物理符号系统。认为AI的研究方法是功能模拟方法。 （2）连接主义（connectionism）：也称为仿生学派（bionicsism）或生理学派（physiologism），其原理主要是神经网络及神经网络间的连接机制和学习算法。认为人的思维基元是神经元。认为AI应着重于结构模拟。 （3）行为主义（actionism）：也称为进化主义（evolutionism）或控制论学派（cyberneticsism），其原理为控制论及感知-动作型控制系统。认为智能取决于感知和行动。认为AI应采用行为模拟方法。 传统人工智能（AI）：以数理逻辑为基础的符号主义，从命题逻辑到谓词逻辑再到多值逻辑，包括模糊逻辑，粗糙集理论，为AI发展做出历史性贡献，符号主义也在发展中不断寻找新的理论、方法和实现途径。 计算智能（computational intelligence, CI）：模仿人的思维、自然特征和生物行为的计算方法（如神经计算、进化计算、自然计算、免疫计算、群计算等）已被引入人工智能学科。CI弥补了传统AI的数学理论和计算的不足，更新并丰富了人工智能的理论框架。 神经网络发展突起： 1943年，麦卡洛克和皮茨提出“似脑机器”，构造一个表示大脑基本组成的神经元模型，由于硬件的局限性，使得ANN在20世纪70年代进入低潮。 1982年，Hopfield提出离散神经网络模型，1984年，又提出连续神经网络模型，促进了ANN研究的复兴。 Bryson和He提出的反向传播（BP）算法及Rumelhart和McClelland在1986年提出的并行分布处理（PDP）理论使得ANN复兴的真正推动力 1987年召开的第一届神经网络国际会议，并成立国际神经网络学会（INNS），ANN正式成为AI的一个重要子学科。 深度学习（deep learning）的研究，并在计算机视觉，自然语言处理和人机博弈等领域较为广泛应用。 超限学习（extreme leaning）是在深度学习的基础上发展而来，推动机器学习的发展。 人类智能和人工智能人的认知过程是一个复杂行为，人的心理活动具有不同的层次，可与计算机的层次相比较： 人类 计算机 生理过程 计算机硬件 初级信息处理 计算机语言 思维策略 计算机程序 研究认知的主要任务：探求高层次思维决策与初级信息处理的关系，并用计算机程序来模拟人的思维策略水平，用计算机语言模拟人的初级信息处理过程。 智能计算机系统：以人的思维方式为模型进行智能信息处理。 研究认知的目标：设计适用于特定领域的高水平智能信息处理系统。如一个具有智能信息处理能力的自动控制系统就是一个智能控制系统，可以是专家系统，或者是智能决策系统等。 人可以看成一个智能信息处理系统（也称符号操作系统）。符号就是模式（pattern）。一个完善的符号系统包括： （1）输入符号（input） （2）输出符号（output） （3）存储符号（store） （4）复制符号（copy） （5）建立符号结构：通过找到各种符号之间的关系，在符号系统中形成符号结构。 （6）条件下迁移（conditional transfer）：根据已有的符号，继续完成活动过程。 任何一个系统能表现出智能，必定能够执行上述6个功能。反之，任何系统如果具有这6种功能，能够表现出智能（人所具有的智能）。 认知生理学：研究认知行为的生理过程，主要是研究神经系统（神经元，中枢神经系统和大脑）的活动，是认知科学的研究底层。 认知心理学：研究认知行为在人体内的初级信息处理，主要研究人的认知行为如何通过初级信息自然处理，由生理活动变为心理活动及其逆过程（心理变为生理行为），是认知活动的中间层，承上启下。 认知工程学：研究认知行为的信息加工处理，主要是研究如何通过计算机为核心的人工信息处理系统，对人的各种认知行为（如知觉，思维，记忆，语言，学习、理解、推理、识别等）进行信息处理。是研究认知活动的工具。 机器智能的研究发展 智能计算机： 计算机技术日新月异，包括自学习、并行处理、启发式搜索、机器学习、智能决策等AI技术已用于博弈程序设计，使得“计算机棋手”水平大为提高。如1997年，IBM公司的深蓝（deep blue）智能计算机，2016年谷歌的alphaGo。 神经计算机（neural computer）： 能够以类似人的方式进行“思考”，力图重建人脑的形象。日本MITI报道，对神经计算机系统的可行性研究早于1989年4月底完成。神经网络热已然持续30多年。 量子计算机，光子计算机等基础研究也取得突破。 人工智能系统的分类 系统 说明 专家系统（expert system） 把专家系统技术和方法，尤其是工程控制论的反馈机制有机结合而建立，广泛应用于故障诊断、工业设计和过程控制。一般由知识库、推理、控制规则集和算法等组成。专家系统所研究一般是不确定性问题。 模糊系统 提出了一种新机制用于实现基于知识（规则）甚至语义描述的表示、推理和操作规律。模糊系统为非线性系统提出一种比较容易的设计方法，尤其是当系统包含不确定性且很难用常规非线性理论处理时。 神经网络系统 主要涉及模式识别、图像处理、自动控制、机器人、信号处理、医疗、军事、商业管理等领域 学习系统 机器学习的研究，尤其是一些新的学习方法为学习系统注入新活力 仿生系统 模仿与模拟人类和生物行为的智能系统。生物通过个体间的选择、交叉、变异来适应环境，把进化算法，尤其是遗传算法机制用于人工系统和过程，可实现仿生智能系统（bionic intelligent systems） 群智能系统 某种交互作用的组合或Agent的结构集合 多真体系统（Multiple Agent system） 并行计算和分布式处理技术（包括分布式AI）的发展，多真体系统应运而生。真体（Agent）可以看成是通过传感器感知环境，并借助执行器作用于该环境的任何事物。 混合智能系统 单一智能机制往往不能满足一些复杂、未知或动态系统的要求，需要开发混合（集成的、综合的、复合的）智能技术和方法 按应用领域分：智能机器人系统、智能决策系统、智能加工系统、智能控制系统、智能规划系统、智能交通系统、智能管理系统、智能家电系统等。 人工智能研究基本内容 分类 说明 认知建模 Houston等把认知归纳为5种类型：（1）信息处理；（2）心理上的符号运算；（3）问题求解；（4）思维；（5）诸如知觉、记忆、思考、判断、推理、想象、学习、问题求解、概念形成和语言使用等关联活动 知识表示（Knowledge representation，KB） 把人类知识概念化、形式化和模型化。一般地，运用符号知识、算法和状态图等来描述待解决问题。已提出的知识表示方法主要包括：符号表示、神经网络表示。 知识推理 让机器从一些已知判断或前提推导出一个新的判断或结论的思维过程。形式逻辑中的推理分为：演绎推理、归纳推理和类比推理。 知识应用 专家系统，以及现在的机器学习和自然语言处理应用研究，自动规划，智能控制等 机器感知 使机器具有类似于人的感觉，如视、听、力、触、嗅、痛觉，接近感和速度感。主要在计算机视觉和机器听觉上的应用有较多进展 机器思维 对传感信息和机器内部的工作信息进行有目的的处理，综合应用知识表示、知识推理、认知建模（启发式搜索和控制策略）和机器感知（人脑结构和神经网络的工作机制）等方面研究 机器学习 继专家系统后AI一个重要研究领域，是AI和神经计算的核心研究之一。使机器（计算机）具有学习新知识新技术，并能在实践中不断改进和完善的能力。 机器行为 智能系统（计算机、机器人）具有表达和行为能力。机器思维是机器行为的基础。 智能系统构建 一些能简化演绎、机器人操作和认知模型的专用程序设计以及计算机的分布式系统、并行处理、多机协同和计算机网络等的发展，有益于AI的开发。 人工智能研究和计算方法 研究方法 计算方法 功能模拟法（符号主义） 概率计算 结构模拟法（连接主义） 符号规则逻辑运算 行为模拟法（行为主义） 模糊计算 集成模拟法（取长补短） 神经计算 进化计算与免疫计算 群优化计算，蚁群算法 人工智能研究和应用领域 分类 说明 问题求解与博弈 目前AI程序能够搜索解答空间，寻找较优解答 逻辑推理和定理证明 计算智能 涉及神经计算、模糊计算、进化计算、粒子群计算、自然计算、免疫计算和人工生命等领域 分布式人工智能（DAI）与Agent DAI系统以鲁棒性为控制系统质量的标准，并具有互操作性，即不同的异构系统在快速变化的环境中具有交换信息和协同工作的能力。目标是创建一种能够描述自然系统和社会系统的精确概念模型 自动程序设计 能够以各种不同的目的的描述来编写计算机程序 专家系统 一个智能计算机程序系统，内部具有大量专家水平的某个领域知识和经验，能够利用人类专家的知识和解决问题的方法来解决该领域的问题 机器学习 自动获取新的事实及新的推理算法，使计算机具有智能的根本途径。近年也发展了基于解释的学习、基于实例的学习、基于概念的学习、基于神经网络的学习、遗传学习、增强学习、深度学习、超限学习、迁移学习以及数据挖掘和知识发现等 自然语言理解 机器人学 具有广泛的学科交叉 模式识别 利用计算机代替人类或帮助人感知模式，是对人类感知外界功能的模拟，研究的是计算机模式识别模拟，使得计算机系统具有模拟人类通过感官接受外界信息、识别和理解周围环境的感知能力 机器视觉/计算机视觉 神经网络 智能控制 同时具有以知识表示的非数学广义世界模型和以数学公式模型表示的混合控制过程，任务在于实际环境或过程进行组织（决策和规划），以实现广义问题求解 智能调度和指挥 一个典型是旅行商问题（TSP） 智能检索 系统与语言工具 最新的发展水平待补充 专家系统概述20世界60年代开始，专家系统作为一种研究工具而被开发，作为AI的一个特定部分，可以成功解决某些领域的复杂问题（包括：医疗、商业、科学、工程、制造和其它很多具有良定义问题的领域） 良定义：基本含义是人类专家可以确定解决一个问题的推理步骤，那么专家系统也可以做到。 专家系统在AI技术领域有很多非常成熟的应用，如遗传算法、ANN等和专家系统结合。 智能系统（intelligent system）：使用了人工智能的系统。 领域（domain）：规划的要解决问题的范围。是解决任何问题的第一步。 人工智能和人工生命：从计算机角度来说，生命就如同软件一样，从29世纪90年代开始，可以成功克隆动物，改进生命体，这些是自然界所没有的，是人工生命。但同时这些生物是有智能的，也就具有了人工智能。只是不是通过计算机的形式表现的。 创造进化系统（creative evolutionary system）：人工生命的拓展领域，该系统中人工生命系统可以根据进化压力改变自己的程序，如遗传算法。 智能的一个定义（de Silva）：学习、获取、适应、修正和扩展知识以便解决问题的能力。通过机器、工厂、工具和其他硬件建立能和现实世界交互的智能机器。其中的挑战是把现实世界中复杂的人类思维融合到机器中，包括：歧义、含糊、一般、不精确、不确定、模糊、信任、似然等。 同时具有人工智能和意识的系统：对于人脑工作原理的了解。 专家系统（expert system）定义专家系统的开拓者费根鲍姆1982年对专家系统的定义： Expert system is “ an intelligent computer program that uses knowledge and inference procedures to solve problems that are different enough to require significant human expertise for their solutions.” That is , an expert system is a computer system tha emulates the decision-making ability of a human expert. The term emulate means that the expert system is intended to act in all respects like a human expert. 专家系统（expert system）：是对传统人工智能问题中智能程序设计一个非常成功的近似解决办法。Stanford的Edward Feigenbaum教授定义为：“一种智能的计算机程序，运用知识和推理过程来解决只有专家才能解决的复杂问题。” 专家系统是一种模拟专家决策能力的计算机系统，大量利用专业知识以解决只有专家才能解决的问题，模拟（emulate）比模仿（simulation）更进一步。 20世纪80年代中期，专家系统一直由基于规则的系统主宰；80年代后期，开始向面向对象的系统转变。 明斯基提出基于框架（frame）的专家系统，具有更容易表示描述性和行为性对象信息的能力以及一套强有力的表工具，能够处理比基于规则的专家系统更复杂的问题。 1982年Hopfield提出Hopfield模型神经系统，第一次建立了动态稳定网络中存储信息的原理；BP算法的发展，成为训练神经网络最流行的学习算法，是解决实际数据分类的有效工具。 1965年扎德提出模糊逻辑，20世纪70年代，曼达尼和阿斯里安建立第一个模糊专家系统，以控制蒸汽机。 20世纪80年代后期，已有的专家系统存在缺乏常识知识、应用领域狭窄、知识获取困难、推理机制单一、不能分布处理等问题。对存在的这些问题探讨和基本观点的争论，有助于人工智能摆脱困境，迎来新的发展机遇。 专家系统分类 问题求解类型 描述 控制 管理系统行为以满足要求 设计 按约束配置对象 诊断 从观察到的现象推断出系统的故障 教学 诊断、调试并修复 解释 从数据推断出现状描述 监视 把观察的对象与期望进行比较 规划 设计行为 预测 推断给定的情况下的可能结论 调试 推荐系统故障的解决方案 筛选 从一组可能性中挑选最好的选择 仿真 对系统组件之间的交互进行建模 决策 为用户各种请求寻找最佳的实施方案 专家系统结构与特点专家系统技术包括：专门的专家系统语言、程序、辅助专家系统开发和执行而设计的硬件。 专家系统内部包括两个部分：知识库（knowledge base）和推理机（inference engine）。知识库包含有让推理机得出结论而使用的专家知识。这些结论是专家系统对用户询问的响应。 专家知识是指特定问题域（problem domain）方面的知识（与通用问题求解技术方面知识不同），解决特定问题的专家知识称为专家的知识域（knowledge domain）。 知识库：专家系统内包含领域知识的部分 工作内存：专家系统内包含执行任务时发现的问题事实的部分 推理机：知识处理器，将工作内存中的事实和知识库的领域知识相匹配，以得出问题的结论。推理机处理工作内存中的事实和知识库中的领域知识，以提取新信息。它搜寻约定在工作内存里信息之间的匹配，当找到匹配时，就把规则的结论加入工作内存中，并继续扫描，寻求新的匹配。 专家系统主要组成部分归纳如下： （1）知识库（knowledge base，KB） 用于存储某领域专家系统的专门知识，包括事实、可行操作与规则等。需要解决知识获取和知识表示问题。 知识获取：知识工程师（knowledge engineer）如何从专家那里获得专门知识的问题 知识表示：解决如何用计算机能够理解的形式表达和存储知识的问题 （2）综合数据库（global database） 用于存储领域或问题的初始数据和推理过程中得到的中间数据（信息），即被处理对象的一些当前事实。 （3）推理机（reasoning machine） 用于记忆所采用的规则和控制策略的程序，使得整个专家系统能够以逻辑方式协调地工作。推理机能够根据知识进行推理和导出结论。 （4）解释器（interpreter） 能够向用户解释专家系统行为，包括解释推理结论的正确性以及系统输出其他候选解的原因 （5）接口（interface） 即界面，能够使系统与用户进行对话，使用户能够输入必要的数据、提出问题和了解推理过程及推理结果等。系统可以通过接口，回答用户，进行必要解释。 专家系统的特点 特点 描述 符号推理 按符号形式表示知识，运用专家的知识与经验推理、判断和决策 启发式推理 擅长提取经验，凭经验形成对问题的实际理解，并把经验运用于启发信息中 透明性 能够解释本身的推理过程和回答用户提出的问题，让了解推理过程，提高信赖感 灵活性 能不断增、改、更新知识 知识与控制分离 把系统知识与其控制分离，知识库和推理机是专家系统独立的模块 允许非精确推理 这类应用特征有不确定、模糊或不可行的信息和本来就不精确的领域知识 擅长复杂推理 会犯错误 专家系统和传统程序的比较 传统程序 专家系统 数字式 符号表示 算法式 启发式 信息与控制集成 知识与控制分离 难于修改 易于修改 精确信息 不确定信息 指令界面 带解释自然对话 给出最终结果 解释性建议 最优解 可接受解 专家系统特点：高性能（给出建议的质量要高）、适当的响应时间（时间限制问题）、可靠性（不易崩溃）、可解释性（系统能解释推理步骤，而不是黑盒子）、灵活性（增加、修改、删除知识的高效机制）、列出所有支持和反对某个假设的原因、列出所有可解释观测证据的假设、解释假设的所有推断结果、给出当假设是正确时将发生的一个预测（prognosis）、提供程序需要用户进一步信息的问题的依据、提供程序所用知识正确的依据。 专家系统的主要优点：解释、说明性，系统稳定，可靠性强。 专家系统构建步骤专家系统的开发一般步骤：知识工程师通过与专家进行对话获取专家知识（类似于传统程序设计人员与用户讨论需求），然后知识工程师将知识编码到知识库中，随后专家评估系统并返回意见给知识工程师。推理机不停循环工作，直到某种执行终止的判定标准出现。 （1）设计初是数据库。包括 ①问题知识化：辨别所研究问题的实质，如要解决什么问题，如何定义该问题，可否能分解为子问题或子任务，包含哪些典型数据等。 ②知识概念化：概括知识表示所需要的关键概念及其关系，如数据类型、已知条件（状态）和目标（状态）、提出的假设以及控制策略等。 ③概念形式化：确定用来组织知识的数据结构形式，应用AI中各种知识表示方法把关键概念、子问题及信息流特性等转换为比较正式的表达，包括假设空间、过程模型和数据特性等。 ④形式规则化：编制规则，把形式化的知识变换为由编程语言表示的可供计算机执行的语句和程序 ⑤规则合法化：确定规则化的知识的合理性，检验规则的有效性。 （2）原型机（prototype）的开发与试验 选定知识表示方法后，着手建立整个系统所需要的实验子集，包括整个模型的典型知识，且只涉及与试验有关的足够简单的任务和推理过程 （3）知识库的改进和归纳 反复对知识库及推理规则进行改进试验，归纳出更完善的结果，经过一段时间后，使得系统在一定范围内达到人类专家的水平 专家系统问题求解通常没有算法求解，而是依靠推理获得一个合理的解决方法。算法是一种理想解决方案，它会在有限时间内给出答案。专家系统依赖于推理，必须能解释这个过程，推理过程是可以检查的。解释机（explanation facility）是复杂专家系统的一个必要部分。 专家系统的局限性专家系统一个不足是知识的局限性：当问题达到未知界限（limits of ignorance）时，专家系统会给建议打上一个折扣。专家系统缺乏因果知识（causal knowledge），它不能真正理解系统中隐含的原因和结果。用基于经验和启发性的浅（shallow）知识设计专家系统比用基于对象的基本结构、功能和行为的深（deep）知识容易些。 启发性知识（heuristic knowledge）：一种从实践中获得经验性知识，可以对问题的求解有帮助作用，但不能保证一定有效。启发性知识能够提供有价值的捷径，可以帮助减少时间和花费。 专家系统的另一个不足是知识受限于系统的知识域：典型的专家系统不能像人类一样，通过类比（analogy）来一般化知识以获得求解新的问题的方法。专家系统可以通过规则的归纳，获得少量新知识，但是把人类知识转为专家系统过程中存在一个知识获取瓶颈（knowledge acquisition bottleneck），限制专家系统的创建。 人在专家系统中的作用主要作用：领域专家、知识工程师和终端用户 （1）领域专家（domain expert）：具有以超越他人的方式求解特定问题的知识和技能的人。是专家系统的知识来源。 （2）知识工程师：主要职责是获取知识、处理知识，并对知识进行编码 （3）终端用户（end-user）：应用专家系统进行工作的人员，主要职责包括：规定接口技术规范、辅助知识获取等。 专家系统语言IF-THEN规则：Newell和simon证明的一个重要结果：大部分人类问题求解或认知（cognition）可以用IF-THEN类型的产生式规则（production rule）表达。与一个小的、模块化的知识集相对应的规则称为一块（chunk），块以松散的形式连接、组织，并与相关的知识有联系。人类的记忆是都以块的形式组织。 现代基于规则的专家系统基础：Newell和Simon把人类问题求解的模型归纳为：长期记忆（规则）、短期记忆（工作内存）、认知处理器（推理机）。 设计专家系统的过程中，一个重要的因素是知识的数量和规则的粒度（granularity）。粒度太小，则在没有其他规则参考的情况下很难理解规则，粒度太大，专家系统很难修改。 专家系统基本是为符号推理而设计的，虽然一些经典的人工智能语言LISP和PROLOG也可用作符号操作的语言，但它们的应用并不局限于专家系统外壳。很多专家系统是用PROLOG和LISP建造的。PROLOG用于诊断系统有很多优点（内含反向链推理）。使用专门为专家系统建造而设计的外壳和实用程序来建造大型专家系统更为方便，效率高，不必从头做起。 专家系统语言：和SQL一样，专家系统语言是一种比LISP或C语言层次更高的语言，但能解决的问题的范围更小一些，使得适合专家系统而不适合一般的编程。甚至有时候需要从专家系统语言中暂时退出以便去执行过程语言的一个函数。（LISP特意设计了这一种转换） 过程语言：提供一种灵活的、健壮的技术表示数据。如线性表、记录、链表、队列和树的数据结构很容易被创建并熟练操作。现代语言如Java和C#等，通过面向对象、方法和软件包的形式更好的辅助数据抽象。数据和操纵它的方法紧密交织在对象中。 专家系统语言：提供灵活、健壮的方法去表示知识。允许有两个抽象层，数据抽象和知识抽象（knowledge abstraction），专家系统语言把数据和操作数据的方法分离。如在一个基于规则的专家系统语言中，事实（数据抽象）和规则（知识抽象）是分离的。 专家系统语言的选择：CLIPS是一种程序规模小，在实时响应要求严格时执行速度快，是入门较好选择。 语言：按照某种特定语法书写命令的编译器。专家系统会提供一个推理机去执行该语言的语句。推理机会提供正向链、反向链或者两者都提供。 工具：语言加上有关的实用程序，以使应用程序的开发、调试、交付更加方便。实用程序可能包括文本图形编译器、调试器、文件管理器、代码生成器等。 外壳（shell）：一种专门工具，为某些类型的应用而设计。在这些应用中用户仅仅提供所需的知识库。一个典例是EMYCIN外壳，是通过去除MYCIN专家系统中的医学知识库而建立的。 基于规则的专家系统产生式系统 基于规则的专家系统是以多个规则的方式说明在不同的情况下有什么样的结果，而不是采用静态的断言来表达知识。 一个基于规则的系统包括：IF…THEN规则、事实、解释器。解释器根据工作记忆中的事实来决定哪条规则被激活。 基于规则的系统分为：正向链、反向链。 规则的流行原因： （1）模块化特征。规则使得知识容易封装并不断扩充 （2）解释机制。规则容易建立解释机，一个规则的前件指明了激活这个规则的条件，追踪已触发的规则，解释机可以得到某个结论的推理链。 （3）类似人类认知的过程。IF-THEN便于解释知识的结构。 Post产生式系统 Post基本思想：任何数学或逻辑系统都只是一组特定规则，用来将一组符号转换为另一组符号。提供一个输入（前件），产生式规则能够产生一个新的输出串（结果）。 产生式系统字符串的使用是基于语法结构而非语义，不必让产生式系统懂其中含义，人知道在现实世界里这些字符串所代表的含义，但一个产生式系统只需要将一个字符串转换为另一个字符串。 Post产生式系统根本缺陷：缺乏控制策略（control strategy）来指导规则的应用。 马尔可夫算法 Markov给出了一个产生式系统的控制结构。Markov algorithm就是一组按优先级排序的产生式顺序作用于输入串。若最高优先级的规则不适用，则尝试次高优先权的规则，如此类推。 Rete算法 Markov算法对具有大量规则的系统来说是低效的。效率问题的解决办法之一是Rete算法（Rete algorithm）。Rete算法通过在网络上存储规则信息来提高反应和规则激发速度。Rete算法是一个动态的数据存储结构，类似标准的B+树，可以自发优化查询。 Rete算法是一个非常快速的模式匹配器，通过把有关规则的信息在存储器中以网络的形式存放来获取速度。Rete算法通过限制一个规则激发后重新计算冲突集合的耗费来提高正向链规则系统的速度。缺点是需要较多存储空间（现已不存问题） 推理机在每个识别动作循环中，Rete算法不是用事实去匹配每一个规则，而是仅仅考察哪些有变化的匹配，没有变化的匹配可忽略，大大提高了事实与前件的匹配速度。 快速匹配算法，如Rete算法等奠定了专家系统走向实际应用的基础。 专家系统可以认为是一个说明性程序设计，程序员不必要详细说明如何完成目标的具体算法。如在一个基于规则的专家系统中，任何一个规则，只要其左部与事实匹配，那就可以被激发并加入到议程中。规则的顺序不影响其激活。 专家系统经常用于处理不确定性问题，推理是处理不确定最好的工具之一。 使用人工神经网络建立专家系统是可能的。ANS（artificial neural system）可以作为一个知识库，该知识库通过训练有关案例来构建。 专家系统根据训练得到的疾病症状来分类疾病，推理机也称为MACIE（matrix controlled inference engine，阵列控制推理机）是用ANS知识库设计的，系统用正向链的方式来进行推理，用反向链的方式来询问用户所需要的数据。虽然ANS自身不能解释权值是如何设置的，但是MACIE能解释ANS并生成IF…THEN型规则来解释它的知识。 专家系统知识表示概述知识表示（knowledge representation，KR）被看作是人工智能的核心，类似数据库设计，利用数据挖掘技术提取知识。 数据挖掘使用存储在数据仓库（data warehouse）中的档案数据（archival data）来预测未来趋势。 KR在专家系统中重要的原因：一是专家系统是专为某一类基于逻辑规则（推理）的知识表示设计的。二是影响着专家系统的开发、效率、速度和维护，如同程序设计中对数据结构的选择。 在CLIPS中，KR可以是规则、自定义模板、对象和事实。 关于知识的分类，其中有两个特别的类别：先验知识（priori）和后验知识（posteriori）。 先验知识不依赖于源于感官所获得的知识，被认为是普遍正确的，没有反例。 后验知识是由感官所获得的知识，正确与否可以用感觉经验来证明。但感官所得不是一直可信，后验知识可以在新知识的基础上被否定。 知识表示技术：包括产生式规则、语义网、框架、脚本、逻辑、概念组等，以及KL-1知识表示语言（包括其模型基础发展CLASSIC），可视化语言等。 产生式规则常被作为知识库用在专家系统中。定义产生式的一种形式方法是Backus-Naur范式（BNF）。这种方法是一种定义语法的元语言（metalanguage），语法（syntax）定义了形式，语义（semantics）指出含义。 语义网络（semantic network）：用于表示命题信息的一种经典的人工智能表示技术。 对象-属性-值三元组（OAV，objected-attribute-value triple）可以用来表示语义网中的所有知识。在MYCIN专家系统使用诊断传染病。 语义网很容易翻译成PROLOG语句。PROLOG是一种带有可执行语句的计算机语言。 语义网的难点：①缺乏连接命名标准，难以理解语义网的设计意图以及是否是以一种一致的方法来设计；结点的命名会存在歧义。语义网表示确切知识（definitive knowledge），知识是能够定义的，必须严格定义连接和结点名字。（XML可扩展标记语言和本体论解决这个问题，XML提供一种标准的方法把形式语义编码进任何语言方面。）②搜索结点时的组合爆炸，尤其是查询的回答是否定的时候。对一个产生否定结点的查询，可能要在语义网络中搜索很多或所有的连接。 语义网络是一种浅知识结构（shallow knowledge structure），语义网的所有知识都包含在连接和结点中。 模式（schema）：用来描述比语义更复杂的知识结构。是一种深知识结构，可表示因果关系，解释一些事情发生的原因。通常模式有关于结点的内在结构信息而语义网没有。 框架（frame）：提供一个方便的结构表示在特定环境中的典型对象。特别地，框架适合模拟常识。 逻辑（logic）：表示知识。逻辑主要研究规则的精确推理，推理主要是从假设中推出结论。运用计算机进行推理出现了逻辑程序设计（logic programming）和基于逻辑的语言开发，如PROLOG。 专家系统推理方法概述专家系统广泛地应用于没有合适的算法或无算法的情况下，特别是在信息不完整或缺乏的情况下，专家系统应该像人类一样能够从一系列的推理中得到结论。 一个简单的使用算法的计算机程序在缺乏参数的情况下是不能够得到问题的解的。当无法找到最优解的时候，专家系统能能够像人类一样做出最好的猜测。有比没有好的原则。 专家系统推理策略：常用的正向链（forward chaining）和反向链（background chaining）。一些特殊需要使用：手段—目的分析、问题简化、回溯、规则-生成-测试、逐级规划和最小满足原则、约束处理。 正向链：从事实到结论的推理。从已知的初始事实出发，通过使用规则得到新的结论或做出某种行动。以数据驱动为主。 反向链：从假设（要证明的结论）到事实的推理。从某个假设或要证明的目标出发，不断寻找能使假设成立的规则。过程中可能产生新的子目标便于把大问题分解成更容易证明的小问题。以目标驱动。 OPSS和CLIPS是正向链推理，PROLOG是反向链推理，Eclipse两者都提供。 推理机的选择取决于问题的类型，诊断问题最好是反向链推理；预测、监视、控制最好使用正向链推理。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[理解Fourier变换（一）]]></title>
    <url>%2Fblog%2F2020%2F11%2F10%2F%E7%90%86%E8%A7%A3Fourier%E5%8F%98%E6%8D%A2%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[欧拉公式与旋转欧拉公式：$$e^{\pi i} +1 =0 \tag 1$$ $e^{\pi i}$表示在单位圆上逆时针旋转180°的变换。 公式里面涉及到指数函数（Exponentiation）。指数函数是加法和乘法运算的桥梁，在自变量包含复数时表示旋转。 结合泰勒公式，欧拉公式会有一个十分优雅的变形：$$e^{x}=1+x+\frac{1}{2!}x^2 +\frac{1}{3!}x^3 +\dots \tag 2 \\sin(x) = x-\frac{1}{3!} x^3 +\frac{1}{5!} x^5 +\dots \ cos(x) = x-\frac{1}{2!} x^2 +\frac{1}{4!} x^4 +\dots$$令$x=i\theta$，$\theta$为自变量，则(2)式变为：$$\begin{align}e^{i\theta}&amp;= 1+i\theta +\frac{1}{2!}(i\theta)^2 +\dots \ &amp;=(1-\frac{\theta^2}{2!}+\frac{\theta^4}{4!}+\dots) + i(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}+\dots) \\&amp;= cos(\theta)+isin(\theta)\tag{3}\end{align}$$ 纵坐标自带虚数单位$i$（复平面），$sin(\theta)$为纵坐标（自带虚数单位），$cos(\theta)$为横坐标。 $e^{i\theta}$表示一个圆心在原点，半径为1的单位圆，图中$\alpha$即为（3）式中的$\theta$。 $e^{i\theta}$等价于一种旋转，$\theta$为旋转角的度数（弧度制），$\theta = 2\pi$即是单位圆。 数字本身有两种表达方式（操作），加法和乘法。加法对应数轴的平移变换（一个操作），乘法对应数轴的伸缩变换（一个变换）。 还有一个旋转操作，旋转是沿着一个圆弧运动的过程，一个单位$i$表示旋转$90°$。如果$y$轴自带虚数单位，如$i,2i,3i,\dots,$就有了旋转操作，$y轴$的平移就可以表示旋转。 指数函数（Exponentiation）指数函数的一个重要特性：加法变乘法，即$a^{x+y} = a^x \times a^y$。 可以通过指数函数，做到使用平移变换来描述伸缩变换。 图中上面数轴，表示的是平移变换-1（右移一个单位）和+2（左移两个单位），图中下面数轴将上面这两个数作为输入，代入指数函数$f(x) = 2^x$，函数输出值就是两次伸缩变换（乘法），一次是收缩为原来的$\frac{1}{2}$，另一次是拉伸到原来的$2^2=4$倍。 复平面（Complex Plane）构造复平面，把虚数单位$i$加到纵轴上，同时拥有了伸缩和旋转。 横坐标红线表示，横向平移映射到伸缩操作的可视化 纵坐标虚数单位，纵向平移映射到旋转操作的可视化，正为逆时针旋转。 把底数换成$e$，更加方便表示圆的概念。 每走一个单位的纵向位移，在圆周上旋转的圆弧长度为1，$e^{\pi i}$正好代表逆时针旋转180°，并且落在位置为（-1，0），这也就是欧拉公式的几何直观可视化。 $e^x$在复平面，或者说$x=ai$（$a$为某个常数，即弧度制的圆周长度）表示：旋转 傅里叶变换 傅里叶变换（不加限定，表示连续傅里叶变换） 傅里叶级数 傅里叶变换还包括：离散时间傅里叶变换、离散傅里叶变换、傅里叶逆变换、快速傅里叶变换等。还有进一步演化的拉普拉斯变换、小波变换、$z$变换等。 公式表示$$\hat{f}(\xi) = \int_{-\infty}^{+\infty} f(x) e^{-2\pi i x \xi}dx \quad \xi 为任意实数 \tag{4}$$ 一般，$\hat{f}(\xi)$写成$F(w)$或$F(f)$表示角速度或者频率，后面量纲对应修改；自变量$x$一般写成$t$表示时间。 声音表示一个标准音A（下图黄色表示），频率为440 Hz，下图紫红色表示信号频率为294 Hz，两个信号所有时间点振幅叠加得到两个音同时发出的信号时域图。 给一段任意时间的气压曲线，找到其中原有的组成音符。 可视化方法 假设有一个每秒3拍子的声音信号，Intensity为强度，只取前面4.5秒（图像中画出来部分）。将黄色曲线缠绕到一个圆上，大小是原本信号的强度（可以理解为振幅）。 圆周围的图像由白色箭头绘制而成，速度可变，0.50 cycles/second表示白色箭头每秒钟转过半圆。 此时有两个频率在起作用，一个是信号频率：3拍/秒，固定值。另一个是白色箭头绘制缠绕中心圆图像的频率，图中是0.5圈/秒，是一个变量，不同缠绕频率变化时，中心圆周边绘制的曲线可视化表现为： 注意到：当绕单位圆的白色箭头滑动速度为3拍/秒（和原始声音信号3拍/秒相同时），会出现一个稳定图像，该图像记录了原始信号幅值变化并且每一圈都相同（周期性）。 质心记录法上面绕单位圆以不同速度绘制周边曲线的可视化图中，自变量输入是可变化的转圈速度，可以看成$f(x)$中的$x$。 输出的圆圈图像基本关于原点对称，当稳定时，是“头重脚轻”的，描述“头重脚轻”的最好方法是用【质心】（质心描述了物体的空间分布特征）。取质心的横坐标来表示质心的特征。 注意到一点：在横坐标等于0处有一个很大的值，原因是原来的图像没有关于横轴对称，有一个偏移量。 得到的新图像的横坐标是【频率（Frequency）】，即缠绕圆圈的记录速度。频率可以抽象理解为围着圆圈跑的速度。 该工具应用到两个声音的组合图像中： 公式推导第一步：旋转表示 旋转的桥梁是复平面，背后原理是指数函数结合泰勒公式： $\pi$单位的$e^{6.28i}$表示一个单位圆旋转一圈，$e^{2\pi i t}$表示一秒钟一圈的旋转方程，引入$f$频率，控制旋转的速度，$f=\frac{1}{10}$，合起来表示一秒钟十分之一圈。 第二步：缠绕的表示 依据下面动图，我们规定旋转是顺时针的（为了统一标准，也考虑到简洁和便于计算），所以先加一个符号。假设原来函数是$g(t)$，两者相乘就能得到缠绕图像，$g(t)e^{-2\pi i f t}$。 第三步：质心表示 以正方形的质心为例，在四周边框取$N$个等距离分布的点，计算这几个点的位置的平均值，推广到一般情况下： 图中，紫红色的点为采样点，得到：$\frac{1}{N}\sum\limits_{k=1}^{N}g(t_k)e^{-2\pi i f t_k}$ 随着采样点的增加，使用积分表示，可以得到：$$\frac{1}{t_2-t_1} \int_{t_1}^{t_2} g(t)e^{-2\pi i ft_k}dt \tag{5}$$常数项$\frac{1}{t_2-t_1}$值越大，表示质心位置偏移越大，去掉该常数项系数，几何直观动图（红色箭头为去掉系数后的常数表示）绘制的图像更集中在对应频率的附近，也就是说在对应的频率位置的值更大。 一般傅里叶变换公式的上下限是正负无穷，其几何直观表示： 其实，信号从开始到结束做傅里叶变换，对应于傅里叶变换积分上下限从负无穷到正无穷。 相位 其中红色部分为质心，长度为振幅大小，对应的角度$\theta$表示相位 原信号长度假设信号时长很长，那么缠绕圆上的线更多，每次接近稳定图像质心的变化速度更快（即频率图像更加密集） 同理，当信号时长较短时候，频率图像会更加稀疏，缠绕圆上的线更少，质心变化的速度相应变慢。 总结 $e^{2\pi i}$表示单位圆，添加自变量即表示旋转，乘以原函数$g(t)$即缠绕到单元圆上。 利用积分计算来求取质心的特征 一个逆时针旋转360°画成的圆→$e^{2\pi i}$ 表示运动，需要原函数的自变量，时间$t\rightarrow e^{2\pi it}$ 表示旋转速度，需要自变量，频率$f\rightarrow e^{2\pi ift}$ 规定变换的采样方向为顺时针，加负号$\rightarrow e^{-2\pi ift}$ 乘以原函数缠绕到单位圆并记录$\rightarrow g(t)e^{-2\pi ift}$（$g$标识原函数，与$f$区分） 计算质心特征，积分$\rightarrow \int_{-\infty}^{+\infty}g(t)e^{-2\pi ift}dt$ 自变量为频率$f$，得到函数表达式$\rightarrow \hat{g}(f) = \int_{-\infty}^{+\infty}g(t)e^{-2\pi ift}dt$ 参考资料直观详解傅里叶变换 3Blue1Brown形象展示傅里叶变换-bilibili.com]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>信号处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机科学和人工智能专业课程整理]]></title>
    <url>%2Fblog%2F2020%2F07%2F15%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[计算机科学该部分内容引用来源如下所述： 本文档是对TeachYourselfCS内容的中文翻译，原作者为Ozan Onay和Myles Byrne。 This document is a Chinese translation of TeachYourselfCS, which is written by Ozan Onay and Myles Byrne. For more information about this translation, please refer to the end of this document. 课程索引 科目 为何要学？ 最佳书籍 最佳视频 编程 不要做一个“永远没彻底搞懂”诸如递归等概念的程序员。 _《计算机程序的构造和解释》_ Brian Harvey’s Berkeley CS 61A 计算机架构 如果你对于计算机如何工作没有具体的概念，那么你所做出的所有高级抽象都是空中楼阁。 _《深入理解计算机系统》_ Berkeley CS 61C 算法与数据结构 如果你不懂得如何使用栈、队列、树、图等常见数据结构，遇到有难度的问题时，你将束手无策。 _《算法设计手册》_ Steven Skiena’s lectures 数学知识 计算机科学基本上是应用数学的一个“跑偏的”分支，因此学习数学将会给你带来竞争优势。 _《计算机科学中的数学》_ Tom Leighton’s MIT 6.042J 操作系统 你所写的代码，基本上都由操作系统来运行，因此你应当了解其运作的原理。 _《操作系统导论》_ Berkeley CS 162 计算机网络 互联网已然势不可挡：理解工作原理才能解锁全部潜力。 _《计算机网络：自顶向下方法》_ Stanford CS 144 数据库 对于多数重要程序，数据是其核心，然而很少人理解数据库系统的工作原理。 _《Readings in Database Systems》 （暂无中译本）_ Joe Hellerstein’s Berkeley CS 186 编程语言与编译器 若你懂得编程语言和编译器如何工作，你就能写出更好的代码，更轻松地学习新的编程语言。 _《Crafting Interpreters》_ Alex Aiken’s course on Lagunita 分布式系统 如今，_多数_ 系统都是分布式的。 _《数据密集型应用系统设计》_ MIT 6.824 如果花几年时间自学 9 门科目让人望而却步，我们建议你只专注于两本书：_《深入理解计算机系统》_ 和 _《数据密集型应用系统设计》_。根据我们的经验，投入到这两本书的时间可以获得极高的回报率，特别适合从事网络应用开发的自学工程师。这两本书也可以作为上面表格中其他科目的纲领。 分科目指引编程大多数计算机专业本科教学以程序设计“导论”作为开始。这类课程的最佳版本不仅能满足初学者的需要，还适用于那些在初学编程阶段遗漏了某些有益的概念和程序设计模式的人。 对于这部分内容，我们的标准推荐是这部经典著作：《计算机程序的构造和解释》。在网络上，这本书既可供免费阅读（英文版），也作为MIT的免费视频课程。不过尽管这些视频课程很不错，我们对于视频课程的推荐实际上是Brian Harvey 开设的 SICP 课程（即 Berkeley 的 61A 课程）。比起MIT的课程，它更加完善，更适用于初学者。 我们建议至少学完SICP的前三章，并完成配套的习题。如果需要额外的练习，可以去解决一些小的程序设计问题，比如exercism。 中文翻译新增： 关于SICP国内视频观看地址 MIT的免费视频课程（中英字幕） Brian Harvey 开设的 SICP 课程（英文字幕） Scheme 学习的相关资源参见：https://github.com/DeathKing/Learning-SICP 更详细的补充说明：#3 自从 2016 年首次发布这份指南以来，最常被问到的一个问题是，我们是否推荐由 John DeNero 讲授的更新的 CS 61A 课程，以及配套的书籍 _《Composing Programs》_，这本书“继承自 SICP” 但使用 Python 讲解。我们认为 DeNero 的课程也很不错，有的学生可能更喜欢，但我们还是建议把 SICP, Scheme 和 Brian Harvey 的视频课程作为首选。 为什么这么说呢？因为 SICP 是独一无二的，它可以——至少很有可能——改变你对计算机和编程的基本认识。不是每个人都有这样的体验。有的人讨厌这本书，有的人看了前几页就放弃了。但潜在的回报让它值得一读。 如果你觉得SICP过于难，试试 _《Composing Programs》_。如果还是不合适，那我们推荐 _《程序设计方法》（中文版，英文版）_ ；如果你觉得SICP过于简单，那我们推荐 _《Concepts, Techniques, and Models of Computer Programming》_ 。如果读这些书让你觉得没有收获，也行你应该先学习其他科目，一两年后再重新审视编程的理念。 新版原文删除了对 _《Concepts, Techniques, and Models of Computer Programming》_ 一书的推荐，但这本书对各种编程模型有深入的见解，值得一读。所以译文中依然保留。— 译者注 最后，有一点要说明的是：本指南不适用于完全不懂编程的新手。我们假定你是一个没有计算机专业背景的程序员，希望填补一些知识空白。事实上，我们把“编程”章节包括进来只是提醒你还有更多知识需要学习。对于那些从来没有学过编程，但又想学的人来说，这份指南更合适。 计算机架构计算机架构——有时候又被称为“计算机系统”或者“计算机组成”——是了解软件底层的的重要视角。根据我们的经验，这是自学的软件工程师最容易忽视的领域。 我们最喜欢的入门书是 _《深入理解计算机系统》_。典型的计算机体系结构导论课程会涵盖本书的 1 - 6 章。 我们喜爱《深入理解计算机系统》，因为它的实用性，并且站在程序员的视角。虽然计算机体系结构的内容比本书所涉及的内容多得多，但对于那些想了解计算机系统以求编写更快、更高效、更可靠的软件的人来说，这本书是很好的起点。 对于那些既想了解这个主题又想兼顾硬件和软件的知识的人来说，我们推荐 _《计算机系统要素》_，又名“从与非门到俄罗斯方块”（“Nand2Tetris”），这本书规模宏大，让读者对计算机内的所有部分如何协同工作有完全的认识。这本书的每一章节对应如何构建计算机整体系统中的一小部分，从用HDL（硬件描述语言）写基本的逻辑门电路出发，途经CPU和汇编，最终抵达诸如俄罗斯方块这般规模的应用程序。 我们推荐把此书的前六章读完，并完成对应的项目练习。这么做，你将更加深入地理解，计算机架构和运行其上的软件之间的关系。 这本书的前半部分（包括所有对应的项目）均可从Nand2Tetris的网站上免费获得。同时，在Coursera上，这是一门视频课程。 为了追求简洁和紧凑，这本书牺牲了内容上的深度。尤其值得注意的是，流水线和存储层次结构是现代计算机架构中极其重要的两个概念，然而这本书对此几乎毫无涉及。 当你掌握了Nand2Tetris的内容后，我们推荐要么回到《深入理解计算机系统》，或者考虑Patterson和Hennessy二人所著的 _《计算机组成与设计》_，一本优秀的经典著作。这本书中的不同章节重要程度不一，因此我们建议根据Berkeley的CS61C课程 “计算机架构中的伟大思想”来着重阅读一些章节。这门课的笔记和实验在网络上可以免费获得，并且在互联网档案中有这门课程的过往资料。 硬件是平台。 — Mike Acton, Engine Director at Insomniac Games(观看他在CppCon上的演说) 算法与数据结构正如几十年来的共识，我们认为，计算机科学教育所赋予人们的最大能量在于对常见算法和数据结构的熟悉。此外，这也可以训练一个人对于各种问题的解决能力，有助于其他领域的学习。 关于算法与数据结构，有成百上千的书可供使用，但是我们的最爱是Steven Skiena编写的 _《算法设计手册》_。显而易见，他对此充满热爱，迫不及待地想要帮助其他人理解。在我们看来，这本书给人一种焕然一新的体验，完全不同于那些更加经常被推荐的书（比如Cormen，Leiserson，Rivest 和 Stein，或者 Sedgewick的书，后两者充斥着过多的证明，不适合以 _解决问题_ 为导向的学习）。 如果你更喜欢视频课程，Skiena慷慨地提供了他的课程。此外，Tim Roughgarden的课程也很不错，在Stanford的MOOC平台Lagunita，或者Coursera上均可获得。Skiena和Roughgarden的这两门课程没有优劣之分，选择何者取决于个人品味。 至于练习，我们推荐学生在Leetcode上解决问题。Leetcode上的问题往往有趣且带有良好的解法和讨论。此外，在竞争日益激烈的软件行业，这些问题可以帮助你评估自己应对技术面试中常见问题的能力。我们建议解决大约100道随机挑选的Leetcode问题，作为学习的一部分。 最后，我们强烈推荐 _《怎样解题》_。这本书极为优秀且独特，指导人们解决广义上的问题，因而一如其适用于数学，它适用于计算机科学。 我可以广泛推荐的方法只有一个： 写之前先思考。 — Richard Hamming 数学知识从某个角度说，计算机科学是应用数学的一个“发育过度”的分支。尽管许多软件工程师试图——并且在不同程度上成功做到——忽视这一点，我们鼓励你用学习来拥抱数学。如若成功，比起那些没有掌握数学的人，你将获得巨大的竞争优势。 对于计算机科学，数学中最相关的领域是“离散数学”，其中的“离散”与“连续”相对立，大致上指的是应用数学中那些有趣的主题，而不是微积分之类的。由于定义比较含糊，试图掌握离散数学的全部内容是没有意义的。较为现实的学习目标是，了解逻辑、排列组合、概率论、集合论、图论以及密码学相关的一些数论知识。考虑到线性代数在计算机图形学和机器学习中的重要性，该领域同样值得学习。 学习离散数学，我们建议从László Lovász的课程笔记开始。Lovász教授成功地让这些内容浅显易懂且符合直觉，因此，比起正式的教材，这更适合初学者。 对于更加高阶的学习，我们推荐 _《计算机科学中的数学》_，MIT同名课程的课程笔记，篇幅与书籍相当（事实上，现已出版）。这门课程的视频同样可免费获得，是我们所推荐的学习视频。 对于线性代数，我们建议从Essence of linear algebra系列视频开始，然后再去学习Gilbert Strang的《线性代数导论》和视频课程。 如果人们不相信数学是简单的，那么只能是因为他们没有意识到生活有多么复杂。 — John von Neumann 操作系统_《操作系统概念》_ （“恐龙书”）和 _《现代操作系统》_ 是操作系统领域的经典书籍。二者都因为写作风格和对学生不友好而招致了一些批评。 _《操作系统导论》（Operating Systems: Three Easy Pieces）_ 是一个不错的替代品，并且可在网上免费获得（英文版）。我们格外喜欢这本书的结构，并且认为这本书的习题很值得一做。 在读完《操作系统导论》后，我们鼓励你探索特定操作系统的设计。可以借助“{OS name} Internals”风格的书籍，比如 _Lion’s commentary on Unix_， _The Design and Implementation of the FreeBSD Operating System_，以及 _Mac OS X Internals_。对于 Linux ，我们推荐 Robert Love 的 _《Linux内核设计与实现》_。 为了巩固对操作系统的理解，阅读小型系统内核的代码并且为其增加特性是一个很不错的方法。比如，xv6，由MIT的一门课程所维护的从Unix V6到ANSI C和x86的移植，就是一个很棒的选择。《操作系统导论》有一个附录，记载了一些可能的xv6实验项目，其中充满了关于潜在项目的很棒想法。 计算机网络鉴于有那么多关于网络服务端和客户端的软件工程，计算机网络是计算机科学中价值最为“立竿见影”的领域之一。我们的学生，系统性地学习了计算机网络，最终能够理解那些曾困扰他们多年的术语、概念和协议。 在这一主题上，我们最爱的书籍是 _《计算机网络：自顶向下方法》_。书中的小项目和习题相当值得练习，尤其是其中的“Wireshark labs”（这部分在网上可以获得）。 如果更喜欢视频课程，我们推荐Stanford的_Introduction to Computer Networking_，可在他们的MOOC平台Lagunita上免费观看。 对于计算机网络的学习，做项目比完成小的习题更有益。一些可能的项目有：HTTP服务器，基于UDP的聊天APP，迷你TCP栈，代理，负载均衡器，或者分布式哈希表。 你无法盯着水晶球预见未来，未来的互联网何去何从取决于社会。 — Bob Kahn 数据库比起其他主题，自学数据库系统需要更多的付出。这是一个相对年轻的研究领域，并且出于很强的商业动机，研究者把想法藏在紧闭的门后。此外，许多原本有潜力写出优秀教材的作者反而选择了加入或创立公司。 鉴于如上情况，我们鼓励自学者大体上抛弃教材，而是从2015年春季学期的CS 186课程（Joe Hellerstein在Berkeley的数据库课程）开始，然后前往阅读论文。 对于初学者，有一篇格外值得提及的论文：“Architecture of a Database System”。这篇论文提供了独特的对关系型数据库管理系统（RDBMS）如何工作的高层次观点，是后续学习的实用梗概。 _《Readings in Database Systems》_，或者以数据库“红书”更为人知，是由Peter Bailis，Joe Hellerstein和Michael Stonebraker编纂的论文合集。对于那些想要在CS 186课程的水平更进一步的学习者，“红书”应当是下一步。 如果你坚持一定要一本导论教材，那我们推荐Ramakrishnan和Gehrke所著的 _《数据库管理系统：原理与设计》_。如需更深一步，Jim Gray的经典著作 _《Transaction Processing: Concepts and Techniques》_ 值得一读，不过我们不建议把这本书当作首要资源。 如果没有编写足够数量的代码，很难巩固数据库理论。CS 186课程的学生给Spark添加特性，倒是不错的项目，不过我们仅仅建议从零实现一个简单的关系型数据库管理系统。自然，它将不会有太多的特性，但是即便只实现典型的关系型数据库管理系统每个方面最基础的功能，也是相当有启发的。 最后，数据模型往往是数据库中一个被忽视的、教学不充分的方面。关于这个主题，我们推荐的书籍是 _Data and Reality: A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World_。 编程语言与编译器多数程序员学习编程语言的知识，而多数计算机科学家学习编程语言 _相关_ 的知识。这使得计算机科学家比起程序员拥有显著的优势，即便在编程领域！因为他们的知识可以推而广之：相较只学习过特定编程语言的人，他们可以更深入更快速地理解新的编程语言。 我们推荐的入门书是 Bob Nystrom 所著的优秀的 _Crafting Interpreters_，可在网上免费获取。这本书条理清晰，富有趣味性，非常适合那些想要更好地理解语言和语言工具的人。我们建议你花时间读完整本书，并尝试任何一个感兴趣的“挑战”。 另一本更为传统的推荐书籍是 _《编译原理》_，通常称为“龙书”。不幸的是，这本书不是为自学者而设计的，而是供教师从中挑选一些主题用于1-2学期的教学。 如果你选择使用龙书进行自学，你需要从中甄选主题，而且最好是在导师的帮助下。我们建议依据某个视频课程来设定学习的结构，然后按需从龙书中获取深入的内容。我们推荐的在线课程是Alex Aiken在MOOC平台 edX 所开设的。 不要做一个只写样板代码的程序员。相反，给用户和其他程序员创造工具。从纺织工业和钢铁工业中学习历史教训：你想制造机器和工具，还是操作这些机器？ — Ras Bodik 在他的编译器课程伊始 分布式系统随着计算机在数量上的增加，计算机同样开始 _分散_。尽管商业公司过去愿意购买越来越大的大型机，现在的典型情况是，甚至很小的应用程序都同时在多台机器上运行。思考这样做的利弊权衡，即是分布式系统的研究所在，也是越来越重要的一项技能。 我们推荐的自学参考书是 Martin Kleppmann 的 _《数据密集型应用系统设计》_。与传统的教科书相比，它是一本为实践者设计的具有很高的可读性的书，并且保持了深度和严谨性。 对于那些偏爱传统教材，或者希望可以从网上免费获取的人，我们推荐的教材是Maarten van Steen和Andrew Tanenbaum所著的 _《分布式系统原理与范型》（中文第二版，英文第三版）_。 对于喜欢视频课程的人，MIT的6.824 是一门很好的在线视频课程，由 Robert Morris 教授的研究生课程，在这里可以看到课程安排。 不管选择怎样的教材或者其他辅助资料，学习分布式系统必然要求阅读论文。这里有一个不错的论文清单，而且我们强烈建议你出席你当地的Papers We Love（仅限美国）。 人工智能/计算机图形学我们试图把计算机科学主题清单限制到那些我们认为 _每一个软件工程师_ 都应该了解的内容，不限于专业或行业。拥有了这些基础，你将能更加轻松地挑选教材或论文，然而无需指引地学习核心概念。在这里，我们给出一些其他常见主题的自学起点： 人工智能：通过观看视频并完成Pacman项目来学习Berkeley的AI课程。至于教材，使用Russell和Norvig编写的 _《人工智能：一种现代方法》_。 机器学习：学习吴恩达在Coursera上的课程。耐心学习，先确保理解了基础概念再奔向类如深度学习的诱人新主题。 计算机图形学：学习Berkeley CS 184课程的材料，使用《计算机图形学：原理及实践》作为教材。 一定要严格遵守推荐的学习次序吗？不过在我们看来，最重要的“先决条件”是：先学计算机架构再学操作系统或数据库，先学计算机网络和操作系统再学分布式系统。 和Open Source Society、freeCodeCamp curricula等比起来，这份指引?OSS指引涵盖太多主题，在许多主题中推荐劣质资源，没有就特定课程哪些方面有价值提供原因或指引。我们努力对这份指引中的课程加以限制，仅仅包括那些你作为软件工程师 _确实需要了解的_，不论你的专业方向，并且对每门课程为何必要做出了解释以帮助你理解。 FreeCodeCamp主要关注编程，而不是计算机科学。至于你为什么要学习计算机科学，参见上文。如果你是个新手，我们建议先学 freeCodeCamp 的课程，一两年后再回归本指南。 XX编程语言怎么样?学习一门特定的编程语言和学习计算机科学的一个领域完全不在一个维度——相比之下，学习语言 _容易_ 且 _缺乏价值_。如果你已经了解了一些语言，我们强烈建议遵照我们的指引，然后在学习的空当中习得语言，或者暂且不管以后再说。如果你已经把编程学得不错了（比如学完了 _《计算机程序的构造和解释》_），尤其是如果你学习过编译器，那么面对一门新的语言，你只需要花一个周末稍多的时间即可基本掌握，之后你可以在工作中学习相关的类库/工具/生态。 XX流行技术怎么样?没有任何一种技术的重要程度可以达到学习其使用足以成为计算机科学教学的核心部分。不过，你对学习那门技术充满热情，这很不错。诀窍是先从特定的技术回退到基本的领域或概念，判断这门流行技术在技术的宏观大局中位于何处，然后才深入学习这门技术。 为什么你们还在推荐SICP?先尝试读一下，有些人觉得 SICP 让人神魂颠倒，这在其他书很少见。如果你不喜欢，你可以尝试其他的东西，也许以后再回到 SICP。 为什么你们还在推荐龙书?龙书依旧是内容最为完整的编译器单本书籍。由于过分强调一些如今不够时新的主题的细节，比如解析，这本书招致了恶评。然而事实上，这本书从未打算供人一页一页的学习，而仅仅是为了给教师准备一门课程提供足够的材料。类似地，自学者可以从书中量身按需挑选主题，或者最好依照公开课授课教师在课程大纲中的建议。 人工智能领域人工智能属于计算机科学的一个分支。传统的人工智能是研究人类如何产生智能，然后让机器学习人的思考方式去行为，现代人工智能概念提出者约翰·麦卡锡认为，机器不一定需要像人一样思考才能获得智能，而重点是让机器能够解决人脑所能解决的问题。 AI目前包含大量各种各样的子领域，范围从通用领域，如学习和感知，到专门领域，如下棋、证明数学定理、在复杂的街道上开车和诊断疾病。计算机尚需具有以下能力： 自然语言处理(natural language processing)使之能成功地用语言交流； 知识表示(knowledge representation)以存储知道的或听到的信息； 自动推理(automated reasoning)以运用存储的信息来回答问题并推出新结论； 机器学习(machine learning)以适应新情况并检测和预测模式。 计算机视觉(computer vision)以感知物体； 机器人学(robotics)以操作和移动对象。 人工智能核心技术发展的两条主线是脑科学和类脑科学的研究。[^2018中国人工智能发展报告] 人工智能的基础哲学：形式规则、思想如何从物理的大脑中产生、知识来自何方、知识如何导致行动 数学：逻辑、计算和概率。其中贝叶斯规则构成了人工智能系统中大多数用于不确定性推理的现代方法的基础。 经济：决策理论、博弈论、运筹学。决策理论把概率理论和效用理论结合起来，在概率描述能适当捕捉决策者制定者的环境的情况下，做出决策提供一个形式化且完整的框架。 神经科学：人脑和数字计算机。 心理学：认知理论。认知理论应该描述详细的信息处理机制，靠这个机制可以实现某种认知功能。 计算机工程：建造高效的计算机。 控制论：目标函数。 语言学：语言与思维的关联。知识表示，自然语言处理。[^人工智能·一种现代的方法(3rd)] 大数据：也是人工智能的基础，而使大数据转变为知识或生产力，离不开机器学习，机器学习可以说是人工智能的核心，是使得机器具有类似人的智能的根本途径。 机器学习和深度学习算法是人工智能中的两大热点，关系如下： 机器学习强调三个关键词：算法、模型、算力。其本质如下： 大数据时代，产生数据的能力空前高涨，如互联网、移动网、物联网、成千上万的传感器、穿戴设备、定位系统等，存储数据、处理数据等能力也得到了几何级数的提升。Hadoop、Spark技术为存储和处理大数据提供了有效的方法。 数据就是信息，其背后隐含着大量不易被我们感官识别的信息、知识、规律等，如何揭示这些信息、规则、趋势，需要用到数据挖掘。 机器学习的任务，就是在大数据量的基础上，发掘有用信息，其处理的数据越多，机器学习就越能体现出优势。如语言识别、图像识别、天气预测等等。 机器学习经典书pattern recognition and machine learning被认为是贝叶斯方法的扛鼎之作，系统地介绍了模式识别和机器学习领域内的概念和基础。整体目录如下： 第一章 绪论 第二章 概率分布 第三章 线性回归模型 第四章 线性分类模型 第五章 神经网络 第六章 核方法 第七章 稀疏核机器 第八章 图模型 第九章 混合模型和EM 第十章 近似判断 第十一章 采样方法 第十二章 连续潜在变量 第十三章 顺序数据 第十四章 组合模型 pattern recognition and machine learning 模式识别与机器学习 PRML读书会-我爱自然语言处理 周志华-机器学习(西瓜书)整体目录如下： 第一章 绪论 第二章 模型评估与选择 第三章 线性模型 第四章 决策树 第五章 神经网络 第六章 支持向量机 第七章 贝叶斯分类 第八章 集成学习 第九章 聚类 第十章 降维与度量学习 第十一章 特征选择与稀疏学习 第十二章 计算学习理论 第十三章 半监督学习 第十四章 概率图模型 第十五章 规则学习 第十六章 强化学习 周志华-机器学习 周志华-机器学习-公式推导 李航-统计学习方法(一、二版)李航-统计学习方法 深度学习经典书深度学习-花书stanford CS231n课程stanford CS231n课程：CS231n的全称是CS231n: Convolutional Neural Networks for Visual Recognition，即面向视觉识别的卷积神经网络。该课程是斯坦福大学计算机视觉实验室推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。本课程将深入讲解深度学习框架的细节问题，聚焦面向视觉识别任务（尤其是图像分类任务）的端到端学习模型。 参考资料]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具资源整理]]></title>
    <url>%2Fblog%2F2020%2F07%2F08%2F%E5%B7%A5%E5%85%B7%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86-%E5%B8%B8%E6%9B%B4%2F</url>
    <content type="text"><![CDATA[GitHub上各式各样的小徽章制作徽章是一种小巧精美的小图标，一般配有相关文字进行辅助说明，富有表现力。 其本质是一种svg格式的矢量图标，不仅出现在GitHub项目主页，能够表现图片的地方都可以出现徽章。 徽章的使用大多是徽章是svg格式，也不排除某些是png格式，都可以当成图标使用。 如果希望在markdown文件中使用徽章，最好使用在线链接，或者引入到本地相关文件中。 徽章格式：[![图片说明](图片源地址)](超链接地址) 即超链接内部嵌套图片 如果是在HTML文件中使用徽章，同样需要先取得在线徽章地址，然后按照HTML语法插入图片即可。 徽章格式：&lt;a href=&quot;超链接地址&quot;&gt;&lt;img src=&quot;图片源地址&quot; alt=&quot;图片文字说明&quot;&gt;&lt;/a&gt; 即超链接内部嵌套图片 不论什么语法，最核心的是获得徽章链接。至于不同语言有着各自的语法，按照语言规则手动拼接即可。 徽章分类以徽章格式为标准，分为svg和png两类 以徽章内容数据是否是动态为标准，可以分为静态数据和动态数据。 还有一些其他分类，不赘诉。 徽章来源徽章有不同的分类，不管是哪一张分类，在线徽章最为简单便捷，下面介绍几个提供在线生成徽章的网站。 shields.io badgen.net forthebadge.com badge.fury.io github.com/boennemann/badges 本文来源]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM网络理解]]></title>
    <url>%2Fblog%2F2020%2F07%2F07%2FLSTM%E7%BD%91%E7%BB%9C%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Recurrent Neural Network传统的神经网络很难处理一件事——使用先前事件推断后续的事件。RNN解决了这个问题，是包含循环的网络，允许信息的持久化。 $A$正在读取输入$x_t$，并输出$h_t$。循环可以使得信息可以从当前步传递到下一步。 RNN can be thought of as multiple copies of the same network, each passing a message to a successor(接替者). Consider what happens if we unroll the loop: This chain-like nature reveals that RNN are intimately related to sequences and lists(和序列和列表相关). There have been incredible success applying RNNs to a variety of problems: Speech recognition, Language modeling, Translation, Image captioning… Essential to these successes is the use of “LSTMs”, a very special kind of RNN which works, for many tasks, much much better than standard version. The problem of Long-Term DependenciesOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task. In theory, RNNs are absolutely capable of handling long-term dependencies. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. While, as the gap grows, RNNs become unable to learn to connect the information. 例如：有一个语言模型用来基于先前的的词来预测下一个词，如果试着预测”the clouds are in the sky”最后的一个词，并不需要任何其他的上下文，显然最后的一个词就是sky。在这样的场景中，相关的信息和预测的词位置之间的间隔非常小，RNN可以学会使用先前的信息。 然而，对于一些更加复杂的场景，如预测”I grew up in France… I speak fluent French”最后的词，当前的信息建议下一个词可能是一种语言的名字，这需要先前提到的离当前位置很远的France的上下文。此时，相关信息和当前预测位置之间的间隔变得很大，在这个间隔不断变大时，RNN会丧失学习到连接如此远的信息的能力。虽然可以仔细挑选参数来解决这类toy problems。但在实践中，RNN肯定不能够成功学习到这些知识，Bengio,et al.(1994)等人对该问题进行了深入研究，发现一些使得RNN训练变得非常困难的根本原因。 LSTM don’t have this problem! LSTM NetworksLong Short Term Memory networks(LSTMs)是一种RNN特殊的类型，可以学习长期依赖信息。LSTM由Hochreiter &amp; Schmidhuber(1997)提出，并被Alex Graves进行了改良和推广。 LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering info for long periods of time is practically their default behavior, not something they struggle to learn. All RNNs have the form of a chain of repeating modules of neural networks(重复神经网络模块的链式结构). 标准的RNN 标准的RNN中，重复的模块只有一个简单的结构，如一个tanh层： LSTM LSTM重复模块不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互： LSTM解析图元素 Neural Network Layer: 学习到的网络层 Pointwise Operation: pointwise操作，如向量的和 Vector Transfer: 传输一整个向量，从一个节点输出到其他节点的输入 Concatenate: 向量的连接 Copy: 内容被复制，然后分发到不同的位置 The Core Idea Behinds LSTMsLSTM的关键是细胞状态，水平线在图上方贯穿运行。 细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互，信息在上面流传保持不变会变得很容易。 LSTM通过精心设计的“门(gates)”结构来去除或者增加信息到细胞状态的能力。 门是一种让信息选择式的通过的方法，包含一个sigmoid神经网络层和一个pointwise乘法操作。 Sigmoid层输出0-1之间的数值，描述每个部分有多少量可以通过。0表示“不允许任何量通过”，1表示“允许任意量通过”。 LSTM有三个门，来保护和控制细胞状态。 Step-by-Step LSTM Walk Through第一步决定从细胞状态中丢弃什么信息。通过忘记门层(forget gate layer)完成。该门读取$h_{t-1}$和$x_t$，输出一个0-1之间的数值给每个在细胞状态$C_{t-1}$中的数字。1表示完全保留，0表示完全舍弃。 第二步决定什么样的新信息被存放到细胞状态中。包含两个部分，一是sigmoid层（输入门层）决定什么值将要更新，还有一个是tanh层将创建一个新的候选值向量$\tilde{C}_t$，会被加入到状态中。 第三步确定更新信息。$C_{t-1}$更新为$C_t$。把旧状态与$f_t$相乘，丢弃掉确定需要丢弃的信息。接着加上$i_t \times \tilde{C}_t$，即是新的候选值，根据我们决定更新每个状态的程度进行变化。 第四步更新细胞状态。最终，确定输出什么值，这个输出将会基于我们的细胞状态，但也是一个过滤后的版本。 首先，运行一个sigmoid层来确定细胞状态的哪一部分将输出出去。接着，把细胞状态通过tanh进行处理，得到一个在-1到1之间的值，并将其与sigmoid门的输出相乘。最终，仅输出我们确定输出的那部分。 Variants on LSTMs以上介绍的是正常的LSTM结构，实际使用上，都会采用微小的变体。 1、流形的LSTM变体。 是由Gers &amp; Schmidhuber(2000)提出的，增加了”peephole connection”。让 门层 也会接受细胞状态的输入。 图例中是所有部分都加入了peephole到每个门上，也有加入部分的peephole。 2、使用coupled忘记和输入门。 不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. 仅仅在输入的当前位置进行忘记，仅仅输入新的值到已经忘记的旧的信息状态里。 3、Gated Recurrent Unit (GRU) 是由Cho,et al.(2014)提出的。将忘记门和输入门合成一个单一的 更新门。同样还混合了细胞状态和隐藏状态以及其他一些改动。最终的模型比标准的LSTM模型更加简单。 还有很多其他类型变体形式，如Yao,et al.(2015)提出的Depth Gated RNN。Koutnik, et al.(2014)提出的Clockwork RNN。Greff, et al.(2015)给出了流行的LSTM变体的比较，结论是基本一样。Jozefowicz,et al.(2015)在超过10000种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比LSTM更好的结果。 参考资料——colah’s blog]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>LSTM网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学中需要用到的数学]]></title>
    <url>%2Fblog%2F2020%2F06%2F10%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%AD%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[摘自Benjamin Obi Tayo Ph.D.博文 I. IntroductionIf you are a data science aspirant, you no doubt have the following questions in mind: Can I become a data scientist with little or no math background?\ What essential math skills are important in data science?\ There are so many good packages that can be used for building predictive models or for producing data visualizations. Some of the most common packages for descriptive and predictive analytics include: Ggplot2 Matplotlib Seaborn Scikit-learn Caret TensorFlow PyTorch Keras Thanks to these packages, anyone can build a model or produce a data visualization. However, very solid background knowledge in mathematics is essential for fine-tuning your models to produce reliable models with optimal performance. It is one thing to build a model, it is another thing to interpret the model and draw out meaningful conclusions that can be used for data-driven decision making. It’s important that before using these packages, you have an understanding of the mathematical basis of each, that way you are not using these packages simply as black-box tools. II. Case Study: Building A Multiple Regression ModelLet’s suppose we are going to be building a multi-regression model. Before doing that, we need to ask ourselves the following questions: How big is my dataset? What are my feature variables and target variable? What predictor features correlate the most with the target variable? What features are important? Should I scale my features? How should my dataset be partitioned into training and testing sets? What is principal component analysis (PCA)? Should I use PCA for removing redundant features? How do I evaluate my model? Should I used R2 score, MSE, or MAE? How can I improve the predictive power of the model? Should I use regularized regression models? What are the regression coefficients? What is the intercept? Should I use non-parametric regression models such as KNeighbors regression or support vector regression? What are the hyperparameters in my model, and how can they be fine-tuned to obtain the model with optimal performance? Without a sound math background, you wouldn’t be able to address the questions raised above. The bottom line is that in data science and machine learning, mathematical skills are as important as programming skills. As a data science aspirant, it is therefore essential that you invest time to study the theoretical and mathematical foundations of data science and machine learning. Your ability to build reliable and efficient models that can be applied to real-world problems depends on how good your mathematical skills are. To see how math skills are applied in building a machine learning regression model, please see this article: Machine Learning Process Tutorial. Let’s now discuss some of the essential math skills needed in data science and machine learning. III. Essential Math Skills for Data Science and Machine Learning1. Statistics and ProbabilityStatistics and Probability is used for visualization of features, data preprocessing, feature transformation, data imputation, dimensionality reduction, feature engineering, model evaluation, etc. Here are the topics you need to be familiar with: Mean, Median, Mode, Standard deviation/variance, Correlation coefficient and the covariance matrix, Probability distributions (Binomial, Poisson, Normal), p-value, Baye’s Theorem (Precision, Recall, Positive Predictive Value, Negative Predictive Value, Confusion Matrix, ROC Curve), Central Limit Theorem, R_2 score, Mean Square Error (MSE), A/B Testing, Monte Carlo Simulation* 2. Multivariable CalculusMost machine learning models are built with a dataset having several features or predictors. Hence, familiarity with multivariable calculus is extremely important for building a machine learning model. Here are the topics you need to be familiar with: Functions of several variables; Derivatives and gradients; Step function, Sigmoid function, Logit function, ReLU (Rectified Linear Unit) function; Cost function; Plotting of functions; Minimum and Maximum values of a function\ 3. Linear AlgebraLinear algebra is the most important math skill in machine learning. A data set is represented as a matrix. Linear algebra is used in data preprocessing, data transformation, dimensionality reduction, and model evaluation. Here are the topics you need to be familiar with: Vectors; Norm of a vector; Matrices; Transpose of a matrix; The inverse of a matrix; The determinant of a matrix; Trace of a Matrix; Dot product; Eigenvalues; Eigenvectors\ 4. Optimization MethodsMost machine learning algorithms perform predictive modeling by minimizing an objective function, thereby learning the weights that must be applied to the testing data in order to obtain the predicted labels. Here are the topics you need to be familiar with: Cost function/Objective function; Likelihood function; Error function; Gradient Descent Algorithm and its variants (e.g. Stochastic Gradient Descent Algorithm)\ IV. Summary and ConclusionIn summary, we’ve discussed the essential math and theoretical skills that are needed in data science and machine learning. There are several free online courses that will teach you the necessary math skills that you need in data science and machine learning. As a data science aspirant, it’s important to keep in mind that the theoretical foundations of data science are very crucial for building efficient and reliable models. You should, therefore, invest enough time to study the mathematical theory behind each machine learning algorithm. V. ReferencesLinear Regression Basics for Absolute Beginners. Mathematics of Principal Component Analysis with R Code Implementation. Machine Learning Process Tutorial.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>data science</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu代理软件Qv2ray使用]]></title>
    <url>%2Fblog%2F2020%2F05%2F27%2Fubuntu%E4%BB%A3%E7%90%86%E8%BD%AF%E4%BB%B6Qv2ray%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Qv2ray是一款全平台的代理软件，简单配置就可以使用。 官方github地址 官方手册 安装官方库下载release的版本（下的2.5.0的），下载的是Ubuntu的程序包： Qv2ray.v2.5.0.linux-x64.AppImage 下载的是.appimage文件，右键属性，在权限一栏，勾选上允许作为程序执行文件，双击打开，进入软件。 v2ray内核设置进入软件后，点击首选项，设置v2ray内核。 这里首先需要下载v2ray-core 然后将其放到Qv2ray的资源目录中。配置如下： 加入订阅链接直接订阅之后，连接服务器即可 Qv2ray的代理端口配置： 系统代理端口设置 浏览器的代理配置使用的是chrome插件：switchOmega 。配置如下： windows端配置下载clash for windows安装后，配置一键导入即可。 本文参考来源 附： 在Ubuntu18.04上用v2rayL代理软件教程。需要的环境较为苛刻（Ubuntu18.04+python3.6），一直没能在Ubuntu16.04上配置成功.]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Qv2ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译器原理简介]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2F%E7%BC%96%E8%AF%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1、概念 广义上说，能把高级语言转化为低级语言的都算编译器，编译的存在就是为高级语言服务。 计算机语言分为三层，最底层，机器语言，即0和1；往上是汇编语言，指明操作什么部件干什么活，是对机器干活的描述，再往上是高级语言，人不用理机器干什么活，只需要明确地表达自己要达到什么目的。 对于CPU而言，只认识二进制的机器码，指挥CPU运行程序，需要一个“翻译”，将代码翻译成机器码给CPU，让它知道如何执行你的程序。常见的“翻译”工具有：汇编器、编译器、解释器。 汇编器：将汇编代码直接翻译成机器码，速度快，但可读性差，写大型程序很难实现。 编译器：将常用的高级语言，如Java/c++等，翻译成机器码，编译很慢，但是执行起来很快。 解释器：程序不需要编译，程序在运行时才翻译成机器码，每执行一次都要翻译一次。效率低，运行慢，典型的解释行语言如PHP/JS/Python。 2、Java代码的执行 依赖于Java虚拟机(JVM)，JVM能直接识别字节码。Java代码经过编译会产生class文件，即字节码文件，再交由JVM处理。当字节码被放到某种指令集的机器上，由运行环境中的虚拟机将字节码直接编译成该指令集的机器码然后执行，所以不管指令集是什么，只要更换运行环境中的虚拟机就能执行，从而实现跨平台性。 以hotspot为例，有几种翻译形式： （1）解释执行 逐条将字节码翻译成机器码并执行；无需等待编译 （2）即时编译（just-in-time,JIT） 将一个方法中包含的所有字节码编译成机器码后再执行；运行速度更快 在ART中，不要求跨平台性，直接在一个平台上运行，可以不用实时编译浪费时间。在ART中，直接将字节码在安装时编译为机器码，在安卓开发中，ART实际上是在机器上实现的，和编译器没有关系，编译器编译的还是字节码。 3、Android代码执行 开发Android目前最多的还是Java，Android工程中的Java源文件经过编译也是生成class文件，最后打包为DEX字节码文件，负责将DEX字节码翻译成字节码的是Dalvik或者ART。 在Android5.0之前，是Dalvik的天下，Dalvik是解释执行加上JIT，每次app运行时，动态的将一部分Dalvik字节码解释为机器码，随着app的运行，更多的字节码被编译和缓存。因为JIT只编译了一部分代码，具有更小的内存占用和更少的设备物理空间占用。但是边解释边执行，效率低下。 Android4.4开始，引入ART，到5.0，正式代替Dalvik，ART使用的是AOT(Ahead of time)的编译方式，即在应用安装过程中，就将所有的Dex字节码编译成机器码存储在设备空间中，完全抛弃JIT。带来的好处： app运行更快，直接运行机器码 减少应用的启动时间，本地代码可以直接执行 节省电量消耗，不需要再去一行一行解释字节码 增强了垃圾回收 增强了开发者工具 但是，在安装期间翻译字节码，安装过程会很长，尤其是对一些大型应用来说，另外，安装过程中翻译出来的机器码占用了更大的机身存储空间。 Android7.0，又加入了JIT，一个具备代码分析功能的即时编译器，根据二八定律，经常运行的热点代码可能只占20%，甚至更少，没有必要通过AOT提前将所有的字节码翻译成机器码。安装过程中，放弃AOT，加快安装速度，初次使用时需要解释执行。使用app时，JIT开始分析代码，在合适的时候将字节码翻译成机器码，在Android应用运行时持续提高其性能，设备空闲时，AOT发挥作用，将热点代码翻译成机器码并保存下来，进一步提高运行效率。 现在的Android是 解释执行、JIT、AOT共存的。所做的无非是 安装速度、空间占用和运行速度的平衡。 4、现代编译器结构 现代编译器通常分为前端和后端，前端将高级语言转化为中间的统一代码，然后由后端将中间代码编译为机器代码，处理硬件架构相关的优化。 当前最主要的编译器有两个：GCC和llvm。早在iOS4时xcode的前端有两种，gcc和clang，后端是llvm和GCC，后来统一为clang+llvm，而目前NDK前端也有GCC和clang，后端为llvm。但安卓已经全面放弃GCC全面转向clang。也就是说，当我们考虑一个现代编译器时，必须前后端同时考虑。 5、方舟编译器 IOS的快、流畅是建立在苹果强大的A系列处理器，精心设计的swift语言，继承swiftc编译命令的xcode工具，高效的IOS系统，严格的权限管理等基础上，称之为软硬件集合一体化的典范。 方舟编译器舍弃了现在Android中的ART虚拟机，不需要编译为字节码文件（DEX），在生成apk的安装包时直接编译生成的适合的机器码，在终端设备上安装后直接就可以执行，省去虚拟机，省去JIT和AOT。 PPT 中最后一句话是 “希望 APP 厂商尽快使用” ，并不是手机厂商，所以不排除方舟编译器可以直接将 Apk ，或者说 Apk 中的 DEX 打包成机器码格式。但由于机器码并不是平台兼容的，所以并不能确定方舟编译器是否必须要绑定 EMUI。 方舟编译器是为了改变现有的代码和编程习惯的基础上进行编译的优化，使得APP运行更为流畅，最终的目的是成为一个跨硬件平台、跨系统、跨语言的软件编译平台。编译器是一个桥梁，连接着上层的开发语言与底层的硬件，又与操作系统紧密结合，掌握了编译器，更换开发语言，更换硬件架构甚至更换操作系统都会有很大的帮助。 参考资料1 参考资料2 参考资料3]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解机器学习原理及方法]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86%E5%8F%8A%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[chapter 2 统计学习理论统计学习理论的一般框架1、学习器的输入 领域集：一个任意的集合$\mathcal{X}$，称为实例空间 标签集：$\mathcal{Y}$ 训练集：带标签的样本 2、学习器的输出 输出是一个预测规则：$h : \mathcal{X} \to \mathcal{Y}$。该函数也称为预测器/分类器，用来预测一个新的领域元素的标签。 3、数据生成模型 训练数据的产生过程：首先根据概率分布$\mathcal{D}$采集样本点$x_i$，然后利用标记函数$f: \mathcal{X} \to \mathcal{Y}$打上合适的标签。 4、衡量成功 分类器的误差：未能成功预测随机数据点正确标签的概率（随机数据点是从潜在的分布中生成的）。 $h$的误差是$h(x) \ne f(x)$的概率，其中$x$是根据分布$\mathcal{D}$采集的随机样本。 经验风险最小化(ERM)一个学习算法的输入是一个训练集$S$，训练集从一个未知分布$\mathcal{D}$中采样获得，通过目标函数$f$对训练样本进行标记。 预测器$h_S : \mathcal{X} \to \mathcal{Y}$，学习算法目标是求出一个最小的预测器$h$，使得关于未知分布$\mathcal{D}$和$f$的预测误差最小化。$$L_S(h) \overset{\rm{def}}{=}\frac{|h(x_i) \ne y_i,i\in \{1,\dots,m\}|}{m}$$从预测器$h$出发到最小化$L_S(h)$，称为经验风险最小化（ERM）。 显然，无论样本是什么，$L_S(h_S) =0$，分类器会选择一种ERM算法（经验最小损失假设，没有分类器会比这种假设具有更小的误差），在训练集上效果很好，在验证集上效果很差，会出现过拟合问题。 考虑归纳偏置的ERMERM规则容易导致过拟合，需要进行修正，找到使其不会过拟合的条件。 通常解决方案：在一个受限的搜索空间使用ERM学习准则。 一个学习器在接触到数据之前选择一个分类器的集合$\mathcal{H}$，这个集合称为假设类。每一个$h \in \mathcal{H}$是从$\mathcal{X}\to \mathcal{Y}$的一个函数。对于给定的假设类$\mathcal{H}$和一个训练样本集$S$，ERM学习器根据在$S$上的最小化概率误差，利用ERM规则选择一个预测器$h \in \mathcal{H}$。$$\rm{ERM}_{\mathcal{H}}(S) \in \arg \min_{h \in \mathcal{H}} L_S(h) ,argmin表示从\mathcal{H}选择使L_S(h)最小的假设h$$通过限制学习器从$\mathcal{H}$中选择预测器，选择偏向于一个特别的预测器集合，这种限制称为归纳偏置。这种选择决定在于学习器接触训练数据之前，因此需要先验知识。 如果$\mathcal{H}$是有限类，当拥有足够多的训练样本，$\rm{ERM}_{\mathcal{H}}$将不会过拟合。 由于$L_{\mathcal{D,f}}(h_s)$是依赖于训练集$S$，训练集通过一个随机过程采样，因此通过风险$L_{\mathcal{D,f}}(h_s)$来选择预测器$h_s$也存在随机性，总有一定概率使得采样获得训练数据中有一些训练数据对于分布$\mathcal{D}$来说完全不具有代表性。一般来说，将采样得到的非代表性样本的概率表示为$\delta$，同时$1-\delta$称为置信参数(confidence parameter)。 由于无法保证标签预测绝对准确，引入一个参数评价预测的质量，称为精度参数(accuracy parameter)，记作$\varepsilon$，当$L_{\mathcal{D,f}}(h_s) \le \varepsilon$，认为算法输出一个近似正确的预测。 chapter 3 概率近似正确(PAC)模型一般学习模型——概率近似正确(PAC)学习模型 在经验风险最小化的规则下，对于一个有限假设类，如果有足够夺得训练样本（训练样本的数量独立于潜在的分布，并且独立于标记函数），那么输出的假设类是概率近似正确的。 (PAC可学习)：若存在一个函数$m_{\mathcal{H}}:(0,1)^2 \to \mathbb{N}$和一个学习算法，使得对于任意$\varepsilon，\delta(0,1)$和$\mathcal{X}$上的任一分布$\mathcal{D}$，任意的标记函数$f:\mathcal{X} \to \{0,1\}$，如果在$\mathcal{H,D,f}$下满足可实现假设，那么当样本数量$m \ge m_{\mathcal{H}}(\varepsilon,\delta)$时，其中样本由分布$\mathcal{D}$独立同分布采样得到并且由函数$f$标记，算法将以不小于$1-\delta$的概率返回一个假设类$h$，使得该假设类$h$满足$L_{\mathcal{D,f}}(h) \le \varepsilon$ PAC包含两个近似参数：准确度参数$\varepsilon$表征输出的分类器和最优分类器之间的距离（PAC的近似正确部分）；置信度参数$\delta$表征分类器达到准确要求的可能性（PAC的概率部分）。 函数$m_{\mathcal{H}}:(0,1)^2\to \mathbb{N}$决定学习假设类$\mathcal{H}$的采样复杂度：保证一个概率近似正确解所需要的样本数量。 任一有限假设类是PAC可学习的，其采样复杂度满足：$$m_{\mathcal{H}}(\varepsilon,\delta) \le \lceil \frac{\log(|\mathcal{H}|) / \delta}{\varepsilon} \rceil$$ (不可知PAC可学习)：若存在一个函数$m_{\mathcal{H}}:(0,1)^2 \to \mathbb{N}$和一个学习算法，使得对于任意$\varepsilon，\delta(0,1)$和$\mathcal{X}$上的任一分布$\mathcal{D}$，当样本数量$m \ge m_{\mathcal{H}}(\varepsilon,\delta)$时，其中样本由分布$\mathcal{D}$独立同分布采样得到，算法将以不小于$1-\delta$的概率返回一个假设类$h$，使得该假设类$h$满足： $L_{\mathcal{D}}(h) \le \min \limits_{h’ \in \mathcal{H}} L_{\mathcal{D}}(h’)+ \varepsilon$ 不可知PAC可学习是PAC可学习的泛化，当不满足可实现的假设时，学习是不能保证任意小的误差的，然而，在不可知PAC学习的定义下，即使和假设类中最好的分类器有差距，学习器依然可以认为是学习成功的。PAC学习要求学习器学到的分类器，其误差达到一个很小的绝对值，而且和假设类可达到的最小误差没有关系。 学习问题建模 多分类问题：不再是二分类问题，对于这类任务，学习器需要根据已有的正确分类，对新的输入给出相应的正确分类。 回归问题：希望找到数据的简单模型——数据$\mathcal{X,Y}$之间的关联函数。度量是否成功的标准不同，可以使用期望平方差来评估假设函数$h:\mathcal{X} \to \mathcal{Y}$给出的预测值与真实值之间的差异，即：$$L_{\mathcal{D}}(h) \overset{def}{=} \mathbb{E}_{(x,y) \sim \mathcal{D}} (h(x) - y)^2$$分类问题的损失函数： 0-1损失：随机变量$z$取值序列对集合$\mathcal{X} \times \mathcal{Y}$，损失函数为：$$\mathscr{L}_{0-1}(h,(x,y))\overset{def} = \cases{0 \qquad 若h(x) = y \ 1 \qquad 若h(x) \ne y}$$平方损失：随机变量$z$取值序列对集合$\mathcal{X} \times \mathcal{Y}$，损失函数为：$$\mathscr{L}_{sq} (h,(x,y)) \overset{def} = (h(x) - y)^2$$ chapter 4 一致收敛在可实现的假设下，任何有限的假设类都是PAC可学习的。 一致收敛：用来表明在有一般损失函数的不可知PAC模型中，只要距离损失函数是有界的，任何有限类都是可学习的。 chapter 5 没有免费午餐定理&amp;误差分解没有学习器能在所有的任务上学习成功，具体阐述为： (没有免费午餐定理) 对实例空间$\mathcal{X}$上0-1损失的二分任务，令$A$表示任意的学习算法，样本大小$m$表示小于$\frac{|\mathcal{X}|}{2}$的任意数，则在$\mathcal{X} \times \{0,1\}$上存在一个分布$\mathcal{D}$，使得： 1、存在一个函数$f:\mathcal{X} \to \{0,1\}$，满足$L_{\mathcal{D}}(f) = 0$ 2、在样本集$\mathcal{S} \sim \mathcal{D}^m$上，以至少$\frac{1}{7}$的概率满足$L_{\mathcal{D}}(A(S)) \ge \frac{1}{8}$ 对于每个学习器，都存在一个任务使其失败，即使这个任务能够被另一个学习器成功学习。即 不存在通用的学习器，每个学习器都有其特定的任务，为了学习成功要采用一些关于任务的先验知识。 误差分解 将一个$\rm{ERM}_{\mathcal{H}}$预测器的误差分解为两部分，$h_S$是预测器的一个假设，有：$$L_{\mathcal{D}}(h_S) = \varepsilon _{app} + \varepsilon_{est}$$其中：$\varepsilon_{app} = \min\limits_{h \in \mathcal{H}} L_{\mathcal{D}}(h)$ 逼近误差：假设类里预测器所取得的最小风险。由于限制到一个具体假设类所引起的风险，即所产生的归纳偏置。逼近误差不依赖于样本大小，取决于所选择的假设类。扩大假设类可以减小逼近误差。 估计误差：逼近误差与ERM预测器误差之间的差异。产生原因是：经验风险（训练误差）是真实风险的一个估计，最小化经验风险预测器只是最小化真实风险预测器的一个估计。 chapter 6 VC维VC维概述如果不对假设类加以限制，任何学习算法总会遇到表现很差的情况，与此同时，总是有学习算法在此情况下表现很好。 使用一个有限集$C \subset \mathcal{X}$，并且考虑在$C$上元素的分布族，其中每个分布由从$C$到$\{0,1\}$目标函数产生，为了使得任何算法都失败，可以从由$C$到$\{0,1\}$所有可能的函数构成的集合中选择一个目标函数。 考虑到一个假设类$\mathcal{H}$的PAC可学习性，需要构建一些分布使得某些假设$h \in \mathcal{H}$达到零风险。 （限制$\mathcal{H}$在$C$上）令$\mathcal{H}$是从$\mathcal{X}$到$\{0,1\}$的一个函数类，并且令$C = \{c_1,\dots,c_m\} \subset \mathcal{X}$。限制$\mathcal{X}$在$C$上就是由来自$\mathcal{H}$从$C$上到$\{0,1\}$的函数构成的集合。即：$$\mathcal{H}_C =\{[h(c_1),\dots,h(c_m)]:h\in \mathcal{H}\}$$将每个从$C$到$\{0,1\}$的函数表示为形如$\{0,1\}^{|C|}$的向量。 （打散）如果限制$\mathcal{H}$在$C$上是从$C$到$\{0,1\}$的所有函数的集合，则假设类$\mathcal{H}$打散了有限集$C \subset \mathcal{X}$，此时$|\mathcal{H}_C | = 2^{|C|}$。 （VC维）假设类$\mathcal{H}$的VC维，记作VCdim($\mathcal{H}$)，是$\mathcal{H}$可以打散的最大集合$C \subset \mathcal{X}$的大小，如果$\mathcal{H}$可以打散任意大的集合，称$\mathcal{H}$的VC维是无穷的。 令$\mathcal{H}$是无穷VC维的假设类，那么$\mathcal{H}$不是PAC可学习的。 PAC学习的基本定理VC维无限的类不是可学习的，因此可以得到统计学习的基本定理： 令$\mathcal{H}$是一个由从$\mathcal{X}$到$\{0,1\}$的映射函数构成的假设类，且令损失函数为0-1损失，下面说法等价： 1、$\mathcal{H}$有一致收敛性 2、任何ERM规则都是对于$\mathcal{H}$成功的不可知PAC学习器 3、$\mathcal{H}$是不可知PAC可学习的 4、$\mathcal{H}$是PAC可学习的 5、任何ERM规则都是对$\mathcal{H}$成功的PAC学习器 6、$\mathcal{H}$的VC维有限 chapter 7 不一致可学习不一致可学习概述不一致可学习 允许学习器针对所竞争的不同假设使用不同数量的样本，认为一个假设$h$以$(\varepsilon ,\delta)$可与另一个假设$h’$竞争，$L_{\mathcal{D}}(h) \le L_{\mathcal{D}}(h’) + \varepsilon$成立的概率不少于$(1-\delta)$。 在PAC可学习中，当我们寻找具有绝对的最小风险的假设（在可能的情况下）或者寻找一个与最小风险差不多风险（在绝对最小风险不可知情况下）的假设，样本数量仅仅依赖于精度和置信度。、 在不一致学习中，允许样本数量以$m_{\mathcal{H}}(\varepsilon ,\delta,h)$的形式表示，也即，不一致可学习在表现形式上也依赖竞争力变量$h$。 （不一致可学习）若存在一个学习算法$A$和一个函数$m_{H}^{NUL}:(0,1)^2 \times \mathcal{H} \to \mathbb{N}$，使得对于任意的$\varepsilon,\delta \in (0,1)，h \in \mathcal{H}$，如果样本数量$m \ge m_{H}^{NUL}(\varepsilon ,\delta,h)$，那么对每个分布$\mathcal{D}$和所有的样本$\mathcal{S} \sim \mathcal{D}^m$，下式成立的概率不少于$1-\delta$，$$L_{\mathcal{D}}(A(S)) \le L_{\mathcal{D}}(h) + \varepsilon$$则假设类$\mathcal{H}$是不一致可学习的。 结构风险最小化(SRM)通过具体化一个假设类$\mathcal{H}$来利用先验知识，并且相信这样一个假设类中包含完成当前任务的有效预测器。 另一种表达先验知识的方式是将假设类$\mathcal{H}$上的偏好具体化，在结构风险最小化范例中，首先假定$\mathcal{H}$能够写成$\mathcal{H} = \mathop{\cup}\limits_{n \in N}\mathcal{H}_n$，然后具体化一个权重函数：$\omega:\mathbb{N} \to [0,1]$，这个权重函数给每个假设类赋予一个权重，高的权值表示对该假设类的强烈偏好。 先验：$\mathcal{H} = \mathop{\cup}\limits_{n \in N}\mathcal{H}_n$，$\mathcal{H}_n$满足一致收敛，复杂度函数为$m_{\mathcal{H}_n}^{UC}$ ​ $\omega: \mathbb{N} \to [0,1]$，其中$\sum\limits_{n}\omega(n) \le 1$ 定义：$\varepsilon_n(m,\delta) = \min \{\varepsilon \in (0,1):m_{\mathcal{H}_n}^{UC}(\varepsilon,\delta) \le m\}$，$n_h = \min\{n: h \in \mathcal{H}_n\}$ 输入：训练集$S \sim \mathcal{D}^m$，置信度$\delta$ 输出：$h \in \arg \min\limits_{h \in \mathcal{H}}[L_S(h) + \varepsilon_{n(h)} (m,\omega(n(h))·\delta)]$ 与经验风险最小化不同，SRM不仅关心经验风险$L_S(h)$，而且为了最小化估计误差，更加关心在最小经验风险的偏置和$\varepsilon_{n(h)} (m,\omega(n(h))·\delta)]$最小化之间取得一个平衡。 chapter 9 线性预测定义仿射函数类：$$L_d = \{h_{\omega,b}: \pmb \omega \in \mathbb{R}^d，b \in \mathbb{R}\}$$ $$h_{\omega,b} (\pmb x) = &lt;\pmb{\omega ,x}&gt; + b = \left( \sum\limits_{i=1}^{d} \omega_i x_i\right) +b$$ 记作：$L_d = \{ \pmb{x\mapsto&lt;\omega,x&gt;} +b:\omega \in \mathbb{R}^d , b \in \mathbb{R}\}$ $L_d$是函数集合，其中每个函数被$\pmb \omega \in \mathbb{R}^d$和$b \in \mathbb{R}$参数化，并以向量$\pmb x$作为输入，以标量$&lt;\pmb {\omega,x}&gt;+b$作为输出。 将偏移量$b$包含在$\pmb \omega$中，可以写成$\pmb \omega’ = (b,\omega_1,\dots,\omega_d) \in \mathbb{R}^{d+1}$且$\pmb x ‘ =(1,x_1,\dots,x_d) \in \mathbb{R}^{d+1}$，从而有：$$h_{\omega,b} (\pmb x) = &lt;\pmb{\omega ,x}&gt; + b = &lt;\pmb {\omega’,x’}&gt;$$ 线性规划在线性不等式约束下最大化线性函数，即：$$\max\limits_{\omega \in \mathbb{R}^d} &lt;\pmb {u,\omega}&gt; \ s.t. A\pmb \omega \ge \pmb v$$其中，$\pmb \omega \in \mathbb{R}^d$是希望求解的参数向量，$A$是$m \times d$维矩阵，$\pmb v \in \mathbb{R}^m,\pmb u \in \mathbb{R}^d$为向量。 感知器算法构建一系列的向量$\pmb{\omega^{(1)},\omega^{(2)},\dots}$，初始的$\pmb {\omega^{(1)}} = 0$，在第$t$次迭代时，感知器找到被$\pmb {\omega^{(t)}}$错分的样本$i$，该样本使得${sign}(&lt;\pmb{\omega^{(t)},x_i}&gt;) \ne y_i$，通过将样本$\pmb x_i$乘比例系数$y_i$加入向量，感知器的更新$\pmb \omega^{(t)}$使得$\pmb \omega^{(t+1)} = \pmb \omega ^{(t)} + y_i \pmb x_i$。 目标是对所有的$i$有$y_i \langle\pmb \omega ,\pmb x_i\rangle &gt; 0$且：$$y_i \langle \pmb\omega^{(t+1)},\pmb x_i\rangle = y_i \langle \pmb\omega^{(t)}+y_i \pmb x_i,\pmb x_i\rangle = y_i \langle \pmb\omega^{(t)},\pmb x_i\rangle + ||\pmb x_i||^2$$ 感知器批处理算法 输入：训练集$(\pmb x_1 , y_1),\dots,(\pmb x_m , y_m )$ 初始化：$\pmb \omega^{(1)} = (0,\dots,0)$ 循环： $t= 1,2,\dots $ ​ 如果$(\exists i\quad s.t. \quad y_i \langle \pmb\omega^{(t)},\pmb x_i\rangle\le 0 $，那么 ​ $\pmb \omega^{(t+1)} = \pmb \omega ^{(t)} + y_i \pmb x_i$ ​ 否则 ​ 输出$\pmb \omega^{(t)} $ chapter 10 boosting]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Anaconda教程]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2FAnaconda%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[修改conda 的镜像源（现在不可用） 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 此时，在目录 C:\Users&lt;你的用户名&gt; 下就会生成配置文件.condarc，内容如下： 1234channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: true jupyter notebook使用创建带jupyter notebook内核的虚拟环境： 1conda create --name py37_Pytorch python=3.7 ipykernel 关联和conda的环境和包： conda install nb_conda Markdown生成目录： conda install -c conda-forge jupyter_contrib_nbextensions jupyter notebook使用教程 在虚拟环境下创建kernel文件： 1conda install -n 环境名称 ipykernel 将环境写入jupyter notebook的kernel中 1python -m ipykernel install --user --name 环境名称 --display-name "Python (环境名称)" pip 安装 PyTorch步骤1pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp37-cp37m-win_amd64.whl 安装版本为torch-1.0.0，CPU版本，没有CUDA，python3.x版本。 1pip install --no-deps torchvision 不使用：pip3 install torchvision，会报错。 PyTorch安装 pip和conda区别conda是一种通用包管理系统，是一个与语言无关的跨平台的环境管理器，可以构建和管理任何语言的任何类型的软件。因此适用于python包。 pip代表pip install packages，是python官方认可的包管理器，最常用于安装python包索引（PyPI）上的发布的包。 conda install包，默认是安装在base（anaconda）环境下，conda在指定的环境下安装包： conda install -n env_name pandas。 anaconda是一个python的发行版，conda是一个包管理器。 pip和pip3区别在安装库numpy，pip3 install numpy和pip install numpy命令效果一致的，但当有多个版本的python虚拟环境时，用pip3可以自动区别用python3来安装库文件，避免发生和python2的冲突。 若只安装了python3，pip和pip3是一样的效果，安装python3后，会自动安装pip3，添加scripts到环境变量(避免出现不是内部或外部命令问题)。 若多版本python存在，使用pip install ，新安装的库文件会放在目录：python2.x/site_packages中；使用 pip3 install ，新安装的库文件会放在目录：python3.x/site-packages中。 一些常用命令conda activate env_name激活环境，当前被激活环境前有*号 conda deactivate env_name关闭环境 （Linux下）： source activate/deactivate env_name conda remove -n env_name --all删除环境 conda create --name env_name python=3.x创建一个基于python3.x的名为env_name的虚拟环境，同样正对于python2.x版本。 conda env list查看所有安装的环境 conda install requests安装包 conda install -n env_name numpy安装在指定的环境下的包，若不指定，则是安装在当前活跃环境，也可通过-c指定某个channel来安装。 conda list查看所安装的包 conda update requests包更新 conda update -n env_name packages指定环境下的包更新 conda remove requests删除包 conda list -n env_name查看指定环境下的已安装包 conda search numpy查找package信息 conda将conda、python等都看成packages，都可以通过conda来管理conda和python版本，如： conda update conda更新conda conda update anaconda更新当前环境下的anaconda。 conda update python假设当前环境下的python为3.7版本，会升级为3.7.x系列的最新版本、 Anaconda的bin目录加入PATH，可能有不同的~/anaconadx/bin，： echo &#39;export PATH = &quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc Linux下的添加命令。 运行conda --version检查是否正确。 Anaconda基本使用总结]]></content>
      <categories>
        <category>教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[概率神经网络和粒子群算法]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2F%E6%A6%82%E7%8E%87%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[概率神经网络Specht.D.F根据已有的神经网络理论同时融入概率学的相关知识，提出了概率神经网络的理论。概率神经网络是一种前馈神经网络，充分利用贝叶斯准则理论，能够进行较为准确的数据分类。在贝叶斯决策中，引用了概率密度函数的知识，同时充分结合了贝叶斯最小风险准则，实现了对数据的分类研究。概率神经网络已广泛应用于数据分类、模式识别、图像或语音识别等领域。 概率神经网络参数较少且内部的隐藏层不需要人为的设定，便于构建网络模型。同时使用贝叶斯准则大大提高了模式分类的准确性。在训练数据充足的情况下，能够很好地表达输入数据与输出数据的内在联系，即使在网络工作的过程中需要加入新的训练样本，网络的整体结构亦不需改进。 概率神经网络与BP网络比较： 概率神经网络采用了 贝叶斯最小风险准则，在模式分类上具有较高的正确率，在故障诊断上有明显优势。 概率神经网络结构简单，需要调节的参数较少，更容易收敛。 根据贝叶斯理论，在一般情况下，概率神经网络的收敛值较为固定，达到了对故障知识的充分利用，结果的可信程度较高，而BP网络的输入到输出的过程中，存在不可预知的规则，受主观影响较大，结果存在不稳定的情况。 概率神经网络在工作过程中，增加或删除某些样本，网络的整体结构不会发生过大的变化，仅仅就是对某一层节点数的增加或删除。对于BP网络，增加或减少训练数据后，网络层与层之间的权值很可能发生改变，如不及时进行更新，输出结果很可能会偏离期望值。BP网络的训练数据的改变意味着整个网络的改变。 粒子群算法粒子群算法(Particle Swarm Optimization)属于仿生学的优化算法。核心思想是从人为设定的随机解出发，通过不断的迭代寻找最优的适应度值。PSO没有像遗传算法那样的变异、交叉等复杂过程。 算法原理： 在一个$D$维空间内，种群由$n$个粒子组成$X=(X_1,X_2,\dots,X_n)$，第$i$个粒子表示为1个$D$维的向量$Xi = (X_{i1},X_{i2},\dots,X_{iD})^T$，表示第$i$个粒子在$D$维空间中的位置，即问题的一个潜在解，由目标函数可以求出每个粒子的适应度值。第$i$个粒子的速度为$Vi = (V_{i1},V_{i2},\dots,V_{iD})^T$,其个体极值为$Pi = (P_{i1},P_{i2},\dots,P_{iD})^T$，群体极值为$P_g = (P_{g1},P_{g2},\dots,P_{gD})^T$。 在每次迭代过程中，粒子通过个体极值与群体极值更新自身的速度和位置，即：$$V_{id}^{k+1} = wV_{id}^k +c_1r_1(P_{id}^k - X_{id}^k) +c_2r_2(P_{gd}^k -X_{id}^k)$$ $$X_{id}^{k+1} = X_{id}^k + V_{id}^{k+1}$$ 其中：$i =1,2,\dots,n$，$n$表示群体的粒子数目 ​ $d=1,2,\dots,D$,$D$是解空间维数 ​ $k$表示当前迭代次数 ​ $w$表示惯性权重 ​ $X_{id}^k$为当前粒子在迭代到第$k$次的位置分量 ​ $V_{id}^k$为当前粒子在迭代到第$k$次的速度分量 ​ $P_{id}^k$为当前粒子在迭代到第$k$次的个体历史最优分量 ​ $P_{gd}^k$为当前粒子在迭代到第$k$次的群体历史最优分量 ​ $r_1,r_2$为[0,1]之间的随机数 ​ $c_1,c_2$为非负的常数，称为学习因子。分别调节自身的认知能力和对群体的认知能力，对粒子寻找到最优值有重要作用。 在粒子在寻优过程中可能出现盲目搜索的现象，需要将粒子搜索的速度和位置限定在一定的区间内保证粒子在可行解空间内有目的的搜索。 算法流程： 主要包括：初始化粒子、更新粒子的位置和速度、最优值的确定以及结束情况的判断。 种群初始化。主要是在可行的解空间内设置：种群规模、学习因子、粒子的位置和速度、最大迭代次数、粒子执行搜索的空间范围和速度范围等参数。 适应度函数的建立。结合优化类型确定适应度函数，然后计算每一个粒子当前的适应度值，通过适应度值，确定粒子的群体极值与个体极值 对粒子的位置和速度进行更新 个体极值的确定，对于群体的每一个粒子，将当前适应度值与粒子自身历史适应度值进行比较。若优于历史适应度值，将当前适应度值作为个体极值，反之历史极值仍为个体极值 群体极值的确定。对于群体中的每一个粒子，将当前的适应度值与整个种群的最优适应度值进行比较。若优于群体最优适应度值，则将当前适应度值作为群体极值，反之，原群体极值仍为群体最优适应度值 对迭代是否结束进行判定，若达到预先设定的误差或者达到了最大的迭代次数，循环结束，输出得到最优值，反之，返回2继续执行。 123456789101112st=&gt;start: 开始op0=&gt;operation: 粒子群的初始化op1=&gt;operation: 计算粒子的适应度值op2=&gt;operation: 根据适应度值更新个体的极值和群体极值op3=&gt;operation: 更新粒子的速度和位置cond=&gt;condition: 满足终止条件e=&gt;end: 结束st-&gt;op0-&gt;op1-&gt;op2-&gt;op3-&gt;condcond(yes)-&gt;econd(no) -&gt;op1 粒子群算法在解空间内存有如下特点： 算法无论在初始化还是寻优等方面都有一定的随机性，这样可以使得找到的结果尽可能地接近最优值，利于解决复杂问题。 对初值地设定随机，其变化不影响最终地结果 目标函数是粒子群算法地寻优依据，函数包括多种类型，包括不可微地函数 粒子群算法作为一种群体智能优化算法，过程易理解，适应性强 与GA的“优胜劣汰”不同，粒子群算法不遵循此项规则，从算法开始到结束，种群中的每一个粒子都参与算法寻优的全过程，使粒子群算法能够很好的表达种群的信息，寻优结果要优于遗传算法。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo多终端同步]]></title>
    <url>%2Fblog%2F2020%2F05%2F18%2Fhexo%E5%A4%9A%E7%BB%88%E7%AB%AF%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[准备条件两台终端，其中一台已经被配置hexo+github博客平台（win平台）。 安装好node和hexo，git环境 其中node使用nvm进行安装： 12nvm list #查找所有的node版本nvm use &lt;版本号&gt; #切换使用指定的版本 备份本地博客的文件由于 Github 上保存的只是生成的网页静态文件，因此需要新建一个分支保存本地原始文件，方便在不同的电脑上写博客。需要解决备份问题。 机制是这样的，hexo d上传部署到GitHub的是hexo编译后的文件，用来生成网页的，不包含源文件。即上传的是在本地目录里自动生成的.deploy_git里面。其它文件，包括写在source里面的，和配置文件，主题文件，都没有上传到GitHub上。 所以可以利用git的分支管理，将源文件上传到GitHub的同一仓库的另一个分支即可。 创建两个分支：master 与 source，在博客目录下： 1234567$ cd blog #注意要删除已经存在.git隐藏文件夹$ git init #在当前目录新建一个git代码库$ git add . #添加blog目录所有文件$ git branch hexo #新建一个hexo分支$ git checkout hexo #切换到hexo分支$ git remote add origin git@github.com：yourname/yourname.github.io.git #本地与github项目对接$ git push origin hexo //将新的分支发布到GitHub上 这样GitHub项目的库里会多出一个hexo分支，是用于多终端同步的关键部分。 出现以下错误：error：failed to push some refs to 解决方法：git pull --rebase origin master。把远程库中的更新合并到本地库中。 在另一个终端中更新博客，只需要将GitHub上source分支clone下来，进行初次相关配置。 12345678910111213141516171819202122232425# 将Github中hexo分支clone到本地$ git clone -b hexo git@github.com:yourname/yourname.github.io.git/jiangxjun.github.io $ git checkout -b hexo #cheackout 远程代码到本地hexo分支# 注意，这里一定要切换到刚刚clone的文件夹内执行$ git init# 新建一个.md文件，并编辑完成自己的博客内容$ hexo new post "new blog name"# 经测试每次只要更新sorcerer中的文件到Github中即可，因为只是新建了一篇新博客/jiangxjun.github.io/hexo $ git add source$ touch README$ git add README$ git commit -m "update blog"$ git remote add origin git@github.com：yourname/yourname.github.io.git# 更新分支$ git push -u origin +hexo# push更新完分支之后将自己写的博客对接到自己搭的博客网站上，同时同步了Github中的master#出现更新被拒绝，当前分支的最新提交落后于其对应的远程分支$ git fetch origin #获取远程更新$ git merge origin/master # 把更新内容合并到本地分支$ git push -u origin +hexo 遇到的坑在github上下载源代码时候，遇到git clone命令出现错误： 1234567Permissiondenied (publickey).fatal:Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 原因是：本地（或服务器上）没有生成ssh key。解决办法如下： 12(base) jiang@jiang-X299:~$ cd ~/.ssh lsls #查看是否有文件id_rsa以及文件id_rsa.pub 没有，则终端命令如下： 12345678910111213141516171819202122 ssh-keygen -t rsa -C "your_email@example.com" #修改为自己的邮箱，途中需要输入密码，不管，一路回车即可，即生成你的ssh key ssh -v git@github.com #最后会出现No more authentication methods to try. Permission denied (publickey) ssh-agent -s #输入该命令，会出现SSH_AUTH_SOCK=/tmp/ssh-a94vS6DUEnSA/agent.14751; export SSH_AUTH_SOCK; SSH_AGENT_PID=14752; export SSH_AGENT_PID; echo Agent pid 14752;ssh-add ~/.ssh/id_rsa #输入该命令，会出现identity added:(一些ssh key文件路径的信息) （注意）如果出现错误提示：Could not open a connection to your authentication agent.请执行命令：eval `ssh-agent -s`后继续执行命令 ssh-add ~/.ssh/id_rsa然后打开你刚刚生成的id_rsa.pub，将里面的内容复制，进入你的github账号，在settings下，SSH and GPG keys下new SSH key，title随便取一个名字，然后将id_rsa.pub里的内容复制到Key中，完成后Add SSH Key。最后一步，验证Key 在ternimal下输入命令： ssh -T git@github.com 提示：Hi xxx! You've successfully authenticated, but GitHub does not provide shell access.]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu使用教程]]></title>
    <url>%2Fblog%2F2020%2F05%2F18%2Fubuntu%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Ubuntu磁盘操作需要root权限 12sudo susu *** 用命令fdisk -l查看系统识别磁盘情况 12345678910111213Device Boot Start End Sectors Size Id Type/dev/sda1 * 2048 960335871 960333824 457.9G 83 Linux/dev/sda2 960337918 976771071 16433154 7.9G 5 ExtendedDisk /dev/sdb: 2.7 TiB, 3000592982016 bytes, 5860533168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: gptDisk identifier: 1AB82561-C5FD-47A9-9A29-31835617EBD3Device Start End Sectors Size Type/dev/sdb1 2048 5860532223 5860530176 2.7T Microsoft basic data 可以看到磁盘的挂载点，然后cd切换到对应的磁盘，访问。 跟磁盘相关的几个常用命令： df -h：查看磁盘占用情况 df -T：查看磁盘的文件系统类型(type) fdisk -l：查看所有被系统识别的磁盘 ubuntu +hexo +github搭建博客1、安装git 2、安装node.js 为了避免安装的 Node.js 版本过旧导致后续的 Hexo 安装过程出错，应该使用 NVM（Node Version Manager）安装 Node.js。官方给出了两个安装脚本，直接复制到命令行即可： 1$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash 1$ wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash 安装完成后，重启终端使用。 123$ nvm install stable #安装 nvm完成安装。安装完成后可使用npm -v 环境配置： 123456# 编辑vim /etc/profile# 在底部添加 PATH 变量export PATH=$PATH:/usr/local/node/bin# 保存生效source etc/profile 具体教程参考此文。 注意node版本和hexo版本匹配问题，如果node版本过高，执行hexo g -d部署会出现问题。 3、安装多版本node/npm 1nvm install 10.15.3 #安装10.15.3版本 具体参考此文 4、修改hexo版本 update hexo in package.json 12345//package.json"dependencies":&#123; "hexo": "^4.2.0" #删除原版本 "hexo": "^4.2.1" #输入新版本&#125; 12npm updatehexo -v]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迁移学习和深度迁移学习原理]]></title>
    <url>%2Fblog%2F2020%2F05%2F18%2F%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[迁移学习 S. J. Pan and Q. Yang, “A Survey on Transfer Learning“ in IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345-1359 迁移学习概念传统机器学习的领域假设训练数据和测试数据属于相同的特征空间并在同一分布，然而现实中这种假设往往得不到满足。例如，我们对目标领域的分类问题，却只有源领域的训练数据，但源领域数据与目标领域数据要么不在同一个特征空间，要么不满足相同的数据分布，例如需要进行的文本分类语言是西班牙语。但只提供了了葡萄牙语的文本。 在某些情况下成功地进行知识迁移能够很大程度上提高学习的性能，但也同时降低了标记目标领域数据带来的大量时间和人力成本。 近年来，迁移学习已经成为一种解决知识迁移问题的新型学习框架，这篇论文【S. J. Pan and Q. Yang, “A Survey on Transfer Learning,” in IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345-1359】讨论了使用迁移学习进行分类、回归和聚类的一般过程，也讨论了迁移学习和其他相关的机器学习技术之间的关系，如领域适应性、多任务学习、样本选择和变量变换。 传统的机器学习/数据挖掘只有在训练集和测试集数据都来自同一个feature space和the same distribution的时候才有较好的表现，也就意味着如果每一次更换了数据集重新训练模型，会变得很麻烦。比如： （1）从数据类型/内容上看，获取新的数据集很贵很麻烦 （2）从时间维度上看，有些数据很容易过期，不同时期的数据分布会不同。对于每个时间段都要进行一次训练很麻烦（如进行室内wifi定位） 2005年DARPA的信息处理办公室(IPTO)给迁移学习有了一个新的定义：一个系统具有把从先前的任务学习到的识别和应用知识的能力运用到新的任务上的能力。 区别于多任务学习(multi-task learning)中将source和target tasks同等对待，迁移学习更多的关注target tasks，在迁移学习中，两者不再对等。 几个概念几个关键词：domain（域）和task（任务），source（源）和target（目标） domain包括两部分：feature space和probability（概率）。domain不同可能分为两种情况，特征空间不同或者概率不同。 task包括两部分：label space和objective predictive function（目标预测函数）。同样如上所述。 source：用于训练模型的域/任务。 target：使用基于source训练出来的模型对自己的数据进行预测/分类/聚类等机器学习任务的域/任务。 领域（domain）$\mathcal{D}$包括特征空间$\mathcal{X}$和边缘分布（marginal probability distribution）$P(X)$，$X = \{x_1,x_2,\dots,x_n\} \in \mathcal{X}$。即：$\mathcal{D}= \{\mathcal{X},P(X)\}$ 任务（task）$\mathcal{T}$包括标签空间$\mathcal{Y}$和目标预测函数$f(·)$，也就是条件概率分布，记作$P(y|x)$。即：$\mathcal{T} = \{\mathcal{Y},f(·)\}$ 迁移学习定义：给出一个源领域$\mathcal{T}_S$和学习任务$\mathcal{T}_S$，一个目标领域$\mathcal{D}_T$和学习任务$\mathcal{T}_T$，迁移学习指的是通过从$\mathcal{D}_S$和$\mathcal{T}_S$学到的知识了来帮助在$\mathcal{D}_T$中目标函数$f_T(·)$的学习。$\mathcal{D}_T \neq \mathcal{D}_S$或者$\mathcal{T}_T \neq \mathcal{T}_S$ 源领域和目标领域不同指的是$\mathcal{X}_S \neq \mathcal{X}_T$【即特征空间不同】或者$P_S(X) \neq P_T(X)$【即源和目标任务的边缘分布不同】 源任务和目标任务不同指的是$\mathcal{Y}_S \neq \mathcal{Y}_T $【即任务标签集不同】或者$P(Y_S |X_S) \neq P(Y_T | X_T)$【即源和目标任务的条件概率不同】 迁移学习的划分①从问题角度来看（1）迁移什么？ 哪一部分知识可以被迁移？ （2）怎么迁移？ 那当然就是训练出适合的模型啦。 （3）什么时候需要用到迁移学习？ 当source domain和target domain没什么关系或者太不相同的时候，迁移效果可能就不那么好了，甚至可能会比不迁移的时候表现要更差，这个就叫做negative transfer了。 可以看到，迁移学习的能力也是有限的，所以我们需要关注迁移学习的边界在哪里，比如用conditional Kolmogorov complexity去衡量tasks之间的相关性。 作者指出，现在很多工作都关注前两个问题，但实际上第三个问题是很重要的，因为你在那捣腾半天最后发现其实迁移了还不如不迁移，那不是白费心思嘛。所以作者认为这个问题应当被重视，比如说可以在迁移之前先看看source和domain之间的transferability（可迁移性）。 ②从迁移场景来看（1）Homogeneous TL（同构学习）：source domain和target domain的feature space相同。 （2）Heterogeneous TL（异构学习）：source domain和target domain的feature space不同。 ③从迁移算法来看（1）Inductive TL（归纳式迁移学习） source和target的domain可能一样或不一样，task不一样；target domain的labeled数据可得，source domain不一定可得。 所以呢，根据source domain的labeled数据可以再细分为两类： multi_task learning（多任务学习）：source domain的labeled数据可得。self-taught learning（自学习）：source domain的labeled数据不可得。 代表性论文引用：Dai W, Yang Q,Xue G R, et al. Boosting for transfer learning[C]//Machine Learning, Proceedings of the Twenty-Fourth International Conference. DBLP, 2007:193-200. （2）Transductive TL（直推式迁移学习） source和target的task一样，domain不一样；source domain的labeled数据可得，target domain的不可得。注意我们提过，domain不一样意味着两种可能：feature space不一样，或者feature space一样而probability不一样。而后一种情况和domain adaptation（域适配）息息相关。这里也可以根据domain和task的个数分为两个情况： Domain Adaptation（域适配）：不同的domains+single taskSample Selection Bias（样本选择偏差）/Covariance Shift（协方差转变）：single domain+single task 代表性论文引用：Arnold A, Nallapati R, Cohen W W. A comparativestudy of methods for transductive transfer learning[C]//Data Mining Workshops,2007. ICDM Workshops 2007. Seventh IEEE International Conference on. IEEE,2007: 77-82. （3）Unsupervised TL（无监督迁移学习） source和target的domain和task都不一样；source domain和target domain的labeled数据都不可得。 代表性论文引用：Dai W, Yang Q, Xue G R, et al. Self-taughtclustering[C]//Proceedings of the 25th international conference on Machinelearning. ACM, 2008: 200-207. 将迁移学习根据领域和任务的不同进行了划分： 综上，这几个方法差别主要是：（1）source和domain之间，domain是否相同，task是否相同；（2）source domain和target domain的labeled数据是否可以得到。 ④从“迁移什么”来看 （1）Instance-based TL（样本迁移） 尽管source domain数据不可以整个直接被用到target domain里，但是在source domain中还是找到一些可以重新被用到target domain中的数据。对它们调整权重，使它能与target domain中的数据匹配之后可以进行迁移。盗一张图，比如在这个例子中就是找到例子3，然后加重它的权值，这样在预测的时候它所占权重较大，预测也可以更准确。 instance reweighting（样本重新调整权重）和importance sampling（重要性采样）是instance-based TL里主要用到的两项技术。 （2）Feature-representation-transfer（特征迁移） 找到一些好的有代表性的特征，通过特征变换把source domain和target domain的特征变换到同样的空间，使得这个空间中source domain和target domain的数据具有相同的分布，然后进行传统的机器学习就可以了。 特征变换这一块可以举个栗子，比如评论男生的时候，你会说”好帅！好有男人味！好有担当！“；评论女生的时候，你会说”好漂亮！好有女人味！好温柔！“可以看出共同的特征就是“好看”。把“好帅”映射到“好看”，把“好漂亮”映射到“好看”，“好看”便是它们的共同特征。 （3）Parameter-transfer（参数/模型迁移） 假设source tasks和target tasks之间共享一些参数，或者共享模型hyperparameters（超参数）的先验分布。这样把原来的模型迁移到新的domain时，也可以达到不错的精度。 下面这个项目感觉用到就是这个parameter-transfer：基于深度学习和迁移学习的识花实践。 （4）Relational-knowledge-transfer（关系迁移） 把相似的关系进行迁移，比如生物病毒传播到计算机病毒传播的迁移，比如师生关系到上司下属关系的迁移。 具体方法概述即上表的扩展分析： ①Inductive TL （1）Instances TL 主要方法：TrAdaBoost（AdaBoost的拓展） 假设：source domain和target domain数据的feature和labels是一样的，但是分布不一样；部分source domain的数据会对target domain的学习有帮助，但有部分可能会不利于target domain的学习。 过程：大致就是不断地给好的source data赋予更高的权重，给不好的赋予更多的权重。 （2）Features TL 需要根据source domain的labeled data是否可得分为两类（回顾：Inductive TL是target domain的labeled data可得，但是source domain的未必可得）： Supervised Feature Construction（监督的特征构建） Unsupervised Feature Construction（非监督的特征构建） 大致过程：通过减少model error，找出低维的有代表性的特征 （3）Parameters TL 主要方法：MT-IVM（基于Gaussian Processes） 大致过程（注意这里讲述的不是上面提到的那个主要方法的过程，而是另一个方法的过程）：假设source和target的参数都可以分为两部分，一部分是source/target特有的参数，一部分是它们共同有的参数。把这两个参数丢到改进了的SVM问题中，把参数训练出来就好了。 （4）Relational TL 注意和上面三种方法不同的是，这个方法是在relational domains里进行的，这个domain里的数据不是iid（独立同分布）的，所以它不需要假设每个domain里的数据都必须iid。 主要方法：statistical relational learning（SRL，统计关系学习） ②Transductive TL Transductive TL是source domain的label可得，target domain的label不可得。但是要注意！为了得到target data的边际分布，在training的时候是需要一些unlabeled的target data的。 （1）Instances TL 主要方法：Importance sampling。 大致过程：我们的目标是最小化target domain里的expected risk（期望风险），但是target domain里没有labeled数据可用，所以我们必须替换成source domain里的数据，通过一些方法可以把它替换成source domain里的数据再乘以一个权重，只要把这个权重算出来就好。 （2）Feature Representations TL 主要方法：Structural Correspondence Learning（SCL） 大致过程：定义一些pivot features（就是共同特征），然后把每一个pivot feature都当成是一个新的label vector，通过公式把权重学习出来，然后对权重进行SVD分解，最后在argumented feature vector上使用传统的判别式算法即可。这里argumented feature vector包括这些新的features和所有原来的feature。（我还没有详细看这一块儿，所以先直译了） 难点：如何寻找好的pivot feature、domain之间的依赖性。 ③ Unsupervised TL （1）Feature Representations TL 主要方法：涉及两个 Self-taught clustering（STC），主要用于transfer clustering（迁移聚类）。目标就是希望通过source domain里大量的unlabeled data对少量的target domain里的unlabeled data进行聚类。TDA方法，这个主要用于解决transfer dimensionality reduction（迁移降维）问题 目前存在的问题（1）negative transfer的问题，比如怎么定义transferability，怎么衡量domain之间或task之间的相关性。 （2）目前的TL算法主要都是想要提高feature space相同probability不同时的表现的，domain不同或者task不同都有两种情况，一种是space不同，一种是space相同probability不同 ，但是这里说的就是目前TL算法主要致力于提高的都是概率不同的。但是很多时候我们其实也想要对feature space不同的domain和task进行迁移。即提到的heterogeneous TL（异构学习）问题。 （3）现在的TL主要都是应用到小且波动不大的数据集中（例如传感器数据、文本分类、图片分类等），以后要考虑如何用到更广泛的数据场景中。 迁移学习的基本过程当可用的数据集特别少时，从头开始训练一个神经网络往往不能得到很好的结果，于是就从一个预训练模型开始训练，让网络本身已经具备一定的训练基础，然后用小数据集进行微调，便可以得到一个不错的结果。 通常加载预训练后，冻结模型的部分参数，一般只训练模型的最后几层，这样可以保留整个模型前面对物体特征提取的能力。预训练模型一定要与新的数据集有共同点，这样才能有效地预训练模型里的特征提取能力迁移到新的模型上。 一般过程： 1、加载预训练模型 2、冻结模型前面部分的参数 3、添加可训练的自定义的分类层，或使用原模型的分类层（如果可重用的话） 4、在新数据集上训练 准备数据集按照50%，25%，25%的比例划分training，valid，和testing。 目录整理如下： 1234567891011/datadir /train /class1 /class2 /valid 验证集 /class1 /class2 /test /class1 /class2 预训练模型是基于ImageNet的，训练图像大小224×224，需要对数据集的图像进行大小缩放。 图像增广(data augmentation)图像增广一般用来人工产生不同的图像，比如对图像进行旋转、翻转、随机裁剪、缩放等等。选择训练阶段对输入进行增广。 图像预处理定义training和validation的预处理方式。 定义dataset和dataloader。用datasets.imagesfolder来定义dataset时，pytorch可以自动将图片与对应的文件夹分类对应起来，应用上面定义好的transformers，然后dataset传入到dataloader里，dataloader在每一个循环会自动生成batchsize大小的图像和label。 ImageNet的预训练模型PyTorch自带了很多ImageNet上的预训练模型 PyTorch迁移学习实现图像分类 PyTorch 中文网 深度迁移学习 论文：A Survey on Deep Transfer Learning 清华大学智能技术与系统国家重点实验室近期发表的深度迁移学习综述，首次定义了深度迁移学习的四个分类，包括基于实例、映射、网络和对抗的迁移学习方法，并在每个方向上都给出了丰富的参考文献。 参考 回顾迁移学习迁移学习：解决训练数据不足这一基本问题，试图放松训练数据和测试数据必须是独立同分布(i.i.d)的假设，将知识从源域迁移到目标域。 给出一个源领域$\mathcal{T}_S$和学习任务$\mathcal{T}_S$，一个目标领域$\mathcal{D}_T$和学习任务$\mathcal{T}_T$，迁移学习指的是通过从$\mathcal{D}_S$和$\mathcal{T}_S$学到的知识了来帮助在$\mathcal{D}_T$中目标函数$f_T(·)$的学习。$\mathcal{D}_T \neq \mathcal{D}_S$或者$\mathcal{T}_T \neq \mathcal{T}_S$ 深度迁移学习如何利用深度神经网络进行有效的知识传递很重要，即深度迁移学习。定义如下： 深度迁移学习定义：给定一个迁移学习任务，定义为$\left(\mathcal{D}_S,\mathcal{T}_S,\mathcal{D}_T,\mathcal{T}_T,f_{\mathcal{T}}(·)\right)$。其中$f_{\mathcal{T}}(·)$是一个反映深度神经网络的非线性函数。 深度迁移学习分类：基于实例的深度迁移学习、基于映射的深度迁移学习、基于网络的深度迁移学习和基于对抗的深度迁移学习。 基于实例的深度迁移学习使用特定的权重调整策略，从源域中选择部分实例作为目标域训练集的补充，并为这些选择的实例分配合适的权值。 是基于这样的假设：“虽然两个域之间存在差异，但是源域中的部分实例可以被具有适当权重的目标域使用”。 基于映射的深度迁移学习将实例从源域和目标域映射到新的数据空间，在这个新的数据空间中，来自两个域的实例是相似的，适合于联合深度神经网络。 是基于这样的假设：“尽管两个源域之间存在差异，但它们在一个复杂的新数据空间中可能更加相似”。 基于网络的深度迁移学习将源领域中预先训练好的部分网络，包括其网络结构和连接参数，重新利用，将其转化为用于目标领域的深度神经网络的一部分。 是基于这样的假设：“神经网络类似于人脑的处理机制，是一个迭代的、连续的抽象过程”。该网络的前端层可以看作是一个特征提取器，所提取的特征是通用的。 基于对抗的深度迁移学习在生成对抗网络(GAN)的启发下，引入对抗技术，寻找既适合于源域又适用于目标域的可迁移表达。 是基于这样的假设：“为了有效的迁移，良好的表征应该是对主要学习任务的区别性，以及对源域和目标域的不加区分”。 更详细的中文描述]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>迁移学习</tag>
        <tag>深度迁移学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马尔科夫链-蒙特卡罗方法]]></title>
    <url>%2Fblog%2F2020%2F05%2F16%2F%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[MCMC(markov chain monte carlo)马尔科夫链蒙特卡洛随机采样方法，是很多复杂算法求解的基础，用作一些复杂运算的近似求解。 蒙特卡洛方法部分概述 通过大量的随机样本，去了解一个系统，进而得到所要计算的值。是以概率为基础的方法，与之对应的是确定性算法。 基本思想 ​ 所求解的问题是某随机事件A出现的概率（或者是某随机变量B的期望值）。通过某种“实验方法”，得出A事件出现的概率，以此估计出A事件出现的概率（或者得到随机变量B的某些数字特征，得出B的期望值）。 工作过程 ​ Monte Carlo方法模拟某一个过程时，需要产生各种概率分布的随机变量，用统计方法把模拟的数字特征估计出来，从而得到实际问题的数值解。 分类 一类是所求解的问题具有内在的随机性，借助计算机的运算能力可以直接模拟这种随机过程。如分析中子在反应堆中的传输过程，依据其概率随机抽样得到裂变位置、速度和方向，模拟大量中子的行为后，经过统计得到中子传输的范围，作为堆设计的依据。 一类是所求解问题可以转化为某种随机分布的特征数，如随机事件发生的概率，或者随机变量的期望值，通过随机抽样，以随机事件出现的频率估计其概率，或者以抽样的数字特征估算随机变量的数字特征，作为问题的解。多用于求解复杂的多维积分问题。 举例 求解积分：$ \theta = \int_a^b f(x)dx $ 一个简单近似求解方法是在$[a,b]$之间随机采样一个点，如$x_0$，然后用$f(x_0)$代表在这个区间上所有的$f(x)$的值，近似求解为：$$(b-a)f(x_0)$$在区间$[a,b]$上采样n个值：$x_0,x_1,\dots,x_{n-1}$，用其均值来代表区间上所有的$f(x)$的值，近似求解为：$$\frac{b-a}{n}\sum\limits_{i=0}^{n-1}f(x_i)$$上述是在假设$x$是在$[a,b]$上均匀分布的情况下，若不是均匀分布，则可以采用：如果得到$x$在$[a,b]$上的概率分布函数$p(x)$，有$$\theta = \int_{a}^b f(x)dx = \int_a^b \frac{f(x)}{p(x)} p(x)dx \approx \frac{1}{n}\sum\limits_{i=0}^{n-1}\frac{f(x_i)}{p(x_i)}$$上式最右边为蒙特卡洛方法的一般形式，此处是连续函数形式的。 可见，当假设$x$在区间内均匀分布时候，即：$p(x_i) = 1/(b-a)$，带入上式可以得到：$$\frac{1}{n}\sum\limits_{i=0}^{n-1}\frac{f(x_i)}{1/(b-a)} =\frac{b-a}{n}\sum\limits_{i=0}^{n-1}f(x_i)$$概率分布采样 如何基于概率分布去采样基于该概率分布的$n$个$x$样本集问题： 对于常见的均匀分布uniform（0，1）是非常容易采样的，一般是通过线性同余发生器可以生成(0,1)之间的伪随机数样本。 其他常见的概率分布，可以通过uniform(0,1)的样本转换得到，如二维的正太分布样本$(Z_1,Z_2)$可以通过独立采样得到uniform(0,1)样本对$(X_1,X_2)$。 其他的常见的连续分布，如$t$分布，$F$分布等，都可以通过类似得方式转换为uniform(0,1)得到的采样样本转化得到，在python中的numpy，scikit-learn等类库中，有生成这些常用分布样本的函数可以使用。 接受-拒绝采样 对于不常见的概率分布，一个可行的解决办法是：接受——拒绝采样。 设定一个程序可采样的分布$q(x)$，如高斯分布，然后按照一定的方法拒绝某些样本，以达到接近$p(x)$分布的目的，其中$q(x)$成为proposal distribution。 具体采用过程为： 设定一个方便采样的常用概率分布函数$q(x)$,以及一个常量$k$，使得$p(x)$总在$kq(x)$的下方。 采样得到的$q(x)$的一个样本$z_0$，然后，从均匀分布$(0,kq(z_0))$中采样得到一个值$u$，若$u$落在上图中的灰色区域，则拒绝抽样，否则接受这个样本$z_0$，重复以上过程得到$n$个接受的样本$z_0,z_1,\dots,z_{n-1}$，最后的蒙特卡罗方法求解结果为：$$\frac{1}{n}\sum\limits_{i=0}^{n-1}\frac{f(x_i)}{p(x_i)}$$通过一系列的接受拒绝决策来达到用$q(x)$模拟$p(x)$概率分布。 马尔科夫链部分概述 假设某一个时刻状态转移的概率只依赖于它的前一个状态，这样做可以大大简化模型的复杂度，因此马尔科夫链在很多时间序列模型中得到广泛的应用，如循环神经网络RNN，隐式马尔科夫链HMM等。 数学定义描述为： 假设序列状态是$\dots X_{t-2}, X_{t-1},X_{t},X_{t+1},\dots$，那么在时刻$X_{t+1}$的状态的条件概率仅仅依赖于时刻$X_t$，即：$$P(X_{t+1} | \dots X_{t-2}, X_{t-1},X_{t})= P(X_{t+1}|X_t)$$因为某一时刻状态转移的概率只依赖于它的前一个状态，因而只能求出系统中任意两个状态之间的转换概率。 举例 表示股市模型，共三种状态：牛市、熊市和横盘(stagnant market)。每一个状态都以一定的概率转化到下一个状态。如，牛市以0.025概率转化为横盘的状态。 上图可以表示为矩阵的形式，如果定义矩阵$P$某一个位置$P(i,j)$的值为$P(j|i)$，即从状态$i$转化为状态$j$的概率，定义牛市状态为0，熊市状态为1，横盘为2，得到马尔科夫链模型的状态转移矩阵为：$$P = \pmatrix{0.9 &amp; 0.075&amp;0.025 \ 0.15 &amp; 0.8 &amp; 0.05 \ 0.25 &amp; 0.25 &amp; 0.5}$$状态转移矩阵的性质 马尔科夫链模型的状态转移矩阵收敛到稳定概率分布与初始状态概率分布无关，即 得到稳定分布对应的马尔科夫链模型的状态转移矩阵，可以用任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，经过一系列的转换，最终可以得到符合对应稳定概率分布的样本。 对于一个确定的状态转移矩阵$P$，$P^n$在$n$大于一定的值的时候也可以发现是确定的。 马尔科夫链性质数学描述： 如果一个非周期的马尔科夫链有状态转移矩阵$P$，并且它的任意两个状态是连通的，$\lim \limits_{n \rightarrow \infty} P_{ij}^n$与$i$无关，有： $\lim \limits_{n \rightarrow \infty} P_{ij}^n =\pi(j)$ $\lim\limits_{n \rightarrow \infty} P^n = \pmatrix{\pi(1) &amp; \pi(2) &amp; \dots &amp; \pi(j) &amp; \dots \ \pi(1) &amp; \pi(2) &amp; \dots &amp; \pi(j) &amp; \dots \ \dots \ \pi(1) &amp; \pi(2) &amp; \dots &amp; \pi(j) &amp; \dots \ \dots}$ $\pi(j) = \sum\limits_{i=0}^{\infty}\pi(i) P_{ij}$ $\pi$是方程$\pi P = \pi$的唯一非负解，其中：$\pi = [\pi(1),\pi(2),\dots,\pi(j), \dots]\sum\limits_{i=0}^{\infty}\pi(i) =1$ （1）非周期性的马尔科夫链：这个主要是指马尔科夫链的状态转化不是循环的，如果是循环的则永远不会收敛。对于任意某一个状态$i,d$为集合$\{n|n \ge 1, P_{ii}^n &gt; 0\}$的最大公约数，如果$d=1$，则状态为非周期的。 （2）任何两个状态是连通的：从任意一个状态可以通过有限步到达其他的任意一个状态，不会出现条件概率一直为0导致不可达的情况。 （3）马尔科夫链的状态数是有限的，也可以是无限的，可以用于连续概率分布和离散概率分布 （4）$\pi$通常是马尔科夫链的平稳分布 基于马尔科夫链采样 得到了某个平稳分布所对应的马尔科夫链状态转移矩阵，很容易采样出这个平稳分布的样本集。 假设任意初始的概率分布为$\pi_0(x)$，经过第一轮马尔科夫链状态转移后的概率分布是$\pi_1(x)$，第$i$轮的概率分布是$\pi_i(x)$。经过$n$轮后的马尔科夫链收敛到平稳分布$\pi(x)$，即：$$\pi_n(x) = \pi_{n+1}(x)= \dots = \pi(x)$$对于每个分布$\pi_i(x)$，有：$$\pi_i(x) = \pi_{i-1}(x)P = \pi_{i-2}(x)P^2 = \pi_0(x)P^{i}$$采样过程： 首先，基于初始简单概率分布（如高斯分布）$\pi_0(x)$采样得到状态值$\pi_0$，基于条件概率分布$P(x|x_0)$采样状态值$x_1$，一直进行下去，当状态转移进行到一定的次数时，如$n$次，认为此时的样本集$(x_n,x_{n+1},x_{n+2},\dots)$即是符合平稳分布的对应的样本集，可以用来做蒙特卡洛模拟求和。 输入马尔科夫链状态转移矩阵P，设定状态转移次数阈值$n_1$，需要样本个数$n_2$ 从任意简答概率分布采样得到初始状态值$x_0$ for $t=0$ to $n_1+n_2 -1$：从条件概率分布$P(x|x_t)$中采样得到样本$x_{t+1}$样本集$(x_{n_1},x_{n_1+1},\dots,x_{n_1+n_2-1})$即为需要的平稳分布对应的样本集。 问题：随意给定一个平稳分布$\pi$，如何得到它所对应的马尔科夫链状态转移矩阵$P$。 解决办法：MCMC采样和其易用版M-H采样。 MCMC采样和M-H采样马尔科夫链的细致平稳条件 定义： 如果非周期的马尔科夫链的状态转移矩阵P和概率分布$\pi(x)$对于所有的$i,j$满足：$$\pi(i)P(i,j) = \pi(j)P(i,j)$$称概率分布$\pi(x)$是状态转移矩阵$P$的平稳分布。其中，$P(i,j)$表示从状态$i$到状态$j$的转化概率。 证明：$$\sum\limits_{i=1}^{\infty}\pi(i)P(i,j) = \sum\limits_{i=1}^{\infty}\pi(j)P(i,j) =\pi(j)\sum\limits_{i=1}^{\infty}P(j,i)=\pi(j)$$写成矩阵形式为：$$\pi P = \pi$$满足马尔科夫链的收敛性质，即只要找打可以使得概率分布$\pi(x)$满足细致平稳分布的矩阵P即可。 MCMC采样 一般情况下，目标平稳分布$\pi(x)$和某一个马尔科夫链状态转移矩阵Q不满足细致平稳条件，即：$$\pi(i)Q(i,j) \ne \pi(j)Q(j,i)$$引入$\alpha(i,j)$，使得上式成立，即：$$\pi(i)Q(i,j)\alpha(i,j) = \pi(j)Q(j,i)\alpha(j,i)$$其中：$$\alpha(i,j) = \pi(j)Q(j,i)$$ $$\alpha(j,i) = \pi(i)Q(i,j)$$ 得到分布$\pi(x)$对应的马尔科夫链状态转移矩阵P，满足：$$P(i,j) = Q(i,j)\alpha(i,j)$$即：目标矩阵P可以通过任意一个马尔科夫链状态转移矩阵Q乘以$\alpha(i,j)$得到，$\alpha(i,j)$称为接受率。取值$[0,1]$之间，相当于一个概率值。 MCMC采样过程： 输入任意选定的马尔科夫链状态转移矩阵Q，平稳分布$\pi(x)$，设定状态转移次数阈值$n_1$，需要样本个数$n_2$ 从任意简单概率分布采样得到初始状态值$x_0$ for $t=0$ to $n_1+n_2-1$: (a)从条件概率分布$Q(x|x_t)$中采样得到样本$x_*$ (b)从均匀分布采样u~uniform[0,1] (c)如果$u &lt; \alpha(x_t,x_) = \pi(x_)Q(x_,x_t)$，则接受转移$x_t \rightarrow x_$，即$x_{t+1} = x_*$ (d)否则不接受转移，即$x_{t+1} = x_t$ 样本集$(x_{n_1}，x_{n_1+1},\dots,x_{n_1+n_2-1})$即为所需要的平稳分布对应的样本集。 问题：(c)步骤中，采样接受率$\alpha(x_t,x_*)$可能非常小，如0.1，导致大部分的采样值都被拒绝转移，采样效率很低，有可能采样百万次马尔科夫链还没有收敛，即$n_1$阈值次数需要设置的非常大。此时需要M-H采样。 解决办法： M-H采样，解决MCMC采样接受率过低问题 M-H采样 Metropolis-Hastings采样的简称。 对接受率改造如下：$$\alpha(i,j) = \min \left\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1 \right\}$$M-H采样过程： 输入任意选定的马尔科夫链状态转移矩阵Q，平稳分布$\pi(x)$，设定状态转移次数阈值$n_1$，需要样本个数$n_2$ 从任意简单概率分布采样得到初始状态值$x_0$ for $t=0$ to $n_1+n_2-1$: (a)从条件概率分布$Q(x|x_t)$中采样得到样本$x_*$ (b)从均匀分布采样u~uniform[0,1] (c)如果$u &lt; \alpha(x_t,x_) =\min \left\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1 \right\} $，则接受转移$x_t \rightarrow x_$，即$x_{t+1} = x_*$ (d)否则不接受转移，即$x_{t+1} = x_t$ 样本集$(x_{n_1}，x_{n_1+1},\dots,x_{n_1+n_2-1})$即为所需要的平稳分布对应的样本集。 如果选择的马尔科夫链状态转移矩阵$Q$是对称的，即满足$Q(i,j) = Q(j,i)$，此时，接受率可以进一步简化为：$$\alpha(i,j) = \min \left\{ \frac{\pi(j)}{\pi(i)},1 \right\}$$问题： （1）大数据时代，数据特征非常多，M-H采样由于接受率计算式$\frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)}$的存在，在高维需要的计算时间非常可观，算法效率低，同时，由于接受率小于1，计算得到的结果会被拒绝。 （2）特征维度大，很多时候很难求出目标的各特征维度联合分布，但是可以方便的求出各个特征之间的条件概率分布，需要考虑的是 能否只有各维度之间条件概率分布的情况下方便的采样。 解决办法： Gibbs采样。 Gibbs采样重新寻找合适的细致平稳条件 假设$\pi(x_1,x_2)$是一个二维联合分布，观察第一个特征维度相同的两个点$A(x_1^{(1)},x_2^{(2)}),B(x_1^{(1)},x_2^{(2)})$，有：$$\pi(x_1^{(1)},x_2^{(1)})\pi(x_2^{(2)}|x_1^{(1)}) = \pi(x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)})\pi(x_2^{(2)}|x_1^{(1)})$$ $$\pi(x_1^{(1)},x_2^{(2)})\pi(x_2^{(1)}|x_1^{(1)}) = \pi(x_1^{(1)})\pi(x_2^{(2)}|x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)})$$ 根据条件概率公式可得。 右侧相等，可得：$$\pi(x_1^{(1)},x_2^{(1)})\pi(x_2^{(2)}|x_1^{(1)}) =\pi(x_1^{(1)},x_2^{(2)})\pi(x_2^{(1)}|x_1^{(1)})$$ $$\pi(A)\pi(x_2^{(2)}|x_1^{(1)}) =\pi(B)\pi(x_2^{(1)}|x_1^{(1)})$$ 如果非周期的马尔科夫链的状态转移矩阵P和概率分布$\pi(x)$对于所有的$i,j$满足：$$\pi(i)P(i,j) = \pi(j)P(i,j)$$称概率分布$\pi(x)$是状态转移矩阵$P$的平稳分布。其中，$P(i,j)$表示从状态$i$到状态$j$的转化概率。 对比可见，在$x_1 = x_1^{(1)}$这条直线上，如果用条件概率分布$\pi(x_2|x_1^{(1)})$作为马尔科夫链的状态转移概率，则任意两点之间的转移满足细致平稳条件。同理，在直线$x_2 = x_2^{(1)}$上。 构造新的满足细致平稳条件：$$\pi(E) P(E \rightarrow F) = \pi(F) P(F \rightarrow E)$$二维Gibbs采样 需要两个维度之间的条件概率，具体过程如下： (1)输入平稳分布$\pi(x_1,x_2)$，设定状态转移次数阈值$n_1$，需要样本个数$n_2$ (2)随机初始化初始状态值$x_1^{(0)},x_2^{(0)}$ (3)for $t=0$ to $n_1+n_2-1$： ​ (a)从条件概率分布$P(x_2|x_1^{(t)})$中采样得到样本$x_2^{t+1}$ ​ (a)从条件概率分布$P(x_1|x_2^{(t+1)})$中采样得到样本$x_1^{t+1}$ 样本集$\{(x_1^{(n_1)},x_2^{(n_1)}), (x_1^{(n_1+1)},x_2^{(n_1+1)}),\dots,(x_1^{(n_1+n_2-1)},x_2^{(n_1+n_2-1)}) \}$即为需要的平稳分布对应的样本集。 整个采样过程中，通过轮换坐标轴，采样的过程为：$$\left(x_1^{(1)},x_2^{(1)}\right) \rightarrow \left(x_1^{(1)},x_2^{(2)}\right) \rightarrow \left(x_1^{(2)},x_2^{(2)}\right) \rightarrow \dots \rightarrow \left(x_1^{(n_1+n_2-1)},x_2^{(n_1+n_2-1)}\right)$$ 采样是在两个坐标轴上不停的轮换，也可以每次随机选择一个坐标轴进行采样。常用的Gibbs采样实现都是基于坐标轴轮换的。 多维Gibbs采样 一个n维概率分布$\pi(x_1,x_2,\dots,x_n)$，可以通过在n个坐标轴上轮换采样，来得到新的样本，对于轮换到的任意一个坐标轴$x_i$上的转移，马尔科夫链的状态转移概率为$P(x_i | x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)$，即固定$n-1$个坐标轴，在某一个坐标轴上移动。 整个采样过程和Lasso回归的坐标轴下降法算法 非常类似，只不过Lasso回归是固定n−1个特征，对某一个特征求极值。而Gibbs采样是固定n−1个特征在某一个特征采样。 Gibbs采样在高维特征的优势，采样要求数据至少两个维度，一维概率分布的采样无法用Gibbs采样的，此时M-H采样仍然成立。 Gibbs采样获取概率分布的样本集，蒙特卡罗方法用样本集模拟求和，奠定MCMC算法在大数据时代高维数据模拟求和时的作用。 MCMC方法参考资料 Lasso回归]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机控制系统]]></title>
    <url>%2Fblog%2F2020%2F05%2F15%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[参考书籍 《计算机控制系统》刘建昌等著 概述一个典型的计算机控制系统的架构 数字信号： $r(kT)$——给定输入，$y(kT)$——经A/D转换后的系统输出，$u(kT)$——由数字控制器计算的控制的控制信号，$e(kT)=r(kT)-y(kT)$——偏差信号。 离散模拟信号： $y^*(t)$——经过采样开关的被控量信号【时间上离散，幅值上连续】 模拟信号： $y(t)$——系统输出（被控量） 量化模拟信号： $u^*(t)$——经D/A转换后的模拟控制信号【时间上连续，幅值上量化】 典型的计算机控制系统是：连续-离散混合系统。 特点是：模拟、数字和离散模拟信号同在，输入输出均为模拟量的连续环节（被控对象、传感器）、输入和输出均为数字量环节（数字控制器、偏差计算）、输入输出为两类不同量的离散模拟环节（A/D，D/A）共存。 计算机控制系统的应用要求 可靠性高 实时性好。对过程进行实时控制和监测 环境适应性强 过程输入和输出配套较好 系统扩展性好 系统开放性。在主系统接口、网络通信、软件兼容及升级等方面遵守开放性原则，便于系统扩展，异机种链接、软件的可移植和互换。 控制软件包功能强。具备丰富的控制算法，同时具备方便的人机交互，实时性好等性能 计算机控制系统的性能指标1、稳定性 2、稳态指标。 ​ 衡量控制系统精度的指标，用稳态误差来表征。稳态误差是输出量$y(t)$的稳态值$y(\infty)$与给定值$y_0$的差值，定义为：$$e(\infty) = y_0-y(\infty)$$其中$e(\infty)$表示控制精度，越小越好。稳态误差$e(\infty)$与控制系统本身的特性（如系统的开环传递函数）、系统的输入信号（如阶跃、速度或加速度输入信号）、反馈通道的干扰（测量干扰或监测回路的干扰）有关。 3、动态指标 ​ 能较为直观的反映控制系统的过渡过程特性。包括超调量$\sigma \%$、调节时间$t$、峰值时间$t_p$、衰减比$\eta$和振荡次数$N$。 4、综合指标 ​ 设计最优控制系统，常用的综合性能指标是积分型指标。如：$$J=\int_0^t e^2(t)dt$$这种“先误差平方后积分”形式的性能指标用于权衡系统总体误差的大小。数学上容易处理，可以得到解析解。 一个计算机控制系统的典型硬件组成 包括三部分： 过程装置。包括被控对象、执行机构和测量变送装置 输入输出通道。包括过程通道和总线接口 计算机系统。计算机【计算机软件系统=系统软件+应用软件+数据库】和外部设备 模拟量输入通道通常由信号变换器、滤波器、多路模拟开关、前置放大器、采样保持器、A/D转换器、接口和控制电路等部分组成。 模拟量输出通道通常由接口控制电路、D/A转换器（零阶保持期）、滤波器等部分组成。模拟量输出通道有两种结构形式：一是每个通道配置一个D/A转换器，二是通过多路模拟开关共用一个D/A转换器。 数字量输入通道基本功能就是把来自现场的数字信号或开关信号、脉冲信号，按照一定的时序要求送入数字控制器。 数字量输出通道基本功能是把控制器输出的数字控制信号，按照一定的时序要求，送入输出通道中部的数字执行机构，如继电器、可编程器件、步进电机等，通过数字执行机的动作实现被控对象的控制作用。 总线接口内部总线即计算机内部各外围芯片与处理器之间的总线，用于芯片一级的互连，是微处理器与外部硬件接口的通路。 内部并行总线包括：地址、数据和控制总线 内部串行总线SPI的典型结构图如下： SPI系统使用4条线：串行时钟线（SCK）、主机输入/从机输出数据线MISO、主机输出、从机输入数据线MOSI和低电平有效的从机选择线SS。SCK、MOSI、MISO为共享数据线。 系统总线计算机和各个插件板与系统板之间的总线（多总线multibus，STD bus，PC bus等）。 CPU的总线驱动能力有限，大量的接口芯片不能直接挂在微处理器芯片上；存储器、I/O接口芯片太多，电路板安排不下，采用模块化设计会增加总线负载。此时需要在微处理器芯片和总线之间必须加上驱动器。 系统总线可分为5个主要类型： 数据线。决定数据宽度 地址线。决定直接选址范围 控制线。具有控制、时序和中断功能，决定总线功能和适应性的好坏 电源线和地线 备用线 外部总线计算机和计算机之间、计算机和外部其它仪表或设备之间连接通信的总线。 模数之间转换D/A转换按照规定的时间间隔T对控制器输出的数字量进行D/A转换。基本原则“按权展开求和”，对数字量中的每一位，按权值分别转换为模拟量，然后通过运放求和，得到相应的模拟量的输出。 $n$位D/A转换器件（DAC）的输出电压$V_{out}$为：$$V_{out} = V_{FSR}\left(\frac{B_1}{2}+\dots+\frac{B_n}{2^n}\right)$$其中，$V_{FSR}$为输出的满幅值电压；$B_1$是二进制最高有效位，$B_n$为最低有效位。 D/A转换器包括解码和信号恢复两个变换。解码用于把数字量转换为幅值等于该数字量的模拟脉冲信号（离散模拟信号，时间离散，幅值是模拟脉冲信号（电压、电流））。 信号恢复器（保持器）将离散的模拟脉冲信号按一定的规则保持规定的时间间隔T，把时间离散变成时间上连续。 D/A转换的误差：主要由D/A转换器转换精度（转换器字长）和保持器（采样点之间的插值）的形式以及时间间隔T来决定。 A/D转换四种类型： 计数器式。速度慢，便宜 并行比较式。用在高速采样 ，当位数多成本高 双积分式。精度高，有较强抗干扰能力，速度慢，用在高精度低速度场合 逐次逼近式。兼顾精度和速度，在16位以下广泛使用 A/D转换要完成采样、量化和编码3个变换。 采用保持器对连续的模拟输入信号按一定时间间隔T进行采样，变成时间离散、幅值等于采样时刻输入信号值的序列信号。 量化是将采样时刻的信号幅值按最小量化单位取整过程。量化单位越小，采样时刻信号的幅值与变换成的有限维数的二进制数码的差异越小。精度越高。 编码将量化的分层信号变换成二进制数码（只是信号形式的改变），是一个无差的等效变换过程。 A/D转换误差主要由A/D转换器转换速率（孔径时间）和转换精度（量化误差）来决定。 计算机控制系统的理论问题1、信号转换问题 ​ 计算机控制系统在结构上 通常是模拟和数字部件组成的混合系统。 2、对象建模与性能分析 ​ 计算机控制系统是由纯离散系统的计算机和纯连续系统的被控对象构成的混合系统。为了便于分析和设计，都等效地看成离散系统处理。对离散系统通常采用时域的差分方程、复数域的z变换和脉冲传递函数、频域的频率特性以及离散状态空间方程作为系统数学描述的基本工具。 3、控制算法设计 ​ 研究对象的日趋复杂化，常规控制理论常常难以解决复杂控制系统的控制问题。 4、控制系统实现技术 ​ 采用数字控制器因而会产生数值误差。 计算机控制系统的基本类型按照功能和结构划分，有： 1、操作指导控制系统。 ​ 提供现场情况和进行异常报警，还按照预先建立的数学模型和控制算法进行运算和处理，得出最优设定值打印显示，操作人员根据计算机给出的操作指导，并根据实际经验，经过分析判断，由人直接改变调节器的给定值或操作执行机构。 2、直接数字控制系统。 ​ DDC系统是计算机把运算结果直接输出去控制生产过程。属于闭环系统，计算机系统对生产过程各参量进行检测，根据规定的数学模型，如PID算法进行运算，发出控制信号，直接控制生产过程。 ​ 不仅能完全取代模拟调节器，而且只要改变程序就可以实现其他的复杂控制规律，如前馈控制、非线性控制等。 3、计算机监督控制系统。 ​ SCC（supervisory computer control）也称为计算机设定值控制系统。计算机的输出用来直接改变模拟调节器或DDC的设定值。 ​ 由两种形式： ​ SCC+模拟调节器的系统 ​ 优点：能始终使得生产过程处于最优运行状态，与操作指导控制系统相比，不会因手调设定值的方式不同而引起控制质量的差异。灵活安全，出现故障仍可由模拟调节器单独完成操作。 ​ 缺点：需要采用模拟调节器。 ​ SCC+DCC系统 ​ SCC的输出直接改变DDC的设定值，两者之间通过数据传输直接实现。通常一台SCC可以控制数个DDC计算机，一旦DDC发生故障，可用SCC计算机代替DDC，确保生产的正常进行。 4、分级控制系统。 ​ 由管理信息系统（MIS）、计算机监督控制（SCC）和直接数字控制（DDC）三级控制组成。 SCC级为分级控制的中间级，功能是集中生产过程信息，对生产过程进行优化、实现自适应或最控制等，指挥DCC，接收MIS级命令并向MIS级汇报。 DDC用于直接控制生产过程，多采用微型机。 5、集散控制系统 ​ DCS（distributed control system）是由微型机为核心的过程控制单元（PCU）、高速数据通道（DHW）、操作人员接口单元（OIU）和监控计算机等部分组成。 由于生产过程的大型化、复杂化和分散化，若采用一台计算机控制和管理，一旦计算机发生故障，整个系统停顿，“危险集中”。 集散控制的设计思想：“危险分散”，将控制功能分散，将监控和操作功能高度集中。 PCU：由很多模板组成，每个控制模板是以微处理器为核心组成的功能板，可以对几个回路进行PID、前馈等多种控制。一个控件发生故障，只影响与之相关的几个回路，达到“危险分散”目的。PCU可以安装在离变送器和执行机构近的地方，缩短控制回路长度，减少噪声，提高可靠性，达到“地理上”分散。 DHW：将各个PCU、OIU、监控计算机等有机连接【相互协调不可分】起来以实现高级控制和集中控制。挂在DHW上任一单元发生故障，都不会影响其他单元之间的通信联系和正常工作。 OIU：实现集中监视和集中操作 监控计算机：实现最优控制和管理。监控机的功能是存取工厂所有的信息和控制参数，打印综合报告，能进行长期趋势分析以及进行最优化计算机控制，控制各个现场过程控制单元（PCU）工作。 6、总线控制系统。 ​ 现场总线控制系统（field control system，FCS）的体系结构主要表现在：现场通信网络、现场设备互连、控制功能分散、通信线供电、开放式互联网络等方面。 ​ FCS底层产品都带有CPU的智能单元【包括智能传感器、智能执行器等】，突破传统DCS的底层产品4-20mA模拟信号的传输。智能单元靠近现场设备，可以独立地完成测量、校正、调整、诊断和控制功能。现场总线协议将它们连接在仪器，任何一个单元出现故障都不会影响到其他单元，更不会影响全局，实现彻底的分散控制，更安全、可靠。 7、以太控制网络。 ​ 以太控制网络最典型的应用形式为：顶层采用Ethernet（以太网，局域网技术，包括物理层的连线、电子信号和介质访问层协议的内容），网络层和传输层采用TCP/IP协议。嵌入式控制器、智能现场测控仪表和传感器可以很方便的接入以太控制网。 ​ 以太控制网容易与信息网络集成，组建统一的企业风格。以太控制避免了现场总线技术游离于计算机网络技术之外，使得现场总线技术和网络技术融合，实现网络控制系统的彻底开放。 信号转换信号变换原理计算机控制系统信号转换分析 （1）模拟信号。时间上连续，幅值上连续。即连续信号。 （2）离散模拟信号。时间上离散，幅值上连续的信号。即采样信号 （3）数字信号。时间上离散，幅值上离散（已经量化）的信号，可用一个序列数字表示 （4）量化。采用一组数码（多用二进制数码）来逼近 离散模拟信号的幅值，将其转换为数字信号 （5）采样。将模拟信号按一定的时间间隔抽样成离散模拟信号的过程。 采样函数可以用$x^(t)$、$y^(t)$及$e^(t)$表示，``表示离散化。 采样分为： 均匀采样：采样周期不变 非均匀采样：采样周期变化 随机采样：采样间隔大小随机变化 采样过程及采样函数的数学表示 得到时间上离散的数值序列：$$f^*(t) = \{f(0T),f(1T),f(2T),\dots,f(kT),\dots\}$$其中$T$为采样周期。 ==脉冲采样==： 采样周期$T$比采样开关闭合时间$\tau$大很多，即$\tau \ll T$，$\tau$比起被控对象的时间常数也非常小，认为$\tau \rightarrow 0$。 连续函数$f(t)$，经脉冲采样器调制后输出的一个采样函数$f^*(t)$。其中$\delta_T(t)=\sum\limits_{k=0}^{\infty} \delta(t-kT)$为单位理想脉冲序列。 采样函数：$$f^(t) = f(t)\delta_T(t)=f(t)\sum\limits_{k=0}^{\infty} \delta(t-kT)$$其中$ \delta(t-kT)$为$t=kT$时刻的理想单位脉冲，定义为：$$\delta(t-kT) =\begin{equation}\left\{\begin{array}{lr} \infty , t=kT\\ 0, t \ne kT \end{array} \right.\end{equation}$$且冲量为1，即：$$\int_{0}^{\infty} \delta(t-kT)dt = 1$$得到理想脉冲采样函数数学表达式：$$f^(t) =\sum\limits_{k=0}^{\infty} f(kT)\delta(t-kT)$$其中，$\delta(t-kT)$仅表示脉冲存在的时刻，冲量为1；脉冲的大小由采样时刻的函数值$f(kT)$决定，称为脉冲强度。 采样函数的频谱分析及采样定理$$f^*(t) =f(t)\sum\limits_{k=0}^{\infty} \delta(t-kT)$$ $$\delta_T(t) = \sum\limits_{k=-\infty}^{\infty}\delta(t-kT)= \sum\limits_{k=-\infty}^{\infty}C_ke^{jkw_st}$$ 其中，$w_s = \frac{2\pi}{T}$为采样角频率；$C_k$为傅里叶系数，表示为：$$C_k = \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} \delta_T(t)e^{-jkw_st}dt$$$\delta_T(t)$在$\left[-\frac{T}{2},\frac{T}{2}\right]$时间内，仅在$t=0$时有脉冲，脉冲函数的筛选性，即：$$\int_{-\infty}^{\infty}\delta(t)f(t)dt = f(t) |_{t=0}$$ $$C_k = \frac{1}{T}e^{-jkw_s t}|_{t=0} = \frac{1}{T}$$ $$\delta_T(t) = \frac{1}{T}\sum\limits_{k=-\infty}^{\infty}e^{jkw_st}$$ 所以，采样函数可以写成：$$f^(t) = \frac{1}{T}\sum\limits_{k=-\infty}^{\infty}f(t)e^{jkw_st}$$由拉氏变换$F(s) = \int_{0}^{\infty} f(t)e^{-st}dt$可得：$$F^(s) = \int_{0}^{\infty}f^(t)e^{-st}dt=\int_{0}^{\infty}\frac{1}{T}\sum\limits_{k=-\infty}^{\infty}f(t)e^{jkw_et}e^{-st}dt$$根据拉氏变换复位移定理得：$$F^(s) = \frac{1}{T} \sum\limits_{k=-\infty}^{\infty}F(s-jkw_s)$$令$s=jw，n=-k$，直接求得采样函数得傅里叶变换式：$$F^(jw) = \frac{1}{T}\sum\limits_{n=-\infty}^{\infty}F(jw+jnw_s)$$$F^(jw)$为采样函数$f^*(t)$得频谱函数。 采样周期$T$的选择会影响$f^(t)$的频谱，*采样定理要解决的问题是：采样周期选多大，才能将采样信号较少失真地恢复为原连续信号。 ==shannon采样定理==：$$w_s \ge 2w_{max}$$$$T \le \frac{\pi}{w_{max}}$$ 如果一个连续信号不包含高于频率$w_{max}$的频率分量，完全可以用周期$T&lt; \frac{\pi}{w_{max}}$的均匀采样值来描述，即当采样频率$w_s &gt; 2w_{max}$，可以从采样信号中不失真地恢复原连续信号。 采样恢复与保持器计算机作为信息处理装置，其输出一般有两种形式：一是直接数字量输出，如开关形式、步进电机控制等；一种是需要将数字信号$u(kT)$转换为连续信号$u(t)$。 数字信号无失真的恢复成连续信号，有Shannon采样定理，采样频率$w_s \ge 2w_{max}$，则在被控对象前加一个理想滤波器： 实际所采用的是保持器，与理想滤波器特性相近的物理可实现。 从脉冲序列$u^(t)$的全部信息种恢复原来的连续信号$u(t)$，通过保持器来完成这个恢复过程。实际是一个多项式外推装置。一个方法是利用$u(t)$的幂级数展开公式，即：$$u(t)=u(kT) +u’(kT)(t-kT)+\frac{u’’(kT)}{2}(t-kT)^2+\dots,kT\le t&lt;(k+1)T$$若按第一项组成外推器，所用$u(t)$的多项式是零阶的，称为零阶保持器，若按前两项组成外推装置，所用的多项式是一阶的，称为*一阶保持器。 导数值可以用各个采样时刻的各阶差商来表示，即：$$u’(kT) = \frac{1}{T}\{u(kT)-u[(k-1)T]\}$$ $$u’’(kT) = \frac{1}{T}\{u’(kT)-u’[(k-1)T]\}$$ 同理可得$u’[(k-1)T]$，以此类推得：$$u’’(kT) = \frac{1}{T^2}\{u(kT) - 2u[(k-1)T]+u[(k-2)T]\}$$ 零阶保持器$$u_h(t) =u(t)= u(kT),kT \le t\le(k+1)T$$ 特点是：零阶保持器把$kT$时刻的采样值，简单的、不增不减地保持到下一个采样时刻$(k+1)T$到来之前。 零阶保持器的传递函数： 脉冲响应函数$g_0(t)$分解后线性叠加：$$g_0(t) = l(t)-l(t-T)$$阶跃函数$l(t) = \cases{1,t\ge0 \ 0,t&lt;0}$ 拉氏变换为：$$G_0(s) = L[g_0(t)] = \frac{1}{s}-\frac{1}{s}e^{-st} = \frac{1-e^{-st}}{s}$$输入单位脉冲$\delta(t)$的拉氏变换：$$X(s) = L[\delta(t)]=1$$零阶保持器的传递函数为：$$W_{h0}(s) = \frac{G_0(s)}{X(s)} = \frac{1-e^{-sT}}{s}$$零$s=jw$，得到零阶保持器的频率特性。 一阶保持器$$\begin{equation}\begin{aligned}u_h(t) &amp;= u(kT)+u’(kT)(t-kT),kT \le t &lt;(k+1)T \ &amp;= u(kT) +\frac{u(kT)-u[(k-1)T]}{T}(t-kT)\end{aligned}\end{equation}$$ z变换及反变换定义是拉氏变换的特殊形式，在离散系统的分析及设计中发挥重要作用。$$F(s)= L[f(t)] = \int_{-\infty}^{\infty}f(t)e^{-st}dt$$ $$f^*(t) =\sum\limits_{k=0}^{\infty} f(kT)\delta(t-kT)$$ 其拉氏变换为：$$F^(s) = L[f^(T)] =\sum\limits_{k=0}^{\infty}f(kT)\left[\int_{-\infty}^{\infty}\delta(t-kT)e^{-st}dt\right]$$根据广义脉冲函数$\delta(t)$的性质：$$\int_{-\infty}^{\infty}\delta(t-kT)e^{-st}dt = e^{-skT}=L[\delta(t-kT)]$$令$z= e^{sT}$，得采样函数$f^(t)$的$z$变换为：$$F^(s) = \sum\limits_{k=0}^{\infty}f(kT)e^{-skT}$$$$F(z) = \sum\limits_{k=0}^{\infty}f(kT)z^{-k}$$ 其中$f(kT)$表示时间序列的强度，$z^{-k}$表示时间序列出现的时刻，相对时间的起点，延迟了$k$个采样周期。$F(z)$既包含了信号幅值的信息，又包含了时间信息。 注意到：在z变换中，仅仅考虑到采样时刻的采样值，所以$F(z)$只能表征采样函数$f^*(t)$的z变换，只能表征连续时间函数$f(t)$在采样时刻上的特性，不能表征采样点之间的特性。 z变换方法z变换的基本定理z反变换从$z$变换$F(z)$求出采样函数$f^(t)$，称为$z$反变换，表示为：$$Z^{-1}[F(z)] = f^(t)$$$z$变换得到的是各采样时刻上连续函数$f(t)$的数值序列值$f(kT)$，得不到两个采样时刻之间的连续函数的信息，因此无法用$z$反变换方法求出原连续函数$f(t)$，即$Z^{-1}[F(z)] \ne f(t)$。 扩展z变换因为$F(z)$只能反映连续信号$f(t)$在各个采样时刻的变换情况 ，而不能反映$f(t)$在采样时刻之间的任何变换信息。$z$变换的分析方法及所得结论只针对一些离散时刻有效，而在这些离散时刻之间的时刻是无效的。 在计算机控制系统的分析和设计时，不仅需要知道在采样点上的输入、输出关系，还要知道采样点之间的输入、输出关系，需要扩展$z$变换。 计算机控制系统数学描述与性能分析离散系统离散时间系统的输入和输出均为离散信号，离散系统可以抽象为一种系统的离散输入信号和系统的离散输出信号之间的数学变换和映射。 设单输入单输出的离散系统$D$的输入为$e(k)$，输出为$u(k)=u(kT)$。两者都是离散的数值序列。 有：$$u(k) = D[e(k)]$$$$e(k)=ae_1(k) +be_2(k)$$ $$u(k) = aD[e_1(k)]+bD[e_2(k)]$$ 该系统的变换函数$D$是线性的，$u(k)$与$e(k)$之间是线性关系。 系统$D$的参数不随时间变化，即系统$D$的响应不取决于输入作用的时刻，系统是常系数的，即定常系统。 线性常系数离散系统一般采用差分方程来描述。系统在某一时刻$k$的输出$u(k)$，不仅取决于本时刻的输入$e(k)$，与过去的时刻的输入数值序列$e(k-1),e(k-2),\dots$有关，还与该时刻以前的输出值有关，即：$$\begin{equation}\begin{aligned}&amp; u(k)+a_1u(k-1)+a_2u(k-2)+\dots+a_nu(k-n) \ &amp;=b_0e(k) +b_1e(k-1)+b_2e(k-2)+\dots+b_me(k-m)\end{aligned}\end{equation}$$即：$$u(k) = -\sum\limits_{i=1}^na_iu(k-i) +\sum\limits_{j=0}^mb_je(k-j)$$当系数均为常数时，是一个$n$阶线性常系数差分方程。上式是后向非齐次差分方程。 前向差分方程和后向差分方程区别：前向多用于描述非零初始值的离散系统，后向多用于描述全零初始值的离散系统。 差分方程描述离散控制系统，通过差分方程的求解，来分析和设计离散控制系统。 差分方程的解法有迭代法、经典解法和$z$变换解法。 差分方程解法脉冲传递函数线性连续系统的动态特性主要用传递函数来描述 ，线性离散控制系统则主要用脉冲传递函数来描述，脉冲传递函数也简称为$z$传递函数。 脉冲传递函数定义： 在线性离散控制系统中，在零初始条件下，一个 系统（或环节）输出脉冲序列的$z$变换与输入脉冲序列的$z$变换之比，即：$$W(z)= \frac{Y(z)}{X(z)}=\frac{输出脉冲序列的z变换 }{输入脉冲序列的z变换}$$脉冲传递函数仅取决于系统本身的特性，与输入量无关。 脉冲传递函数的推导： 计算机控制系统的脉冲传递函数 计算机控制系统由数字部分和连续对象部分构成的闭环控制系统，数字部分表示控制算法，它的输入和输出皆为离散信号序列。 可用脉冲传递函数$D(z)$来表示输出输入关系。再求出连续系统的等效脉冲传递函数$W_d(z)$，得到控制系统的各种脉冲传递函数。 计算机控制系统的稳定性分析稳定性分析的基础是$z$变换，$z$变换与连续系统$s$变换在数学上的内在联系，可经过一定的变换把分析连续系统稳定性的方法引入到离散控制系统中。 由$s$平面上稳定性条件来分析$z$平面的稳定条件，然后由$s$平面到$z$平面的映射，分析采样周期对系统稳定性的影响。 离散系统的稳定性条件在连续系统中，闭环传递函数可以写成两个多项式之比：$$\frac{Y(s)}{R(s)} = \frac{b_0s^m +\dots+b_{m-1}s+b_m}{s^n +a_1s^{n-1}+\dots +a_{n-1}s +a_n}$$连续系统稳定条件：闭环传递函数的全部极点位于$s$平面的做半平面内，或传递函数的极点具有负实部。 在离散系统中，若输入序列有限，其输出序列也是有限的。 离散系统稳定性条件：闭环脉冲传递函数的全部极点位于$z$平面上以原点为圆心的单位圆内。 采样周期与系统稳定性关系计算机控制系统的控制对象是连续系统，等效离散化后的闭环系统的$z$传递函数模型与采样周期选取有关，极点分布也与采样周期的选取有关。 一般来说，采样周期越小，系统稳定性越高，采样周期对系统稳定性的影响主要是由计算机控制系统中采样保持器引起的。 离散系统稳定性的代数判据劳斯稳定性判据连续系统通过判断系统特征方程的根是否都在$s$平面虚轴左边来确定系统是否稳定。离散系统的稳定边界是$z$平面的单位圆，而不是虚轴，连续系统的Routh判据不能直接应用于离散系统的稳定性判别，需要引入$\omega$变换（双线性变换）。 通过$\omega$变换，把离散系统在$z$平面的稳定边界单位圆映射为新的$\omega$平面的虚轴；把离散系统$z$平面的稳定域——单位圆内部区域映射为新的$\omega$平面的左半平面，并且将离散系统原来以$z$为变量的特征多项式化为以$\omega$为变量的特征多项式。 $\omega$变换的定义为：$$z= \frac{1+(T/2)\omega}{1-(T/2)\omega}$$其中$T$为采样周期，得到$\omega$的解为：$$\omega = \frac{2}{T}·\frac{z-1}{z+1}$$ 朱利稳定性判据Jury稳定性判据是根据系统特征方程的系数判断系统的稳定性，不用求特征方程的根。 Jury一个重要优点是可以在$z$域直接进行，不需要进行$z-\omega$变换。 Routh判据不仅可以判断系统的稳定性，还可以判断出不稳定极点的个数，Jury判据只能判断出系统是否稳定。 计算机控制系统稳态过程分析稳态指标是用稳态误差来表示，稳态误差是系统过渡过程结束到达到稳态以后，系统参数输入与系统输出之间的偏差。 稳态误差越小，系统控制稳态精度越高。 计算机控制系统暂态分析暂态分析主要用系统在单位阶跃输入信号作用下的相应特性来描述，反映控制系统的动态过程。 主要性能指标用超调量$\sigma \%$、上升时间$t_r$、峰值时间$t_p$和调节时间$t_s$表示。 计算机控制系统的频域特性分析离散系统的频域描述在连续系统中，一个系统（或环节）的频域特性是指，在正弦信号作用下，系统（或环节）的稳态输出与输入的复数比随输入信号频率变换的特性。 连续系统的频率特性公式：$$W(jw)= W(s)|_{s=jw}$$可得到离散系统的频率特性公式：$$W(e^{jwT})=W(z)|_{z=e^{jwT}}$$连续系统的频率特性$W(jw)$随着$w$变化，相当于$W(s)$当$s$沿虚轴变化时$s=jw$的特性； 离散系统的频率特性$W(e^{jwT})$相当于考察传递函数$W(z)$当$z$沿单位圆变化时$z=e^{jwT}$的特性。 离散系统频域稳定性分析在离散系统中，奈奎斯特稳定判据（奈氏判据）根据复变函数的幅角原理，利用开环频率特性来判别闭环系统的稳定性。 设离散系统的开环脉冲传递函数为$W_K(z) = \frac{M(z)}{N(z)}$，阶次低于$N(z)$的阶次，相应的单位反馈系统的闭环脉冲传递函数为：$$W_B(z) = \frac{W_K(z)}{1+W_K(z)} = \frac{M(z)}{M(z)+N(z)} = \frac{M(z)}{F(z)}$$系统闭环特征方程为：$$P(z) = 1+W_K(z) =\frac{M(z)+N(z)}{N(z)}=\frac{F(z)}{N(z)} =0$$其中$N(z)$是系统开环特征多项式，零点为开环系统极点；$F(z)$为系统闭环特征多项式，零点为闭环系统极点。 闭环系统稳定的充要条件：$F(z)$(或P(z))在单位圆外无零点。 离散系统伯德图分析伯德图广泛应用于单输入单输出连续系统的设计，原理是利用开环系统的对数频率特性，对系统稳定性、稳态性能和暂态性能及逆行分析，是常用的系统频率特性的性能分析和校正环节的设计方法。 数字控制器模拟化设计方法 对于混合计算机控制系统，包括数字信号、模拟信号、离散模拟信号和量化模拟信号。 模拟化设计方法有两种：一是利用熟悉的各种模拟系统设计方法（连续域设计方法）设计满意的模拟控制器，然后将其离散化为数字控制器。【数字控制器的模拟化设计方法】；二是首先把模拟被控对象连续部分离散化，然后直接在离散域设计数字控制器。【离散化设计（直接设计）方法】 对于数字化控制器模拟化设计方法：关心点在于把混合系统当作模拟系统来设计需要什么约束条件以及模拟控制器的离散化会给系统的性能带来什么影响。 连续控制器的离散化方法连续控制器的离散化是求连续控制器的传递函数$D(s)$的等效离散传递函数$D(z)$。 离散化的方法包括：$z$变换、差分变换、双线性变换、零极点匹配法等 数字PID控制器PID控制器表示比例（proportional）-积分（integral）-微分（differential）控制规律。控制器的输出和输入是比例-积分-微分关系。 基本的数字PID控制算法包括位置式PID控制算法和增量式PID控制算法。 模拟PID控制器算法为：$$u(t) = K_p\left[e(t)+\frac{1}{T_i}\int_{0}^{t}e(t)dt+T_d\frac{de(t)}{dt}\right]$$其中，$u(t)$为输出；$e(t)$为输入；$K_p$为比例系数；$T_i$为积分时间常数；$T_d$为微分时间常数。 模拟PID的传递函数形式：$$D(s)= \frac{U(s)}{E(s)} = K_p\left(1+\frac{1}{T_is} +T_d s\right)$$模拟PID控制器离散化处理：用后向差分近似代替微分$$\begin{equation}\left\{ \begin{array}{lr}u(t)\approx u(kT)\\e(t) \approx e(kT) \ \int_{0}^{t}e(t)dt \approx T\sum\limits_{i=1}^{k}e(iT) \ \frac{de(t)}{dt}\approx \frac{e(kT)-e(kT-T)}{T}\end{array}\right.\end{equation}$$省略采样周期$T$，即$kT=T$，有：$$\begin{equation}\begin{aligned}u(k)&amp;= K_p \left\{ e(k) +\frac{T}{T_i}\sum\limits_{j=1}^{k}e(j) + \frac{T_d}{T}[e(k)-e(k-1)] \right\} \ &amp;= K_pe(k) +K_i\sum\limits_{j=1}^{k}e(j) +K_d[e(k)-e(k-1)]\end{aligned}\end{equation}$$其中$K_i = K_p\frac{T}{T_i}$为积分系数；$K_d$为微分系数，上式为位置式数字PID控制算法。 增量式数字PID算法： 由位置式PID控制算法得：$$u(k-1) = K_pe(k-1) +K_i\sum\limits_{j=1}^{k-1}e(j) +K_d[e(k-1)-e(k-2)]$$ $$\begin{equation}\begin{aligned}\Delta u(k) &amp;= u(k) - u(k-1)\ &amp; = K_p[e(k)-e(k-1)]+K_ie(k) +K_d[e(k) -2e(k-1)+e(k-2)]\end{aligned}\end{equation}$$ 增量式PID算法表示执行机构（如阀门、步进电机等）得调节增量，即$k$时刻相对于$k-1$时刻的调节增量。 位置式数字PID控制算法一般形式： 由增量式PID算法得：$$\begin{equation}\begin{aligned}u(k) &amp;= u(k-1) +\Delta u(k) \ &amp;= u(k-1) +K_p[e(k)-e(k-1)]+K_ie(k) +K_d[e(k)-2e(k-1)+e(k-2)] \end{aligned}\end{equation}$$ 数字PID控制器算法工程化改进积分分离数字PID算法（PD算法）很多控制系统在开始启动、停止或较大幅度改变给定信号时，控制器输入端都会产生较大的偏差（系统给定和输出信号之间的偏差），PID算法中的积分项经过短时间积累就会使得控制量$u(k)$变得很大甚至达到饱和（执行机构的机械极限），此时控制系统会处于一种非线性状态，不能根据控制器输入偏差的变化按预期控制规律来正确地改变控制量。 积分项很大，需要经过很长时间误差才能被减下来，系统会产生严重超调。 直接改进方法是把积分项分离出来，当偏差绝对值超出分离阈值A时，积分不起作用，构成PD控制器。当偏差绝对值在阈值范围内，积分起作用。$$u(k) = K_pe(k)+K_1K_i\sum\limits_{j=1}^{k} e(j) K_d[e(k) - e(k-1)]$$ $$K_1 = \cases{1, |e(j| \le A \ 0,|e(j)| &gt; A}$$ 其中，$K_1$为逻辑系数；$A$为积分分离阈值。 算法流程： 带死区的数字PID控制算法被控变量达到工艺要求的精度，即系统的输出与输入之间的偏差达到要求的控制进度，偏差无限小，代价大；复杂的控制过程中有多个控制系统并存，各个被控变量之间可能存在一定的关联，过分追求一个指标，可能会影响其他指标，只需要达到设定指标即可。 死区算法为：$$e’(k) = \cases{e(k), |e(k| &gt;B \ 0,|e(k| \le B}$$有：$$\Delta u(k) =K_p[e’(k) - e’(k-1)] +K_ie’(k) +K_d[e’(k)-2e’(k-1)+e’(k-2)]$$带死区的位置式PID算法为：$$u(k) =u(k-1) +\Delta u(k)$$注意：由于PID控制器积分的保持作用，当$e’(k) = 0$时，PID控制器的输出保持$k-1$时刻的值的输出，而非零。 不完全微分PID算法PID控制算法中微分环节对改善系统超调量等动态性能具有重要作用，但是它对高频干扰信号比较敏感。 当控制器输入偏差信号突然变化时，PID控制器中的微分项将很大，持续时间又很短，产生微分失控现象。 不完全微分PID控制算法中的微分作用持续很长时间，具有更好的抗干扰作用。 不完全微分PID控制算法增量形式为：$$\Delta u(k) = \alpha \Delta u(k-1) +(1-\alpha)\Delta u’(k)$$其中：$$\Delta u’(k) = K_p[e(k)-e(k-1)] +K_i e(k) +K_d [e(k) -2e(k-1)+e(k-2)]$$ 微分先行PID控制算法在给定值频繁升降变换的场合，为了避免系统超调量过大甚至发生振荡，导致执行机构剧烈动作，需要对模拟PID控制器进行改进。 输出微分先行：只对输出微分，不对输入微分。适合给定值频繁升降的场合，可以避免给定值引起的超调量过大。 偏差微分先行：对给定值和输出量都有微分作用。适合串级控制的副控制回路。 数字控制器离散化（直接）设计方法基本原理设计思路：将连续的控制对象及其零阶保持器用适当的方法离散化后，系统完全变成离散系统，因而可以用离散系统的设计方法直接在$z$域进行控制器的设计。 这种离散化的设计方法，稳定性好、精度高，一般用于可以精确建立对象的数学模型的情况。是在给定的采样周期下进行设计的，因此采样周期的选择取决于被控对象的特性，不受分析设计方法的限制。 闭环系统的脉冲传递函数为：$$W_B(z) = \frac{Y(z)}{R(z)} = \frac{D(z)W_d(z)}{1+D(z)W_d(z)} = \frac{W_K(z)}{1+W_K(z)}$$系统闭环的误差的脉冲传递函数为：$$W_e(z)=\frac{E(z)}{R(z)} = \frac{1}{1+D(z)W_d(z)}$$ $$W_B(z) = 1-W_e(z)$$ 得到控制器的脉冲传递函数为：$$D(z) = \frac{W_B(z)}{W_d(z)[1-W_B(z)]} = \frac{1-W_e(z)}{W_d(z)W_e(z)} = \frac{W_B(z)}{W_d(z)W_e(z)}$$若已知被控对象的脉冲传递函数$W_d(z)$，并根据性能指标要求确定出整个系统的闭环脉冲传递函数$W_B(z)$或闭环误差脉冲传递函数$W_e(z)$，则数字控制器$D(z)$可以唯一确定。 数字控制器直接设计法的解析设计过程如下： 确定被控对象的传递函数模型，是算法设计的基础。直接设计法假定得到了被控对象的精确模型$W(s)$，将其连同前面的零阶保持器一起离散化后，得到被控对象的广义被控模型$W_d(z)$。 根据控制系统的性能指标要求及其它约束条件，确定出闭环系统的传递函数$W_B(z)$或$W_e(z)$，或者两者同时确定，满足约束条件$W_B(z) = 1-W_e(z)$。 根据上述$D(z)$计算公式确定控制器传递函数模型$D(z)$。 根据$D(z)$编制控制算法。 直接设计方法的关键在于：闭环系统的脉冲传递函数$W_B(z)$（或者$W_e(z)$ ）的选择。要满足数字控制器物理可实现性、稳定性、准确性和快速性等方面的要求。 最小拍控制器的设计方法最小拍系统：最少调整时间系统或最快响应系统，是指系统对单位阶跃输入、单位速度输入或单位加速度输入等典型输入信号，具有最快的响应速度，经过最少个采样周期，使得输出的稳态误差为零，达到输出完全跟踪输入的目的。 根据以上原则来确定控制器$D(z)$的脉冲传递函数，即计算机控制算法 。 最小拍控制器工程化改进最小拍控制系统存在不足之处： 输出在采样点之间可能存在波纹 对各种典型输入函数的适应性差 对被控对象的模型参数变化敏感 大林算法最小拍控制设计方法，只适合一些计算机控制系统，对于系统输出的超调量有严格限制的系统并不理想。 对于一些纯滞后调节系统，滞后时间比较长，要求没有超调量或很少超调量，而调节时间允许在较多的采样周期内结束，超调是主要的设计指标。此时PID算法效果也欠佳。 大林（Dahlin）算法针对纯滞后的一阶和二阶惯性环节所提出的一种直接综合设计方法，具有良好的控制效果。 算法设计原理设被控对象为带有纯滞后的一阶或二阶环节，即：$$W(s) = \frac{K}{T_1s+1}e^{-\tau s}, \tau = NT$$ $$W(s) = \frac{K}{(T_1s +1)(T_2s +1)}e^{-\tau s}，\tau = NT$$ 其中，$T_1,T_2$为对象时间常数；$\tau$为对象纯滞后时间，一般是采样周期的整数倍$NT$；$T$为采样周期。 大林算法的控制目标是：设计合适的数字控制器，使得整个闭环系统的传递函数为带有纯滞后的一阶惯性环节，且要求闭环系统的纯滞后时间等于对象的纯滞后时间，即：$$W_B(s) = \frac{e^{-\tau s}}{T_0s +1},\tau = NT$$其中，$T_0$为等效的闭环系统的时间常数。 数字控制器程序实现从$D(z)$算式的复杂性和控制系统的灵活性出发，采用计算机软件的方法实现更适宜。 包括直接程序设计、串联程序设计和并行程序设计。 状态空间模型古典控制理论中，常采用线性微分方程和传递函数这两种输入输出的数学模型来描述线性定常动态系统。把系统看成一个“黑箱”来处理，不表征系统内部结构和内部变量，只反映外部变量即输入输出变量间的因果关系，称为外部描述。外部描述不能反映系统内部的某些特性。 现代控制理论中，采用状态空间描述，是一种内部描述的数学模型。用到外部变量和内部变量，由两个数学表达式组成，一个是反映系统内部状态的变量组$x_1,x_2,\dots,x_n$和输入变量组$u_1,u_2,\dots,u_p$之间因果关系的数学表达式。称为状态方程（微分方程或差分方程形式）。 另一个是表征系统内部状态的变量组$x_1,x_2,\dots,x_n$与输入变量组$u_1,u_2,\dots,u_p$和输出变量组$y_1,y_2,\dots,y_q$之间关系的数学表达式，称为输出方程。 状态空间描述的基本定义状态： 控制系统的状态是指系统过去、现在和将来的状况。 状态变量： 指能完全表征系统运动状态的最小一组变量。完全表征是指： 任何时刻$t=t_0$，这组状态变量的值$x_1(t_0),\dots,x_n(t_0)$就表示系统在该时刻的状态。 当$t \ge t_0$时的输入$u(t)$给定，且上述初始状态确定时，状态变量能完全确定系统在$t \ge t_0$的行为。 状态变量的最小性体现在：状态变量$x_1(t),\dots,x_n(t)$是为完全表征系统行为所必需最少个数的系统状态变量，减少状态变量个数会破坏表征的完整性，增加变量的个数是不需要的。 状态向量： 若一个系统有$n$个彼此独立的状态变量$x_1(t),\dots,x_n(t)$，构成状态向量$\pmb X(t)$：$$\pmb X(t)=\begin{bmatrix}x_1(t) \ x_2(t)\ \vdots \ x_n(t)\end{bmatrix}$$状态空间： 以状态变量$x_1(t),\dots,x_n(t)$为坐标轴构成的$n$维空间为状态空间。系统在任意时刻的状态，都可以用状态空间中的一个点来表示。 给定初始时刻$t_0$的状态$\pmb X(t_0)$，得到状态空间中的一个初始点，随着时间的推移，$\pmb X(t)$在状态空间中描绘出一条轨迹，称为状态轨迹。 状态方程： 系统的状态变量与输入变量之间的关系用一组一阶微分方程来描述的数学模型称为状态方程。 输出方程： 输出变量与状态变量、输入变量之间的关系的数学表达式 状态空间表达式： 状态方程和输出方程组合，构成对一个系统动态行为的完整描述，称为状态空间表达式（或状态空间模型）。 离散系统的状态空间模型可以表示成：$$\cases{\pmb X(k+1) = F \pmb X(k) + G\pmb u(k) \ \pmb y(k) = C\pmb X(k) +D\pmb u(k)}$$其中，$\pmb X$为$n$维状态向量；$\pmb u$为$m$维控制向量；$\pmb y$为$p$维输出向量；$F(n\times n)$为离散系统状态转移矩阵；$G(n \times m)$为离散系统的输入矩阵或控制转移矩阵；$C(p \times n)$为状态输出矩阵；$D(p\times m)$为直接传输矩阵。 系统的能控性和能观性对于一个控制系统，特别是多变量控制系统，有如下两个问题： 在有限时间内，控制作用能否使系统从初始状态转移到要求的状态 【能控性问题】 在有限时间内，能否通过对系统输出的测定来估计系统的初始状态 【能观性问题】 对于一个线性离散系统状态空间表示：$$\begin{equation}\left\{\begin{array}{lr} \pmb X(k+1) = F \pmb X(k) + G\pmb u(k) \ \pmb y(k) = C\pmb X(k)\end{array}\right.\end{equation}$$能控性定义：如果存在控制向量序列$\pmb u(k),\pmb u(k+1),\dots,\pmb u(N-1)$,使得系统从第$k$步的状态向量$\pmb X(k)$开始，在第$N$步达到零状态，即$\pmb X(N) =0$，其中$N$是大于$k$的有限数，称此系统在第$k$步上是能空的。 如果对于每个$k$值，系统的所有状态都是能控的，称系统状态是完全能控的，简称能控。 能控性判据：完全能控的充要条件是矩阵$\pmatrix{G &amp; FG &amp; F^2G &amp; \dots &amp; F^{n-1}G}$的秩为$n$，该矩阵也称为系统的能控性矩阵，以$Q_c$表示，能控性判据为：$$rank Q_c = rank \begin{bmatrix}G &amp;FG &amp;F^2G &amp;\dots &amp; F^{n-1}G\end{bmatrix} = n$$能观性定义：在已知输入变量$\pmb u(k)$的情况下，若能根据第$i$步及$n-1$步的输出观察值$\pmb y(i),\pmb y(i+1),\dots,\pmb y(i+n-1)$唯一能确定出第$i$步上的状态$\pmb X(i)$，称系统在第$i$步上是能观测的。 如果系统在任何$i$步上都是能观测的，称系统状态是完全能观测的，简称能观测。 能观性判据：完全能观性的充要条件是矩阵$\pmatrix{G &amp; FG &amp; F^2G &amp; \dots &amp; F^{n-1}G}^T$的秩为$n$，该矩阵称为系统的能观性矩阵，以$Q_o$表示，能观性判据为：$$rankQ_o = rank\left[\begin{matrix}C \ CF \ \vdots \ CF^{n-1}\end{matrix}\right]=n$$ 状态可测时按极点配置设计控制规律 系统的动态性能：完全取决于系统闭环传递函数的极点。 极点配置法的基本思想：由系统性能要求确定闭环系统期望的极点位置，然后依据期望极点位置确定反馈增益矩阵。 极点配置设计的控制器包括两部分：一部分是状态观测器，根据所测量到的输出量$y(k)$重构出全部状态$\hat{x}(k)$；另一部分是控制规律，直接反馈重构的全部状态。 设控制对象的状态方程为：$$\mathbf x(k+1) = F\mathbf x(k) +G \mathbf u(k)$$ 其中$\pmb x \in R^n,\pmb u\in R^m$，控制规律为线性状态反馈，即：$$\pmb u(k) =-L \pmb x(k)$$如何设计反馈控制规律$L$，使得闭环系统具有所需要的极点配置。 闭环系统的状态方程为：$$\pmb x(k+1) = (F-GL)\pmb x(k)$$闭环系统的特征方程为：$$|z I-F+GL|=0$$设所需要的闭环系统的极点为$\beta_i(i=1,2,\dots,n)$，得到闭环系统的特征方程为：$$\alpha_c(z) = (z-\beta_1)(z-\beta_2)\dots(z-\beta_n)= z^n +\alpha_1z^{n-1}+\dots + \alpha_n =0$$结合上面两个式子，可得到反馈控制$L$应满足的方程：$$|zI - F+GL| = \alpha_c(z)$$对于任意的极点配置，$L$具有唯一解的充要条件是控制对象完全能控，即：$$rank\begin{bmatrix}G &amp;FG&amp; \dots &amp; F^{n-1}G\end{bmatrix}=n$$物理意义：只有当系统的所有状态都是能控的，才能通过适当的状态反馈控制使得闭环系统的极点配置到任意指定的位置上。 关于解反馈控制$L$的方程方法有： 1、首先根据相应连续的系统性能指标的要求来给定$s$平面中的极点，然后再根据$z_i = e^{s_i T}(i=1,2,\dots,n)$的关系来求得$z$平面的极点分布，其中$T$为采样周期。 2、如果将闭环系统的极点均配置在原点，即$\alpha_c(z) = z^n$，最后会得到最小拍控制。所有的状态在经过最多的$n$拍后变能回到零（平衡状态）。采用最小拍控制避免了给定闭环系统极点的困难。有一个缺点：当选取较小的采样周期时，要求很大的控制量。 3、直接方法，展开反馈控制$L$的方程的左边行列式，通过与已知的$\alpha_c(z)$比较系数得到$L$各个元素，此法对于低阶系统合适，对于高阶系统十分困难。 4、对控制对象的离散状态方程$\pmb x(k+1) = F\pmb x(k) + G \pmb u(k)$实行非奇异变换$\bar{\pmb x}(k) = P \pmb x(k)$，使得控制对象的状态方程变为能控标准型。具体如下：$$\bar{\pmb x}(k+1) = \bar{F}\bar{\pmb x}(k) + \bar{G}\pmb u(k)$$其中：$$\bar{F} = PFP^{-1} = \pmatrix{0 &amp; \dots &amp; \ \vdots &amp; I_{n-1}&amp; \ 0 &amp; &amp; \-a_n &amp;\dots &amp; -a_1},\bar{G} = PG = \pmatrix{0 \ \vdots \\0\\1}$$对于新的状态$\bar{\pmb x}(k)$，控制规律变为：$$\pmb u(k) = - \bar{L}\bar{\pmb x}(k)$$其中：$$\bar{L} = L P^{-1}$$这样有：$$\bar{\pmb x}(k+1) =(\bar{F} - \bar{G}\bar{L})\bar{\pmb x}(k) = H\bar{\pmb x}(k)$$ $$H= \pmatrix{0 &amp; \dots &amp; \ \vdots &amp; I_{n-1}&amp; \ 0 &amp; &amp; \-a_n &amp;\dots &amp; -a_1}-\pmatrix{0 \ \vdots \\0\\1}\pmatrix{\bar{L}_1 &amp; \dots &amp; \bar{L}_n } = \pmatrix{0 &amp; \dots &amp; \ \vdots &amp; I_{n-1}&amp; \ 0 &amp; &amp; \-(a_n+\bar{L}_1) &amp;\dots &amp; -(a_1+\bar{L}_n)}$$ $H$为特征多项式的伴随矩阵形式，得到闭环系统的特征方程为：$$z^n + (a_1 + \bar{L}_n) z^{n-1} + \dots + (a_n+ \bar{L}_1)=0$$又因为：$\alpha_c(z) = z^n +\alpha_1z^{n-1}+\dots + \alpha_n =0$，可得到：$$\bar{L}_1 = a_n-a_n,\dots,\bar{L}_n = a_1-a_1$$向量形式：$$\bar{L} = \pmatrix{a_n &amp;a_{n-1} &amp; \dots &amp; a_1} - \pmatrix{a_n &amp;a_{n-1} &amp; \dots &amp; a_1}$$可得到所需要的反馈系数矩阵为：$$L = \bar{L} P$$关键在于非奇异矩阵$P$的计算。 最后可得到：$$L = \pmatrix{0&amp; \dots 0 &amp;1} \pmatrix{G&amp;FG &amp; \dots &amp; F^{n-1}G}^{-1}\alpha_c(F)$$上式是利用极点胚子设计控制规律的实用算法，称为Ackermann公式。 按极点配置设计观测器实际上，按极点配置设计控制规律时，所有的状态不能直接用于反馈。 常用的方法是：找到一个算法，能够根据所测量到的输出量重构出全部状态，记$\hat{\pmb x}(k)$为实际状态$\pmb x(k)$的重构或称为$\pmb x(k)$的估计。用$\pmb u(k) = -L\hat{\pmb x}(k)$代替实际状态的反馈。 这种能够根据输出量来重构系统状态的算法称为观测器。 状态不可测时控制器的设计分离性原理闭环系统的$2n$个极点由两部分组成：一部分是按极点配置设计控制规律所给定的$n$个极点即控制极点。另一部分是按极点配置设计观测器给定的$n$个极点即观测极点。 根据分离性原理可以使得控制规律和观测器的设计分开设计，简化。 控制极点是根据系统的性能要求设计的，因此，闭环系统的性能应主要取决于控制极点，即控制极点应是整个闭环系统的主导极点。 观测器的引入，通常会使得系统的性能变差，为减少观测器极点的影响，观测器的极点所决定的状态重构的跟随速度应远远大于控制极点所决定的系统响应速度，极限状态下，观测器极点放置在原点，此时，状态重构具有最快的跟随速度。 按极点配置控制器设计步骤1、按对系统的性能要求给定$n$个控制极点 2、按极点配置设计出控制规律$L$ 3、合适给定观测器的极点，对全阶观测器给定$n$个极点，对于降阶观测器给定$n-1$个极点。若测量不存在较大的误差或噪声，可考虑将所有的观测器极点放置在原点；反之，考虑按状态重构的跟随速度比控制极点所对应的系统响应速度快4-5倍的要求给定观测器的极点。 4、选择所采用的观测器类型。 ​ 若测量较准确，而且测量量便是其中一个状态，考虑选用降阶观测器，否则选用全阶观测器。若控制器的计算延时和采样周期的大小处于同一量级，可考虑采用预报观测器，否则考虑采用现时观测器。 5、根据给定的观测器极点及所选定的观测器类型计算增益矩阵$K$ 6、根据所设计的控制规律及观测器由计算机加以实现。 先进控制规律的设计方法先进控制的任务是用来处理常规控制效果不好，甚至无法控制的复杂工业过程控制的问题。 先进控制的主要特点是： 1、是一种基于模型的控制策略，可以是一种精确的数学模型，如最优控制。可以是一种不精确的模型，如自校正控制、模型预测控制，或者是基于知识的控制，如模糊控制。 2、通常用于处理复杂的多变量过程控制问题，如大时滞、多变量耦合、被控变量与控制变量存在各种约束等。 3、实现需要足够的计算能力作为支撑平台。 线性二次型最优控制器设计极点配置法主要设计参数是闭环极点得位置，仅限于说明单输入单输出系统。 最优控制寻求一种最优控制策略，使某一性能指标最佳，这一性能指标常以对状态及控制作用得二次型积分（称为代价函数）表示。称为线性二次型LQ（linear quadratic）控制问题。 LQ控制是状态反馈，不仅能用于单输入单输出系统，同时也能用于多输入多输出及时变系统。 设线性时不变系统得离散状态方程为：$$\pmb x(k+1) = F\pmb x(k) + G\pmb u(k)$$初始条件是：$\pmb x(0) = \pmb x_0$；$\pmb x(k)$是$n$维状态向量；$\pmb u(k)$是$m$维控制向量；$F,G$分别是$n \times n,n\times m$系数矩阵。 给定二次型性能指标函数：$$J= \pmb x^T(N) Q_0 \pmb x(N)+ \sum\limits_{k=0}^{N-1}[\pmb x^T(k)Q_1\pmb x(k) + \pmb u^T(k) Q_2\pmb u(k)]$$其中，$Q_0,Q_1$是非负定对称阵，$Q_2$是正定对称阵。 要求确定控制序列$\pmb u(k)(k=0,1,\dots,N)$，使得$J$所示的性能指标函数极小，这样的控制序列$\pmb u(k)$为线性二次型控制问题的最优控制。$\pmb x(k)$为相应的最优轨迹，$J$为最优性能值。 线性最优控制问题有： 1、有限时间最优问题，此时末时刻$N$固定且有限 2、无线时间最优问题，$N=\infty$ 线性二次型最优控制问题可分为： 1、调解问题。综合$\pmb u(k)$，使得系统由初始状态$\pmb x(0)$转移到平衡状态$\pmb x_e =0$，同时使得性能指标$J$极小，称为LQR(linear quadratic regular)问题。 2、跟踪问题。综合$\pmb u(k)$，使得系统输出$\pmb y(k)$跟踪某参考信号$\pmb y_r(k)$，同时使相应的二次型性能指标$J$极小。 自校正控制器设计自适应控制器本身具有逐步减小系统不确定的能力，本身能不断地检测系统参数或运行指标，根据参数的变化或运行指标的变化，改变控制参数或改变控制作用，使得系统运行于最优或接近最优工作状态。 自校正基本思想： 将参数递推估计算法与对系统运行指标的要求结合起来，形成一个能自动校正控制器参数的实时计算机控制系统。 自校正组成部分： 1、参数估计器。根据对象的输入$u$和输出$y$的实测数据，用在线递推辨识方法，辨识被控对象的参数向量$\theta$和随机干扰的数学模型 2、控制器参数计算。按照辨识求得的参数向量估计值$\hat{\theta}$，计算控制器的参数 3、控制器。按照辨识求得的参数向量估计值$\hat{\theta}$和对系统运行指标的要求，随时调整调节器或控制器参数，给出最优控制$u$，使得系统适应于本身参数的变化和环境干扰的变化，处于最优工作状态。 设计自校正控制器主要问题： 用递推辨识算法辨识系统的参数，然后根据系统运行指标来确定控制器的参数。 在实际应用中，常以递推最小二乘法为参数估计方法，以最小方差为控制目标函数。 最小二乘法参数辨识算法==一次完成最小二乘法== 设被辨识的系统模型为：$$A(z^{-1})y(k) = B(z^{-1})u(k) + \xi(k)$$其中：$$A(z^{-1}) = 1+a_1 z^{-1} + \dots + a_n z^{-n}$$ $$B(z^{-1}) = b_0 + b_1 z^{-1} + \dots + b_m z^{-m}$$ $z^{-1}$为时间向后平移算子，$y(k),u(k)$为系统的输出和输入，$\xi(k)$为不可测随机干扰，$n,m$为模型阶次。 上述模型是受控自回归积分滑动平均(CARIMA)模型中$C(z^{-1}) = 1$形式。 把待估计的模型参数和$k$时刻以前的观测数据记为向量形式，有：$$\pmb \theta = \begin{bmatrix} a_1,a_2, \dots ,a_n,b_0,b_1,\dots,b_m \end{bmatrix}^T$$ $$\varphi^T(k) = \begin{bmatrix} -y(k-1),\dots,-y(k-n),u(k),\dots,u(k-m)\end{bmatrix}$$ 辨识系统模型可写为：$$y(k) = \varphi^T(k) \pmb \theta + \xi(k)$$把$k=1,2,\dots,n,\dots,n+N$的全部数据代入可得到$N$个方程，矩阵形式表示为：$$\pmb Y_N = \pmb \Phi_N \pmb \theta +\pmb \xi_N$$其中：$$\pmb Y_N = \begin{bmatrix}y(n+1) \ y(n+2) \ \vdots \ y(n+N)\end{bmatrix}_{N\times 1} , \pmb \xi_N = \begin{bmatrix} \xi(n+1) \\\xi(n+2) \ \vdots \ \xi(n+N)\end{bmatrix}_{N\times 1}$$最小二乘法估计原理：从模型参数向量$\pmb \theta$中找到估计量$\hat{\pmb {\theta}}$，使得模型的输出与实际输出之间的误差的平方和最小。估计准则为：$$J= \sum\limits_{k=n+1}^{n+N}[y(k)- \varphi^T(k) \hat{\pmb{\theta}}]^2 =\pmb (Y_N - \pmb \Phi_N \pmb{\hat{ \theta}})^T (Y_N - \pmb \Phi_N \pmb{\hat{ \theta}})$$两边对$\pmb {\hat{\theta}}$求导，得到使得$J$最小的$\pmb {\hat{\theta}}$。可得：$$\pmb {\hat{\theta}} = (\pmb{\Phi_N^T} \pmb{\Phi_N})^{-1}\pmb{\Phi_N^T}\pmb Y_N$$若获得$k=1,2,\dots,n,\dots,n+N$的全部观测数据后，估计量$\pmb {\hat{\theta}}$一次计算出来，称为一次完成最小二乘算法。 ==递推最小二乘算法== 一次完成最小二乘法需要存储全部观测数据，随着$N$增大，相应的计算量和存储空间将迅速增加。解决最小二乘法的在线辨识问题。 在进行$n+N$次观测后，又获得了一组新的观测数据$\{u(m+N+1),y(n+N+1)\}$,可构成$\pmb Y_{N+1},\pmb \Phi_{N+1}$，并计算出$\pmb {\hat{\theta}}_{N+1}$。 递推最小二乘法公式为：$$\cases{\pmb {\hat{\theta}}(k) = \pmb {\hat{\theta}}(k-1)+\pmb K(k)[y(k)- \varphi^T(k) \hat{\pmb{\theta}}(k-1)]\ \ \pmb K(k) = \pmb P(k-1)\varphi(k)[1+\varphi^T(k)\pmb P(k-1)\varphi(k)]^{-1}\ \ \pmb P(k) = [I-\pmb K(k)\varphi^T(k)]\pmb P(k-1) }$$公式物理意义： 新的参数估计值$\pmb {\hat{\theta}}(k)$是由前一步的估计值$\pmb {\hat{\theta}}(k-1)$和修正项组成，修正项正比于新的观测数据$y(k)$与前一步模型预测量$\varphi^T(k)\pmb {\hat{\theta}}(k-1)$的偏差，$\pmb K(k)$为修正系数矩阵，$\pmb P(k)$正比于参数估计误差的方差，$\pmb P(k)$越大表示参数估计值越不准确，越小表示参数估计值越接近真值。 ==带遗忘因子的递推最小二乘法== 当系统参数随时间变化时，新数据比老数据更能反映参数变化的状况，因此要使参数估计能够适应系统参数的时变特性，需要用指数加权的方法来逐渐削弱或“遗忘”老数据的影响。 带遗忘因子的最小二乘法公式为：$$\cases{\pmb {\hat{\theta}}(k) = \pmb {\hat{\theta}}(k-1)+\pmb K(k)[y(k)- \varphi^T(k) \hat{\pmb{\theta}}(k-1)]\ \ \pmb K(k) = \pmb P(k-1)\varphi(k)[\lambda+\varphi^T(k)\pmb P(k-1)\varphi(k)]^{-1}\ \ \pmb P(k) = \frac{1}{\lambda}[I-\pmb K(k)\varphi^T(k)]\pmb P(k-1) }$$遗忘因子$\lambda$越小，表示遗忘越快，越重视当前数据，越能反映当前系统的变化，适用于参数变化速度相对于辨识速度较快的时变系统。 $\lambda$越大，重视更多的历史数据，得到更多的系统信息，辨识精度越高，适用于参数变化速度远低于辨识速度的慢时变系统。 ==增广最小二乘递推算法== 被辨识的系统的模型为CARMA形式，且其$C(z^{-1} ) \ne 1$，即：$$A(z^{-1})y(k) = B(z^{-1})u(k) +C(z^{-1})\xi(k)$$其中：$C(z^{-1}) = 1+ c_1 z^{-1} + \dots + c_n z^{-n}$ $\xi{(k)}$是零均值白噪声序列，多项式$C(z^{-1})$各项系数未知，需要辨识。这类模型参数的辨识可采用增广最小二乘法来获得模型未知参数的估计。 增广最小二乘法递推算法公式为：$$\cases{\pmb {\hat{\theta}}(k) = \pmb {\hat{\theta}}(k-1)+\pmb K(k)[y(k)- \hat{\varphi}^T(k) \hat{\pmb{\theta}}(k-1)]\ \ \pmb K(k) = \pmb P(k-1)\hat{\varphi}(k)[1+\hat{\varphi}^T(k)\pmb P(k-1)\hat{\varphi}(k)]^{-1}\ \ \pmb P(k) = [I-\pmb K(k)\hat{\varphi}^T(k)]\pmb P(k-1) }$$增广最小二乘法不仅能获得系统控制通道模型参数估计，还能获得噪声通道模型的参数估计，算法与最小二乘法基本相同，不同的只是参数向量$\pmb {\theta}$和数据向量$\varphi^T(k)$的维数扩充了，每次估计都需要计算一次噪声估计值$\hat{\xi}(k)$。 自校正控制器设计基本思想： 将参数估计递推算法与各种不同类型的控制算法相结合，以形成能自动校正控制器参数的实时计算机控制系统。 模型预测控制器设计模型预测控制（MPC）主要特征： 以预测模型为基础，采用二次在线滚动优化性能指标和反馈校正的策略，克服受控对象建模误差和结构、参数与环境等不确定因素的影响。 研究现状： 预测控制开始与极点配置、自适应控制、鲁棒控制、精确线性化、解耦控制和非线性控制结合； 并且随着智能控制技术的发展，预测控制向着智能预测控制方向发展，如：模糊预测控制、神经网络预测控制、遗传算法预测控制以及自学习预测控制等； 将人工智能、大系统递阶原理等引入预测控制，构成多层智能预测控制的模式。 核心： 基于滚动时域原理，算法中包含了预测模型、滚动优化和反馈校正三个基本原理，即 1、在当前时刻，基于过程的动态模型，对未来某段时域内的过程输出序列做出预测，预测值是当前和未来控制作用的函数。 2、按照某个目标函数确定当前和未来控制作用大小，控制作用将使未来输出预测序列沿着某个参考轨迹“最优地”达到期望的输出设定值，但只实施当前控制量。 3、在下一时刻，根据最新实测数据对前一时刻的过程输出预测序列做出校正，并重复1，2。 预测模型是以对象的内部模型，即对象在脉冲或阶跃信号作用下的时间响应为基础，用以估计系统在输入序列作用下的输出。 预测控制算法常用的主要是模型算法控制和动态矩阵控制两种。 ==模型算法控制==（MAC）基本上包括四个部分：预测模型、反馈校正、参考轨迹和滚动优化。 ==动态矩阵控制==（DMC）是一种基于对象阶跃响应的预测控制算法，适用于有时滞、开环渐近稳定的非最小相位系统。算法包括：预测模型、反馈校正和滚动优化。 模糊控制器设计模糊控制器不要求掌握被控对象的精确数学模型，根据人的经验规则组织控制决策表，由该决策表决定控制量的大小。 模糊数学模糊控制原理核心部分是采用模糊控制器。 模糊控制器主要包括：输入量的模糊化接口、知识库、推理机和输出清晰化接口四个部分。 ==模糊化== 仿照人的思维进行模糊控制，必须把输入通道采样得到的精确量变成模糊推理需要的模糊量，通过模糊化接口完成。 ==知识库== 由数据库和规则库两部分组成。 数据库：模糊控制器而得输入变量、输出变量经模糊化处理后，其全部模糊子集的隶属度或隶属函数存放在模糊控制器的数据库中。 规则库：用来存放全部模糊控制规则，在推理时为“推理机”提供控制规则。 ==推理机== 模糊控制器中，根据输入模糊量和知识库（数据库、规则库）完成模糊推理并求解模糊关系方程，从而获得模糊控制量的功能部分。 模糊控制规则是模糊决策，是人们在控制生产过程中的经验总结。 ==清晰化接口== 被控对象每次只能接收一个精确的控制量，需要将模糊控制量转换成精确量，称为去模糊。方法有：最大隶属度方法、加权平均法和中位数判决法等。 模糊PID控制器设计 采用的是双模糊控制器（FC1、FC2）结构，$r,y$分别为系统的设定值和输出，$d$为外部扰动输入，$e$为设定值和输出的偏差，$\Delta e$为系统输出$y$的增量。 网络控制系统(NCS)网络控制在计算机控制系统的基础上，在控制器和被控对象之间加入通信网络（有线或无线网络），使得传感器到控制器的反馈通道信息传输和控制器到执行器的前向通道信息传输通过通信网络进行，实现对被控对象的计算机远程控制。 通信网络的不确定性和复杂性，相当于在控制器和控制对象之间增加了一个不确定的动态环节，$p_k^{sc}(·)$表示反馈通道的动态特性，$p_k^{ca}(·)$表示前向通道的动态特性。 反馈通道的信息$y(k)$经过网络传输到控制器变成$\bar{y}(k) = p_k^{sc}[y(k)]$； 前向通道的信息$u(k)$经过网络传输到执行器变成$\bar{u}(k) = p_k^{ca}[u(k)]$ 网络的加入使得控制对象结构发生了变化，增加了通信接收器和通信发送器。 通信接收器：包括通信网络接口和D/A转换器，作用于执行器 通信发送器：包括A/D转换器和通信网络接口，变成数字信号，通过网络接口将数字信号打包发送到网络上。 通信网络引入控制系统优劣： 连接智能现场设备和自动化系统，实现现场设备控制的分布化和网络化，具有信息资源共享、连接线数大大较少、易于扩展、易于维护、高效、可靠和灵活； 增加控制系统的复杂性，由于网络通信带宽、承载能力和服务能力的限制，使数据的传输不可避免存在时延、丢包、多包传输及抖动等诸多问题，导致系统性能下降甚至不稳定，给系统分析、设计带来很大的困难。 网络控制系统概述传感器、控制器和执行器机构通过通信网络形成闭环控制系统。NCS中控制部件间通过共享通信网络进行信息（对象输出、参考输入和控制器输出等）交换。 直接结构：控制信号封装在帧或报文中，通过网络发送到被控对象，被控对象传感器的测量数据同样以帧或数据包的形式通过网络发送到控制器。 典型应用包括远程学习实验室和直接电动机的速度控制等。 分层结构：主控制器通过网络将计算好的参考信号发送到远程系统，远程系统根据参考信号来执行本地的闭环控制，并将传感器测量数据返回主控制器。 网络控制回路具有比本地控制回路更长的采样周期，典型应用包括移动机器人、遥控操作系统、汽车控制以及航天器等。 对NCS评价的标准：网络服务质量（QoS）和系统控制性能（QoP）. QoS：包括网络吞吐量、传输效率、误码率、时延可预测性和任务的可调度性 QoP：与常规控制系统一样，包括稳定性、快速性、准确性、超调和振荡等。 网络控制系统的研究内容包括对网络的控制(control of network)、通过网络的控制(control through network)和着眼于网络控制系统总体性能指标的综合控制(integrated control and network)。 1、对网络的控制 围绕网络的服务质量、从拓扑结构、任务调度算法和介质访问控制层协议等不同角度提出解决方案，满足系统对实时性的要求，减小网络时延、时序错乱、数据包丢失等一系列问题。 可以通过运筹学和控制理论的方法来实现。 包括：NCS体系结构和通信协议的研究、NCS时延分析和网络调度、NCS数据包的传送问题、NCS中带通信约束的控制问题、NCS的系统与信息的集成。 2、通过网络的控制 在现有的网络条件下，设计相适应的NCS控制器，保证NCS良好的控制性能和稳定性。 可以通过建立NCS数学模型并运用控制理论方法进行研究。 NCS数学模型： NCS中被控对象的连续工作状态方程为：$$\cases{\dot{\pmb x}(t) = A\pmb x(t) +B\pmb u(t-\tau) \ \ \pmb y(t) = C\pmb x(t) }$$其中$\tau$为网络时延。 节点工作方式分为时间驱动和事件驱动两种，根据节点不同的工作方式，可以得到不同的系统离散事件模型。 NCS的常规控制规律设计方法： 将NCS看成一个参数或时延时变系统，将常规的PID控制、Smith预估控制等，进行相应的改进，使之适应NCS控制性能的要求。 NCS先进控制规律设计方法： 包括极点配置设计方法、最优化控制设计方法、鲁棒控制设计方法、智能控制设计方法等。基本思想：利用先进的控制律设计方法，设计网络控制系统的控制器，适应网络控制系统复杂特性的情况，进一步提高网络的控制性能。 3、综合控制 协同考虑控制与调度：NCS闭环性能不仅依赖于控制算法的设计，还依赖于网络资源的调度，需要同时考虑网络协议和控制器的设计。 NCS并行计算：存在大量费时的计算密集问题，并行计算需要多处理器协同工作和相互通信，NCS为并行计算提供必要的硬件环境。需要设计适合控制系统的并行算法。 实时控制网络控制网络的实时性一般采用两种技术来实现： 1、简化技术。将网络形式简化成线型，将通信模型简化为只有物理层、数据链路层和应用层，将节点信息简化到只有几比特。 2、采用网络管理和数据链路调度技术。分时式实时系统的响应具有可预知性，但资源利用率低；抢先式实时系统资源利用率高，但响应具有不可预知性。很多控制网络将两者结合，达到某种平衡。 控制网络的拓扑结构网络拓扑指网络形状，物理上的连通性。以星型拓扑、总线拓扑、环型拓扑三种结构为基础。 拓扑： 把实体抽象为与其大小、形状无关的“点”，把连接实体的线路抽象为“线”，进而以图的形式来表示点与线之间的关系的方法。目的是在于研究这些点、线之间相连的关系。 表示点和线之间的关系的图称为拓扑结构图。 几何结构：考察的是点、线之间的位置关系，强调的是点和线所构成的形状及大小。不同的几何结构可能具有相同的拓扑结构。 网络拓扑结构：在计算机网络中，把计算机、终端、通信处理机等设备抽象成点，把连接这些设备的通信线路抽象成线，并将由这些点和线所构成的拓扑称为网络拓扑结构。 网络拓扑反映网络的结构关系。常见有总线型、星型、环型、树型和网状型等。 参考博客 星型网络：各站点通过点到点的链路与中心站相连，数据的安全性和优先级容易控制，易实现网络监控，但中心节点的故障会引起整个网络瘫痪。 总线型网络：所有站点共享一条数据通道，安装方便，成本低，但介质的故障会导致网络瘫痪，安全性低，监控困难。 环型网络：各站点通过通信介质连成一个封闭的环型，容易安装和监控，但容量有限，网络建成以后，难以增加新的站点。 树型拓扑：从总线拓扑演化而来，树根接收各站点发送的数据，然后广播发送到全网。 网型拓扑：通信站点的互连结构 混合拓扑：将两种或两种以上的单一拓扑结构混合起来，取两者的优点构成的拓扑。 控制网络的协议模型开放系统互连(OSI)参考模型共分为七层：物理层、数据链路层、网络层、传送层、会话层、表示层和应用层。 控制网络一般由OSI参考模型的物理层、数据链路层、应用层三层体系结构和通信媒质构成。 1、物理层。采用EIA-RS-232、EIA-RS-422/RS-485等协议。 2、数据链路层。考虑现场设备故障较多，更换频繁，数据链路层媒体访问控制多采用受控访问协议（包括轮询和令牌）协议，通常各PC、PLC作为主站，传感器、变送器等作为从站。需要支持点对点、点对多和广播通信方式。 3、应用层。解决应用怎样的高级语言（或过程控制语言）来作为面向用户的编程（或组态）语言，包括设备名称、网络变量与配置（捆绑）关系，参数与功能调用及相关说明，一般应具有符合IEC1131-3标准的图形用户界面（GUI）。 控制网络的媒体访问技术媒体访问技术是挂在通信子网上的站点向通信介质存信息或者从通信介质上取信息的控制规则。 是对媒体的使用进行管理，将传输介质的频带有效地分配到网络上各个站点的方法，位于数据链路层中。 媒体访问控制是通信子网的核心内容，各个网络的性能在很大程度上取决于所采用的媒体访问控制方式，受其直接影响的性能有：网络的实时性、网络的吞吐量和有效利用等。 媒体访问控制技术： 有冲突的媒体访问控制：在站点访问媒体时，可能会由于多个站点同时访问媒体而产生冲突，应用较多的协议：带有冲突检测的载波监听多路访问的CSMA/CD控制。 无冲突的媒体访问控制：在网络上的站点访问传输媒体时，不会发生多个站点同时访问媒体的情况。应用较多的控制协议：令牌控制和主从控制。 控制网络的类型按通信方式分为：主从型、客户与服务器型、环型和通信型四种类型。 主从型：通信由一个主站控制，主站轮询方式对子站“逐个”通信，简单，但危险集中，主站出问题，整个系统瘫痪。 客户与服务器型：面向“事件”方式构建，无主站，有需要通信的“事件”发生，优先通信，即时响应好，是发展的主流方向。 环型：所有的通信子站连成“一个环”，以数据移位的方式通信，简单有效，传输效率是目前控制网络最高的，但一个站出问题，通信中断。 通信型：带有完整的OSI网络通信模型，是通信功能最强的控制网络。 按通信协议的特点分为：CSMA方式、TokenBus方式和主从Polling方式。 CSMA方式：Ethernet、CAN、DeviceNet和LONWorks等 TokenBus方式：Profibus、P-Net及ControlNet等 主从Polling方式：FIP、CC-link以及某些专用的主从RS-422/RS-485网络 EtherNet网络采用的是有冲突的媒体访问控制方式的协议，工作过程是完全随机的，既不预先规定时间，也不预先建立每个节点传送信息的先后顺序，而是根据各个节点和传输线路的具体情况来确定。 网络监听算法（CSMA）是CSMA/CD的核心，是一种“先听后讲”的算法，一个准备发送报文的站首先监听网络，以确定网络上是否有其他的站点在发送信号，即确定网络是否空闲。 1、非坚持的CSMA：网络空闲，则可以发送数据。如果网络忙，则等待由概率分布决定的、一定量的随机延迟时间，再监听网络的情况，即在这段随机延迟的时间内不监听网络。网络利用率低。 2、1-坚持CSMA：网络忙就一直监听，直到网络空闲，便立即发送数据。若有冲突，则等待一个随机时间间隔，重新监听。不可避免地有冲突发生，不利于网络吞吐量的提高。 3、P-坚持CSMA：网络忙，站点继续监听直到网络空闲，但发现网络空闲时，不立即发送数据，为减少冲突，以概率P发送数据，以概率（1-P）延迟一个时间单位，再监听网络。这个时间单位通常为网络上最大传播延迟的两倍，P值选择过大，增加冲突，P值过小，媒体利用率大大降低。 EtherNet网络作为最为广泛应用的网络协议，将成为过程级和控制级的主要传输技术。EtherNet能够与工厂信息管理系统进行直接地、无缝地连接，而不需要任何专用设备。 CAN网络CAN(controller area network)是串行总线控制器局域网络，与一般的通信网络相比，CAN的数据通信具有突出的可靠性、实时性和灵活性的特点。 通过相应的CAN接口连接工业设备，构成低成本的网络。直接连接不仅提供了设备级故障诊断通通道，而且提高了通信效率和设备互换性。 CAN应用一种面向无损伤仲裁方法来解决媒体多路访问带来的冲突问题。仲裁过程是： 当网络空闲时，线路表现为“隐性”电平（recessive level），此时任何节点均可发送报文。发送节点发出的帧起始字段产生一个“显性”电平（dominant level），标志着发送开始。所有节点以首先开始发送节点的帧起始前沿来同步，若多个节点同时发送，在发送的仲裁场进行逐位比较。 网络仲裁：当网络多个节点同时发送 网络请求时，网络应如何做出选择，以保证网络最多只能被一个节点传送数据占据。 CAN仲裁原则： （1）每个节点只能使用唯一的标识符发送数据 （2）对于任一标识符，只有具有相应标识符的节点发送带有此标识符的数据帧，而其他节点只能发送带有此标识符的远程帧 （3）不同的节点发送相同的标识符的远程帧时，必须由唯一确定的请求节点发送，或者不同的节点配以不同的DLC（数据帧长度）。 ControlNet网络是基于改型的CAN技术的一种高速确定性网络。作为一种高速串行通信系统和一种确定加预测的模式进行运作，适用于需要实时应用信息交换的设备之间的通信。 ControlNet网络是一种用于对信息传送有时间苛刻要求的、高速确定性网络，同时，它允许传送无时间苛求的报文数据。 ControlNet采用的令牌总线（TokenBus）控制方式。是最为普遍的无冲突介质访问控制协议。是将CSMA/CD和令牌环两种协议相结合，取其优点。 TokenBus只有取得令牌（一组具有特定格式的位）的站点才能发送数据帧，其他节点只能接收信息，或者被动地传送信息（在拥有令牌的节点的要求下发送信息）。发送完后，站点将令牌传递给下一个站点。 在分布式主站系统中，ControlNet应用时间触发方式采取一个特殊的令牌传递机制： 网络上每个节点分配一个唯一的物理地址，持有令牌的节点可以发送数据，但是，网络上并没有真正的令牌在传输，而是每个节点监视收到的其数据帧的源节点地址，在该数据帧结束之后，每个节点设置一个隐性的令牌寄存器，其值为收到的物理地址+1，如果隐性令牌寄存器的值等于某个节点的物理地址，该节点发送数据。 网络中存在唯一的隐性令牌，在由主站构成的逻辑环中循环传递，主站之间通过一定的逻辑调度算法获得令牌的调度权，然后由它发起对其他主站或所属的从站的通信。 令牌轮询技术：通过任务调度来满足周期数据的不同实时性要求，既有静态的事先约定，也可以动态调整。优点是周期任务的实时性好且具有确定的或可预测的最大网络时延。主要缺点：无法处理突发事件。 ControlNet应用了数据分类技术解决这个问题： （1）周期性数据：对应周期数据，按时传输，如数据采样 （2）事件性通信：对应突发数据，在事件发生时传输，如报警 （3）报文通信：对应非实时数据，被请求时传输，如程序下载 网络的媒体存储通过限制时间存储算法来控制，即：采用并行时间域多路存取（CTDMA），控制各个节点在网络刷新时间（NUT）内传送信息的机会。 NUT包括三个部分：预先信息传送时间、非预定信息传送时间和网络维护时间。 通信技术：ControlNet采用的是多信道广播技术，如生产者/客户模式。这种通信模式是基于多信道广播的，允许网络上的所有节点，同时从单个数据源存储相同的数据。 在控制网链路上，由生产者发送的每个报文都包含一个连接标识符，已经组态的节点在收听广播时，可以识别其应收报文的连接标识符，变成一个客户以接收数据。 若该节点要发送信息，变成生产者，提高了网络的效率，特别是对组态信息等公共数据，多信道广播技术是一种有效的通信方式。 CC-Link网络是高速现场总线，不但处理信息数据，也处理控制数据。通过简单的总线，可以将工业设备连接成为分布式控制系统或设备层的网络，同时这个总线可以方便地连接到其他网络。 CC-Link采用单主从轮询工作模式，控制方式：网络的各节点间明显具有上下级特征，上级为主导节点，并由主导节点分配通信媒体的使用权，适用于自动化中的分布式系统，是一种集中控制，主要用于星型网络。 CC-Link数据链路层通信协议：EIA485协议 网络控制系统控制器的设计PID网络控制器设计理想的模拟PID控制器的传递函数为：$$D(s) = \frac{U(s)}{E(s)} = K_P +\frac{K_I}{s} + K_ds$$其中，$K_P$为比例系数，通过加大$K_P$的值来增加系统动态响应速度；$K_I$为积分系数，消除 系统稳态误差；$K_D$为微分系数，改变系统的动态性能。 采用双线性变换法，将上式离散化，得到：$$D(z) = D(s)|_{s=\frac{2}{h} ·\frac{1-z^{-1}}{1+z^{-1}}} = \frac{U(z)}{E(z)} = K_P +\frac{K_Ih}{2}·\frac{1-z^{-1}}{1+z^{-1}} +\frac{2K_D}{h}·\frac{1-z^{-1}}{1+z^{-1}}$$其中，$h$为采样周期。 常规PID控制器应用于网络控制系统的环境，由于网络特性的影响，PID控制器所控制的广义对象（包括网络和控制对象）实际上是一个时变系统，因此需要对PID参数$K_P,K_I,K_D$进行在线修正，以适应于网络控制系统的要求。 极点配置网络控制器设计在NCS中，首先建立系统的随机时变模型，然后对模型进行一系列的处理后设计控制器。控制器设计分为 控制规律的设计和状态观测器的设计。 根据给定的闭环系统的极点和观测器极点分别设计控制规律和观测器。 控制规律作用：根据系统的状态产生控制量。 观测器作用：利用带延迟的部分状态信息估计系统的全部状态。 1、网络控制系统的模型分析 通信网络的存在使得数据传输变得不确定，相当于在闭环中增加了时变的不确定的控制对象。闭环引入网络后，被控对象有了扩展，是包含直接控制对象和通信网络的广义被控对象。 2、控制规律的设计 假设控制器反馈的是系统的全部增广状态，控制规律为线性状态反馈，即：$$\pmb u(k) = -L^k \pmb z(k)$$目的是设计$L^k$使得闭环系统具有所需要的极点配置。 3、状态观测器的设计 实际控制中，控制器不能得到所有对象的状态，只能得到被控对象的输出，因此，在控制器中，要用被控对象的输出来重构出系统的所有的状态，这需要设计状态观测器。 4、控制器的设计 控制器包含观测器和控制规律，目的是设计$L^k$使得闭环系统取得好的性能指标。基于广义的被控对象的控制器的设计依然遵循分离性原理。 即闭环系统的极点由两部分组成，一部分是按照真实状态反馈设计控制规律的控制极点；另一部分是状态观测器的极点。 计算机控制系统设计实现设计原则与设计方法硬件设计总体上包括三个方面内容：计算机部分、执行机构与驱动技术、检测机构与传感器技术。 常见的驱动技术： 采用电动机及其驱动器构成的电气传动系统直接驱动机械设备，在精度不高的情况下，一般采用通用型的传动系统，主要有：直流电气传动系统、交流电气传动系统、同步电动机传动系统等。 在高精度要求下，通常采用伺服驱动系统、直线电动机驱动系统等高精度传动系统。 检测机构与传感器技术： 传感器：能够感受规定的被测量并按照一定规律转换成可用输出信号的器件和装置，通常由敏感元件和转换元件组成。 在计算机控制系统中，对检测机构与传感器基本要求：检测精度，检测机构的量程以及适用的环境等。 软件设计包括监控软件、控制软件和信息管理软件三部分组成。 数字控制器的实现问题数字控制器的设计是计算机控制系统设计的关键所在，包括数字控制器的理论设计与程序实现两部分。 数字控制器的理论设计：是根据被控对象和所确定的计算机控制系统的指标，设计出数字控制器的输入和输出函数的差分方程数学表达式。 信号的数字滤波技术在模拟系统中，信号的处理是采用不同形式的有源或无源滤波器，如低通、高通、带通和带阻滤波器等。 在计算机控制系统中，采用数字滤波器，由软件编程实现。 1、均值滤波 在一个采样周期内，连续采样几个值，取其平均值为实际测量值。数学表达式为：$$Y = \frac{1}{N} \sum\limits_{i=1}^N X_i$$其中，$Y$是数字滤波器输出；$X_i$为第$i$次采样值；$N$为采样次数。 均值滤波实质是对信号的平滑处理，其平滑程度取决于采样次数$N$。$N$越大，计算结果越准确，但灵敏度降低，应折中处理。 2、中值滤波 连续采样三次$X_1,X_2,X_3$，去掉最大值和最小值，取中间值为本次采样值。数学表达式为：如果$X_1 \le X_2 \le X_3$，则中值滤波器的输出为$Y = X_2$。 中值滤波对去掉脉动性的干扰比较有效，但不宜用于快速变化的变量的处理。 3、抗干扰中值滤波 消除脉冲型干扰信号。 连续采样$N$次，得到$X_1,X_2,\dots,X_N$，按数值由小到大额排列$X_1 \le X_2 \le \dots \le X_N$，去掉最小值$X_1$和最大值$X_N$，然后对剩余的采样值进行均值运算，结果作为滤波器的输出$Y$，计算公式为：$$Y = \frac{1}{N-2}\sum\limits_{i=2}^{N-1}X_i$$4、限幅滤波 大的随机干扰或者采样电路的不稳定，使得采样数据明显偏离实际值，或者两次采样之间的变化很大。 基本思路：根据被滤波信号的实际变化范围及变化的频率，确定滤波器的参数，即上下极限幅度$Y_h ,Y_l$及变化极限$\Delta Y_0$，对于一个采样值$x(k)$，滤波器的输出$y(k)$输出为：$$y(k) = \cases{x(k), \qquad Y_1 \le x(k) \le Y_h,|x(k)-y(k-1)|\le \Delta Y_0 \ \ y(k-1) + \Delta Y_0 ,\qquad x(k) - y(k-1) &gt; \Delta Y_0 \ \ y(k-1) - \Delta Y_0 , \qquad x(k)-y(k-1) &lt; -\Delta Y_0 \ \ Y_h ,\qquad x(k) &gt; Y_h \ \ Y_1 , \qquad x(k) &lt; Y_1 }$$其中，滤波器参数$Y_h$和$Y_l$为被滤波信号的最大允许值和最小允许值，与被测信号的变换范围有关；$\Delta Y_0$与采样周期及被测信号的正常变化率有关。 限幅滤波器是针对特定的被测信号而设计的，加入了较多的人工干预成分，对参数选择有一定要求。 5、惯性滤波 是由连续域中的一阶惯性滤波器，经离散化处理后得到的。数学表达式为：$$y(k) = \alpha y(k-1) +(1-\alpha)y(k), \qquad 0&lt; \alpha &lt; 1$$滤波器当前时刻的输出值$y(k)$，是上次采样时刻的输出值$y(k-1)$与本次输入采样值$x(k)$的加权平均值，参数$\alpha$是$y(k-1)$的权，成为滤波系数。 $\alpha$越大，则表示上次滤波器的输出值在本次滤波器输出值中所占比重越大，即惯性越大。 数字控制器程序实现性能分析计算机控制系统数值误差的来源： 1、建立控制器离散数学模型时，需对其参数进行量化，因而会产生参数量化误差。 2、在信号转换过程中，由于A/D转换器的数字量字长有限，同时，受A/D转换时间和采样周期的影响，因此会在检测的反馈信号中产生采样量化误差。 3、因计算机控制系统的实时性要求，在计算机编程中所采用的数字量的字长也是有限的，会产生计算误差。 量化效应与采样周期误差分析A/D转换的量化误差是在一定范围内变化的随机变量，通常所称的量化误差是相应变化范围的最大值，假定A/D转换器对输入模拟信号的转换量程为$0-X_m$，转换字长为$n$，量化误差为：$$q = \frac{X_m}{2^n}$$ 孔径误差设$T$为采样周期，$T_c$为A/D转换器的转换时间，假定$T\ge T_c$。只在$T_c$时间内观察到输入信号$x(t)$的变化，相当于一个时间窗口，$T_c$称为$A/D$转换器的孔径时间。 A/D转换器的孔径时间对于变化的信号在转换过程中会形成一定的误差，若被转换的模拟信号不是一个恒定常量，而是随时间变化形成了连续时间函数$y(t)$，则在有一定的孔径时间$T_c$的情况下，A/D转换器将产生孔径误差。 最终的A/D转换的采样值为：$$y(t_2) = y(t_1) + \Delta y$$其中$\Delta y$为孔径误差，可以表示为：$$\Delta y = T_c \frac{dy}{dt}$$其中$T_c = t_2-t_1$。 若孔径误差过大，可通过选用转换速度更高的A/D转换器或者A/D之前加采样保持器来解决。 采样周期造成的误差一般采样周期$T$远大于A/D转换器的孔径时间$T_c$，对变换的模拟信号的影响远超A/D转换器的孔径时间的影响。 设被转换的模拟信号为正弦函数$x(t) = X_{max} \sin(\omega t + \theta_0)$，信号周期为$T_0 = \frac{2\pi}{\omega}$，采样周期为$T$时，$x(t)$的实际采样函数为：$$x^(t) = X_{max} \sin (\omega kT + \theta_0), \quad kT \le t &lt; kT +T$$设$\frac{T_0}{T} =m$，$m$为自然数，$x^(t)$是周期为$T_0$的周期函数，可表示为如下三角函数形式的傅里叶级数表达式：$$\cases{x^(t) = a_0 \sum\limits_{n=1}^{\infty} [a_n \cos(n\omega t) + b_n \sin(n\omega t)]\ \ a_0 = \frac{1}{T_0} \int_{t-T_0}^{t}x^(t) dt \ \ a_n = \frac{2}{T_0}\int_{t-T_0}^t x^(t) \cos (n\omega t) dt \ \ b_n = \frac{2}{T_0} \int_{t-T_0}^t x^(t) \sin (n\omega t)dt }$$采样前的模拟信号只有基波分量，分析采样前后的误差情况，实际上就是将实际采样函数$x^*(t)$的基波信号的幅值相位与模拟信号相比较，只需要求出基波的系数$a_1$和$b_1$。$$a_1 = \frac{2X_{max}}{T_0} \sum\limits_{i=k-m}^{k-1} \int_{iT}^{(i+1)T} \sin (\omega iT+ \theta_0) \cos(\omega t)dt \ = -\frac{mX_{max}}{\pi} \sin(\frac{\pi}{m}) \sin (\frac{\pi}{m} -\theta_0) +A$$其中，$A = \frac{X_{max}}{\pi} \sin (\frac{\pi}{m}) \sum\limits_{i=k-m}^{k-1} \sin \left(\frac{4\pi}{m} i + \theta_0 + \frac{\pi}{m}\right)$。$$b_1 = \frac{2X_{max}}{T_0} \sum\limits_{i=k-m}^{k-1} \int_{iT}^{(i+1)T} \sin (\omega iT+ \theta_0) \sin(\omega t)dt \ = \frac{mX_{max}}{\pi} \sin(\frac{\pi}{m}) \cos (\frac{\pi}{m} -\theta_0) +B$$其中，$B = -\frac{X_{max}}{\pi} \sin (\frac{\pi}{m}) \sum\limits_{i=k-m}^{k-1} \cos \left(\frac{4\pi}{m} i + \theta_0 + \frac{\pi}{m}\right)$。 可得到实际采样函数的基波$x^_1(t)$的幅值和相位，表达式为：$$x^_1(t) = \sqrt{a_1^2 + b_1^2} \sin (\omega t + \theta_1)$$其中，$\theta_1 = \arctan(a_1/b_1)$。 与原函数相比，幅值误差为：$$\Delta X = X_{max} - \sqrt{a^2 + b^2}$$相位误差为：$$\Delta \theta = \theta_0 - \theta_1$$ 计算机控制系统可靠性提高可靠性的设计原则： 1、保证系统的硬件和软件自身质量，降低自身损坏和冲突的概率 2、提高硬件和软件对外部信号的抗干扰、扛冲击能力 3、系统的硬件和软件要有冗余设计和备份 4、硬件和软件设计要使系统有故障诊断能力，将事故危害范围降到最低限度。 软件部分的措施： 1、采用模块化设计。对系统及软件功能的分析和研究，合理地进行软件的模块划分，并明确每个模块所要完成的功能及与其他模块的调用和被调用关系。 2、充分测试，避免软件的自身冲突。 3、对输入信号的抗干扰设计。软件中设计滤波器，抑制和消除干扰的影响 4、系统的软件应具有一定的故障诊断能力和自修复能力。 5、看门狗技术。是一种CPU监控定时器，看门狗的运行靠硬件和相应软件的配合实现。工作机理： ​ 在一个预先设定的时间内，若没有得到来自CPU的清零脉冲信号，则看门狗定时器将向CPU的复位端输出一个复位脉冲信号，这样可以使得因故障而导致程序陷入死循环的计算机系统重新从头开始工作。 消除或抑制干扰影响的方法伺服运动控制系统概述伺服系统是可以按照一定的指令控制电动机执行规定的运行动作，一般由伺服电动机、传感器和控制器三部分构成闭环，完成精密控制。 伺服系统的基本要求：稳定性好、精度高，一般在0.001-0.01mm之间、快速响应性好，一般在200ms以内，甚至小于几十毫秒。 伺服系统对控制系统要求可分为如下几种形式： 1、点位运动控制。仅对终点位置有要求，与运动的中间过程（运动轨迹无关）。 2、连续轨迹（轮廓控制）运动控制。主要应用在数控系统、切割系统的运动轮廓控制中。 3、同步运动控制。多个轴之间的运动协调控制，可以是多个轴在运动全程中进行同步，也可以是在运动过程中的局部速度同步。 伺服系统主要特点：精确的检测装置、多种反馈比较原理和方法、高性能伺服电机、宽调速范围的速度调节系统、靠偏差工作。 伺服系统一般由电流环、速度环和位置环组成，其中电流环采用矢量控制，速度环和位置环采用PID控制。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[信号处理涉及的几种变换]]></title>
    <url>%2Fblog%2F2020%2F05%2F14%2F%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E6%B6%89%E5%8F%8A%E5%88%B0%E7%9A%84%E5%87%A0%E7%A7%8D%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[小波可以进行伸缩平移，源自傅里叶变换，都是频率变换的方法。 傅里叶变换、短时傅里叶变换、小波变换 内积，基，归一化正交，投影，Hilbert空间，多分辨率，父小波，母小波 ==1、基== 傅里叶变换的基是不同频率的正弦曲线，把信号波分解成不同频率的正弦波的叠加和。 小波变换把信号分解成一系列的小波，小波种类多，同一种小波可以进行尺度变换，小波的特性是在整个时间范围内幅度的平均值是0，具有有限的持续时间和突变的频率和振幅，可以是不规则，不对称的。正弦波不是小波。 基的特性是： 非冗余性 完备性：即使基不是正交的，有相关性，若去掉其中一个，不成为基， 唯一性：给定一族基对一个函数的表达是唯一的 非正交性：一般情况下是非正交的，称为exact frame（resize basis），此时要表示信号可以将基正交化成唯一的正交基（对偶为其自身）；也可以求其对偶框架（dual frame），其对应了小波变换的双正交情形，信号可以依框架进行分解，然后用对偶框架重构。 框架： 在基集中添加一些新的向量，并随意调整空间位置，可能成为框架。把函数与基或框架作内积（函数空间到系数空间的变换），若变换后的能量（内积的平方和度量）仍然有一个大于0的上下界，才可以成为框架。 框架具有冗余性，系数的表达不具有唯一性。 若上下界相等，则为紧框架，且界表示冗余度； 若上下界相等且为1，成为pasval identity frame，此时不一定为正交基（相当于把一组正交基中的某一个拆成两个同方向的基之和，pasval identity 仍然成立），此时若再加上基的长度均为1的条件，则框架退化为正交基。 很多信号表示方法不能构成基，但能构成框架，如短时傅里叶变换中如要求窗函数满足基条件，则可推出函数具有很差的时频局部化性质（退化为傅里叶变换）。 ==2、内积== 在Hilbert空间中，用来刻画两个向量的夹角，当内积为0时，两个向量正交。 若$g$为Hilbert空间里的正交基，内积为$f$向基上的正交投影，有如下推导：$$f·g = f^Tg= = \int f(t) \bar{g(t)}dt = f_1g_1 +f_2g_2 = |f||g|cos \theta$$如果两个向量内积为0，即正交；如果一个向量序列相互对偶正交，并且长度为1，是正交归一化。 对于$f(t) \in L^2(R)$（平方可积），存在$L^2(R)$上的一组标准正交基$g_i(t),i=1,2,\dots,$，使得：$$f(t) = \sum\limits_{i=1}^{\infty}f_ig_i(t)$$$L^2(R)$上任意一个函数$f(t)$都可以由$L^2(R)$的一个规范正交基$g_i(t)$进行线性组合表示出来。 ==3、傅里叶变换== Fourier分析不能刻画时间域上信号的局部特性 Fourier分析对突变和非平稳信号的效果不好，没有时频分析 傅里叶变换将函数投影到三角波上，将函数分解为不同频率的三角波，对于平稳信号的表示，近似最优表示。但是日常信号不是一直光滑的，而且奇异是非凡的，傅里叶在奇异点的表现不是很好，用大量的不同频率的三角波去逼近其系数衰减程度相当缓慢，而且会产生Gibbs效应。（内在原因是其基为全局性基，没有局部化能力，以致于局部一个小小的摆动会影响全局的系数，实际应用中很需要时频局部化，傅里叶缺乏此能力）。 对于非平稳信号，只知道包含哪些频率成分不够，还需要知道各个成分出现的时间。知道信号频率随时间变化的情况，各个时刻的瞬时频率及其值——时频分析。 瞬时频率：绝对意义上的瞬时频率是不存在的，单看一个时刻点的一个信号值，得不到它的频率，可以用很短的一段信号的频率作为该时刻的频率，得到的是时间分辨率有限的近似分析结果（SIFT）。小波等时频分析方法，用如衰减的基函数去测定信号的瞬时频率，思想类似。 ==4、短时傅里叶变换(SIFT)== SIFT针对于傅里叶变换的时域太长而改进，也称为加窗傅里叶变换，这样有了局部化能力。 SIFT定义：把整个时域过程分解为无数个等长的小过程，每个小过程近似平稳，再傅里叶变换，这样可以知道在哪个时间点上出现了什么频率了。即在时域上分成一段一段做FFT。 窗的大小的影响：窗太窄，窗内信号太短，会导致频率分析不够精准，频率分辨率差；窗太宽，时域上不够精细，时间分辨率低。 用海森堡不确定性原理解释：类似不能同时获取一个粒子的动量和位置，依次，也不能同时获取信号绝对精准的时刻和频率，是一对不可兼得的矛盾体。不知道在某个瞬间哪个频率分量存在，只知道在一个时间段内某个频带的分量存在，所以绝对意义的瞬时频率是不存在的。 很多物理量有这样的特征，比如能量和时间、角动量和角度。信号领域就是时域和频域，更为准确的描述是：一个信号不能在时空域和频域上同时过于集中，一个函数时域越“窄”，经傅里叶变换后的频域就越“宽”。 对于时变的非稳态信号，高频适合小窗口，低频适合大窗口，SIFT的窗是固定的，在一次的SIFT中宽度不会变化，所以SIFT无法满足非稳态信号变化的频率的需求。 ==5、小波变换== 小波是基于加窗傅里叶变换的窗口大小改变的思想，但是，SIFT是给信号加窗，分段做FFT；小波变换并没有采用的窗的思想，也没有做傅里叶变换，小波直接将傅里叶变换的基更换——将无限长的三角函数基换成了有限长的会衰减的小波基，这样获得频率，也可以定位到时间。 基函数：会伸缩、会平移（是两个正交基的分解）。缩得窄，对应高频；伸得宽，对应低频。基函数不断和信号做相乘。某一个尺度（宽窄）下乘得结果，可以看成信号所包含得当前尺度对应频率成分有多少。 基函数会在某些尺度下，与信号相乘得到一个很大的值，这样就知道信号包含该频率的成分的多少。小波的改变将无限长的三角函数基换成了有限长的会衰减的小波基。 小波公式：$$WT(a,\tau)= \frac{1}{\sqrt{a}}\int_{-\infty}^{\infty}f(t)*\psi(\frac{t-\tau}{a})dt$$ 变量只有频率$w$，小波变换只有两个变量：尺度$a$（scale）和平移量$\tau$（translation）。尺度$a$控制小波函数的伸缩，平移量$\tau$控制小波函数的平移。尺度对应于频率（反比），平移量$\tau$对应于时间。 当伸缩、平移到上面这一种重合情况时，也会相乘得到一个大的值，与傅里叶不同的是，这不仅知道信号有这样频率的成分，而且知道在时域上存在的具体的位置。【解决局部性问题】当在每个尺度下都平移着和信号都乘过一遍后，能够知道在每个位置都包含哪些频率成分，这样可以做时频分析。【解决时频分析问题】 具体的还有子空间、多分辨率、母小波的变换，如何构造想要的小波函数，离散小波变换、正交小波变换、二维小波变换、小波包的应用。 每一个小波有一个mother wavelet，同时还有一个father wavelet，即scaling function。而小波的basis函数是对这个母小波和父小波的缩放和平移形成的，缩放倍数都是2的级数，平移大小与当前缩放的程度有关。 不同的母小波，衍生的小波基完全不同，小波展开的近似形式为：$$f(t) = \sum\limits_k \sum\limits_{j}a_{j,k}\psi_{j,k}(t)$$即小波级数，这些级数组合形成了小波变换中的基basis。与傅里叶级数不同的是，小波级数通常是orthonormal basis（规范化正交基），即不仅两两正交，还归一化的。 小波变换的三个特点：小波级数是二维的，能定位时域和频域，计算很快。 小波的完整形式： 小波basis的形成，是基于基本的小波函数，即母小波来做缩放和平移的，母小波并非唯一的原始基，在构建小波基函数的集合时，通常用到一个尺度函数scaling function 。即成为父小波，两者都是归一化的，并且满足一个性质，它和对自己本身周期平移函数两两正交：$$f(t) = \sum\limits_k \sum\limits_{j}a_{j,k}\psi_{j,k}(t)$$ $$&lt;\varphi(t),\varphi(t-kT)&gt; = 0 ,\forall k$$ 完整的小波展开由父小波和母小波共同定义的：$$f(t) = \sum\limits_{k=-\infty}^{\infty}c_k\varphi(t-k)+\sum\limits_{k=-\infty}^{\infty}\sum\limits_{j=0}^{\infty}d_{j,k}\psi(2^jt-k)$$其中$\psi(t)$是母小波，$\varphi(t)$是父小波。 小波的一些优势： 对于突变信号，傅里叶变换存在吉布斯效应，用无限长的三角函数拟合不好突变信号，使用衰减的小波，只有小波函数和信号突变处重叠时，系数不为0. 小波可以实现正交化，短时傅里叶变换不能。采用正交基，变换域的系数会没有冗余信息，等于是用最少的数据表达最大的信息量，有利于数值压缩等领域。但是比如在图像增强领域，有时候反而希望能有一些冗余信息，更有利于对噪声的抑制和对某些特征的增强。 小波变换的缺点： 作为图像处理方法，【和多尺度几何分析方法（超小波）比较】：对于图像这种二维信号，二维小波只能沿着2个方向进行，对图像中点的信息表达还可以，但是对线的信息表达比较差。此时ridgelet(脊波)、curvelet(曲波)等多尺度几何分析方法更有优势。 作为时频分析方法，【和HHT比】：小波变换没能脱离海森堡测不准原理的束缚，某种尺度下，不能在时间和频率上同时具有很高的精度，此外小波是非适应性的，基函数选定后不再修改。 ==5、S变换== S-transform可以看成是小波变换和SIFT的继承和法阵，傅里叶变换只能作用于收敛信号，SIFT的窗函数不可变，小波变换虽然窗函数可变，能进行多分辨率分析，但是其基函数选取困难。ST介于两者之间，可以自适应调节分辨率且其逆变换无损可逆。 ST是由地球物理学家Stockwell在1996年提出的一种时频分析方法，定义为：$$S(\tau,f)= \int_{-\infty}^{\infty}h(t)\frac{|f|}{\sqrt{2\pi}}e^{-\frac{(\tau-t)^2f^2}{2}}e^{-i2\pi ft}dt$$其中：$\tau$为时间，控制窗口函数在时间轴上的位置；$h(t)$为分析信号；$f$为频率；$S(\tau,f)$为变换得到的时频谱矩阵。 ST可以写成傅里叶频谱$H(f)$的形式：$$S(\tau,f) = \int_{-\infty}^{\infty}H(\alpha+f)e^{-\frac{2\pi^2 \alpha^2}{f^2}}e^{i2\pi \alpha \tau}d\alpha,(f \ne 0)$$ 对于离散信号，有：$$H\left[\frac{n}{NT}\right] = \frac{1}{N}\sum\limits_{K=0}^{N-1}h[KT]e^{-\frac{i2\pi nk}{N}}$$其中：$K$为离散的时间点，$k=0,1,\dots,N-1;N$为离散时间信号长度；$T$为采样时间间隔。 对于离散信号，令$f = \frac{n}{NT}$,$\tau = jT$，最终的离散信号的S变换可以表示为：$$S\left(jT,\frac{n}{NT}\right)= \sum\limits_{m=0}^{N-1}H\left[\frac{m+n}{NT}\right]e^{-\frac{2\pi^2m^2}{n^2}}e^{\frac{i2\pi mj}{N}},(n\ne 0)$$ST克服了SIFT窗口的时宽不变的缺陷，能根据频率的变化和自适应调整分析时宽和提供直观的时间频率特征，且无须选择窗口函数域分析尺度。 广义S变换公式推导： 对于傅里叶变换，其正变换为：$$H(f) = \int_{-\infty}^{\infty}h(t)\exp(-i2\pi ft)dt$$其中：$h(t)$为待分析的时间信号序列；$f$为频率；$t$表示时间；$H(f)$时是信号$h(t)$的傅里叶变换。 加入对时间序列$h(t)$加上一个窗函数$g(t)$，则其谱变为：$$H(f) = \int_{-\infty}^{\infty}h(t)g(t)\exp(-i2\pi ft)dt$$对于ST，首先定义一个高斯窗函数：$$g(t) = \frac{1}{\sigma \sqrt{2\pi}}\exp\left(\frac{t^2}{2\sigma^2}\right)$$其中：$\sigma$为高斯函数的方差，对上述高斯窗函数进行伸缩与平移，得到$S$变换的公式为：$$S(\tau,f,\sigma) = \int_{-\infty}^{\infty} h(t) \frac{1}{\sigma \sqrt{2\pi}}\exp \left[-\frac{(t-\tau)^2}{2\sigma^2}\right]·\exp(-i2\pi ft)dt$$其中：$\tau$表示高斯窗函数在时间上的平移量。 实际上：S变换就是把小波基函数用高斯窗来代替，也被定义为“相位正交”的连续小波变换。 信号$x(t)$的ST定义如下：$$\cases{S(\tau,f) = \int_{-\infty}^{\infty}x(t)w(t-\tau,f)e^{-i2\pi ft}dt \ w(t-\tau,f) = \frac{|f|}{\sqrt{2\pi}}e^{\frac{-f^2(t-\tau)^2}{2}}}$$其中：$w(t-\tau,f)$为高斯窗函数；$\tau$为时移因子，控制高斯窗在时间轴的位置；$f$为频率。 由上式可知： 高斯窗口克服了短时傅里叶变换窗口高度和宽度固定的缺陷，其随频率而变化。因此，ST既可以获得某一时刻的频率信息，也可获得在某一频率上信号的幅值信息。 S变换的应用： 将ST应用在PQD检测和分类的研究成果最多。PQD（Power Quality Disturbances）电能质量扰动会导致设备过热、电机停转、保护失灵以及计量不准等严重后果。 基于S变换的PQD检测：PQD 检测包括扰动起止时刻的定位、 扰动持续时间和扰动幅值的测量等方面的内容。文献首次提出采用 S 变换模矩阵的等值线检测扰动的发生，并发现时频曲线的标准差与扰动幅值存在一一对应关系，可以用来测量扰动幅值，但该文对 S 变换应用于 PQD 检测仅是探索性的，并未形成一套完整可行的方法。 目前 S 变换应用于 PQD 检测主要集中于扰动频率与幅值、扰动起止时刻与持续时间这些常规参数的检测，而且只是考虑了稳态或存在稳定段的扰动幅值检测，对不存在稳定段的暂态扰动如振荡暂态、脉冲暂态等幅值的检测并没有提出有效的方法，同时对于 S 变换应用于振荡暂态衰减因子、电压切口深度及宽度等参数的检测尚未见文献报道。 基于S变换的PQD分类：PQD的识别实际是一个模式分类问题，关键思想是针对PQD特征选定一个较优的分类器进行模式识别。目前基于S变换的PQD识别方法一般是从待分析的PQD信号的S变换模矩阵中提取特征，然后借助人工智能分类器实现分类。 基于S变换和ANN的PQD分类。 基于S变换和SVM的PQD分类。 基于S变换和规则判别的PQD分类。 S变换在PQD应用方面有待进一步研究的方面有： S变换的结果是二维复矩阵，能够提供丰富的时频信息，其相位和原始信号保持直接的联系，目前对相位信息的利用很少，对于PQD的某些应用如扰动源的定位利用相位信息比幅值信息应该更有效。 由于 S 变换的运算量很大，在实际应用时对计算环境会有很高的要求，因此，对 S 变换算法或计算过程进行改进，在保证 PQD 分析精度的基础上提高其运算实时性，是其现实广泛应用的前提。 对 PQD 最精确和全面的检测应该是将描述其数学模型的所有参量进行准确的估计，而目前S 变换应用于 PQD 检测主要集中于稳态扰动的参数检测，对暂态扰动如振荡暂态的最大幅值和衰减因子等参量的检测并没有提出有效的方法。 S 变换作为 PQD 信号的特征提取工具，如何针对具体应用情况从 S 变换提取最有效的特征量尚缺乏全面和系统的研究。 除检测和分类之外， S 变换在 PQD 其他方面的应用如信号去噪、扰动源定位尚不多见，同时，开发新形式的 S 变换为PQD 分析提供更适宜的时频分辨率，研究确定最优时频分辨率的理论依据也值得进一步研究。 S 变换在 PQD 分析的应用尚处于算法仿真阶段，其在具体硬件环境中的实现及实际应用效果的评价鲜见报道。 参考文献： 易吉良； 彭建春； 谭会生．S变换在电能质量扰动分析中的应用综述：电力系统保护与控制，2011]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[概率图模型]]></title>
    <url>%2Fblog%2F2020%2F05%2F12%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[概率图模型概率图模型(graphical model)是概率论和图论之间的桥梁，概率图模型的基本想法来自模块的概念，即一个复杂的系统是由简单的部分所联合而成，概率部分告诉我们哪些部分是耦合在一起的，然后提供推断模型的方法，图论的作用是给出一个直观的认识，把拥有相互关系的变量看成是数据结构，从而导出一般化的方法来解决问题。 概率图模型刻画模型的随机变量在变量层面的依赖关系，反映问题的概率结构以及推理的难易程度，为推理算法提供可操作的数据结构。概率图模型的表示方法常用的有贝叶斯网络、马尔科夫网络、因子图等。 在概率图模型中，点代表随机变量，点与点之间边的存在与否代表点与点之间存在条件依赖，点与边的组合描绘了联合概率分布的特征结构。假设有$N$个二元随机变量，在没有任何信息的帮助下，联合分布$P(X_1,\dots,X_N)$，需要$O(2^N)$个参数。而通过概率图描绘点与点之间的条件关系之后，表示联合分布，所需要的参数会减少很多，利于之后模型的推断和学习。 基本的概率图模型可以分为两类：贝叶斯网络(bayesian network)和马尔科夫随机场(markov random field)。 贝叶斯网络采用有向无环图(directed acyclic graph)来表示因果关系，马尔科夫随机场采用无向图(undirected graph)来表达变量间的相互作用。无向图多用于物理和图像领域，有向图多用于机器学习。 贝叶斯网络的每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有的单个节点所对应的分布的乘积；马尔科夫场，变量之间没有明确的因果关系，它的联合概率分布通常会表示为一系列的势函数(potential function)的乘积。通常情况下，还需要对这些乘积进行归一化，形成一个有效的概率分布——这给参数估计造成很大的困难。 在实际使用的模型，很多时候是它们的某种形式的结合，如一个马尔科夫随机场可以整体成为一个更大的贝叶斯网络的节点，或者，多个贝叶斯网络可以通过马尔科夫随机场联系，构建混合模型。 PGM综述-by MIT 林达华博士 CMU概率图模型课程 概率图模型资源分享 PGM概述PGM应用场景 一个是用于医学诊断。医生根据掌握的病人的相当数量的信息、各种测试结果、症状等，推断出病人的病情，不同的治疗方案会有怎样的结果等。 另一个是图像分割。给图像中的每个像素贴上类别标签。 这两个问题的共同点是： 1、都具有大量的我们需要从中推理的变量，在图像分割中，不同的像素或者像素构成的小区域的标签称为superpixels。 2、不管算法设计如何清晰，但是正确的结果仍然具有不确定性。 Model是什么 模型是我们理解世界的形象化表示(Declarative representation)。是理解周围世界的声明或者表达方式，在计算机内，一个模型包括我们对若干变量的理解，如变量的含义、变量之间的交互。 模型的特性使得我们能够将新的算法加入模型内部，同时加入新的外界知识，比如专家知识是知道模型，通过学习的方法改善模型。 Probability是什么 首先解释不确定性(uncertainty)，产生不确定性的原因主要有： 1、对世界认知状态的不完整 2、含有噪声的观测(Noisy observation) 3、模型未能覆盖所有实际现象 4、固有的随机性 概率论，通常具有清晰的表达式，强推理模式，可建立的学习方法 Graphical是什么 是一种复杂的数据结构，通常包括顶点和连接顶点的边。 Graphical Models图模型 基本的概率图模型可以分为两类：贝叶斯网络(bayesian network)【有向无环图】和马尔科夫随机场(markov random field)【无向图】 顶点表示随机变量，边表示随机变量之间的概率依赖关系。 分布(Distributions) 联合分布概率密度函数 条件概率分布 PGM模型分类简介概率模型Probabilistic models常用来描述不同的随机变量之间的关系，主要针对变量或变量间的相互不确定性的概率关系建模。 1、参数模型：由一堆有限维参数构成的分布集合$\mathcal{P} = \{\mathbb{P} _{\theta} : \theta \in \Theta\}$。$\theta$是参数，$\Theta \subseteq \mathbb{R}^d$是其可行欧几里得子空间。 2、非参数模型：由一组无限维参数构成的概率分布函数集合，可表示为$\mathcal{P} = \{所有的分布\}$。 PGM是概率模型的一种，通过利用有向图或者无向图来表示变量之间的概率关系，可以将复杂的概率模型转换为纯粹的代数运算。常见的PGM的分类有： PGM中的factorfactors影响因子，在PGM模型中，一个factor可以看成是graph中的一个节点，两个factor之间的联系用图的边来表示。 一个PGM模型最终可以建模为所有factor的联合概率分布，然后根据factor之间的运算规则，可以将联合概率分布转化为可运算的概率分布的乘积。 贝叶斯网络是一种典型的图模型，它对感兴趣变量(variables of interest)及变量之间的关系(relationship)进行建模。贝叶斯模型和统计技术一起使用，具有如下优势： (1) 贝叶斯学习能够方便的处理不完全数据。例如考虑具有相关关系的多个输入变量的分类或回归问题，对标准的监督学习算法而言，变量间的相关性并不是它们处理的关键因素，当这些变量中有某个缺值时，它们的预测结果就会出现很大的偏差。而贝叶斯学习则提供了较为直观的概率关联关系模型。 (2) 贝叶斯学习能够学习变量间的因果关系。因果关系是数据挖掘中极为重要的模式。原因有二：在数据分析中，因果关系有利于对领域知识的理解；在干扰较多时，便于作出精确的预测。 (3) 贝叶斯网络与贝叶斯统计相结合能够充分利用领域知识和样本数据的信息。任何从事过实际建模任务的人都会知道先验信息或领域知识在建模方面的重要性，尤其是在样本数据稀疏或数据较难获得的时候，一些商业方面的专家系统完全根据领域专家知识来构建就是一个很好的例证。贝叶斯网络用弧表示变量间的依赖关系，用概率分布表来表示依赖关系的强弱，将先验信息与样本知识有机结合起来。贝叶斯学习理论在数据挖掘中获得了成功的应用。对贝叶斯学习理论研究最大的动力就是它在实际应用中的巨大作用和潜力。目前，贝叶斯学习理论已成功地应用到智能用户接口、信息滤波、车辆自动导航、武器制导、医疗诊断、经济预测和文本分类等诸多领域。 (4)贝叶斯统计方法可以和贝叶斯网络一起使用，避免了数据过度拟合（the overfiting of data）。 基本概念 是有向无环图模型(directed acyclic graphical model)，是一种概率图型模型。通过有向无环图得知一组随机变量$\{X_1,\dots,X_n\}$及其$n$组条件概率分配(conditional probability distributions，CPDs)性质。 例如，贝叶斯网络可以表示疾病和其相关症状间的概率关系，假若已知某种症状下，贝叶斯网络就可以用来计算各种可能罹患疾病的发生概率。 一般而言，贝叶斯网络的有向无环图的节点表示随机变量，它们可以是可观察的变量或隐变量，未知参数等连接两个节点的箭头表示之间具有因果关系或是非条件独立。 节点中变量之间若没有箭头相互连接，称变量彼此之间为条件独立；若以单箭头连接在一起，一个是因(parents)，另一个是果(descendants or children)。 令$G =(I,E)$表示一个有向无环图(DAG)，其中$I$代表所有节点的集合，$E$代表有向连接线段的集合，令$X = X_i, i \in I$，为其有向无环图中的某一节点$i$代表的随机变量，若节点$X$的联合概率分布可以表示为：$$p(x) = \prod\limits_{i \in I} p(x_i |x_{pa(i)})$$称$X$为相对于一有向无环图$G$的贝叶斯网络，其中$x_{pa(i)}$表示节点$i$的 因 对任意的随机变量，其联合分布可由各自的局部条件概率分配相乘得出：$$P(X_1 = x_1,\dots, X_n = x_n) = \prod_{i=1}^n P(X_i = x_i | X_{i+1} = x_{i+1},\dots,X_n = x_n)$$对于贝叶斯网络的联合概率而言：$$P(X_1 = x_1,\dots, X_n = x_n) = \prod_{i=1}^n P(X_i = x_i | X_{j} = x_{j},对比每个相对于X_i的 因变量X_j而言)$$主要区别在于在贝叶斯网络中，若已知因变量下，某些节点会与其因变量条件独立，只有与因变量有关的节点才会有条件概率的存在。 独立与分割 独立：令$X,Y,Z$代表概率事件，如果$X,Y$满足$P(X,Y) = P(X) \times P(Y)$，称相互独立(independent)；如果$P(X,Y,Z)$正比于$\phi_1(X,Z)\times \phi_2(Y,Z)$，称$X,Y$在条件$Z$下独立。 有向分割(D-Separated)：对于变量集合$Z$中任意一个变量，如果$X,Y$之间没有活动路径，称$X,Y$为有向分割。 推理(inference) 贝叶斯网络可以利用变量之间的条件独立对联合分布进行分解，降低参数个数，推理是通过计算回答查询的过程。 （1）变量消元算法(Variable elimination) 利用概率分解降低推理的复杂度。 使用运算局部化。消元过程实质上就是一个边缘化的过程。 最优消元顺序：最大势搜索，最小缺边搜索。 （2）团树传播算法 利用步骤共享来加快推理的算法。团树（clique tree）是一种无向树，其中每一个节点代表一个变量集合，称为团（clique）。团树必须满足变量的连通性，即包含同一变量的所有团所导出的子图必须是连通的。 基本步骤： 将贝叶斯网络转化为团树 团树初始化 在团树中选一个团作为枢纽 全局概率传播：collect_message;distribute_message 边缘化，归一化 构建贝叶斯网络 包括（1）变量的定义；（2）结构学习；（3）参数学习 包含两个任务： 一是在领域专家的指导下选取合适的研究问题领域内的变量，同时在有些情况下也需要一定的策略，从专家提供的变量中选择重要因子。 二是构建出一个有向无环图并给出图中每个结点的分布参数，即每个节点都对应一个条件概率分布表(CPT)。 一般情况下，有两种不同方式构造贝叶斯网络： （1）完整学习：完全由人主观定义网络的结构和参数，有领域专家确定贝叶斯网络的变量，通过专家知识确定贝叶斯网络的结构，并指定分布参数。这样构建的网络，会因为人的知识的有限性，导致出现很大的偏差。 （2）部分学习：由人主观定义网络的节点变量，然后通过大量的训练数据来学习网络的结构和参数。这完全是一种数据驱动的方法，具有很强的适应性。 朴素贝叶斯分类器朴素贝叶斯分类器(Naive Bayes Classifier)特别适用于输入数据维数较高的情况。 先验知识：知道一个球要么是红色要么是绿色。假定红球20，绿球40个 任务：当有新的输入(new cases)时，给出的新输入的物体的类别（红或绿），即贝叶斯分类器的标签-label。 图中，绿球的数量明显大于红球，有理由认为，新输入更有可能是绿球。$$\rm{ prior probability for GREEN = } \frac{ \rm{ number of GREEN objects}}{\rm{Total number of objects}} = \frac{40}{60}$$ $$\rm{ prior probability for RED = } \frac{ \rm{ number of RED objects}}{\rm{Total number of objects}} = \frac{20}{60}$$ 似然(likehood)：有了先验知识，可以对新的输入(白色圈表示)进行分类，如果要取得比较准确的分类结果，猜测是绿色比较保险，即新物体与绿球的likehood比与红球的likehood更大。$$\rm{ Likehood of X given GREEN = } \frac{ \rm{ number of GREEN in the vicinity of X}}{\rm{Total number of GREEN cases}} =\frac{1}{40}$$ $$\rm{ Likehood of X given RED = } \frac{ \rm{ number of RED in the vicinity of X}}{\rm{Total number of RED cases}} = \frac{3}{20}$$ 存在情况是：对于先验知识，$X$是绿球的可能性比红球更大，但是似然(likehood)的表现是相反的 在贝叶斯分析中，最后的类别是上述两个概率（先验和似然），即：$$\rm{ posterior probability of X being GREEN =} \frac{ \rm{ prior probability for GREEN} \times \rm{Likehood of X given GREEN}}{在圈内球的概率 = \frac{4}{60}} = \frac{1}{4}$$ $$\rm{ posterior probability of X being RED =} \frac{ \rm{ prior probability for RED} \times \rm{Likehood of X given RED}}{在圈内球的概率 = \frac{4}{60}} = \frac{3}{4}$$ 实际使用过程中，需要进行归一化(normalized)处理。 Factor之间的推理Factor之间的推理（Reasoning Patterns）可以分类为： 1、causal reasoning：因果推理 2、evidential reasoning：证据推理 也称为D-S推理。将假设看成一个集合，引入信任函数、似信度函数、类概率函数等描述命题的精确信任程度、信任程度和估计信任程度，对命题的不确定性作多角度的描述。 对从不同性质的数据源中提取证据，利用正交求和方法综合证据，通过证据的积累缩小集合，从而获得问题的解。 3、intercausal reasoning：混合推理 结合因果推理和证据推理。 PGM课程笔记-coursera]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[makefile简介]]></title>
    <url>%2Fblog%2F2020%2F05%2F11%2Fmakefile%2F</url>
    <content type="text"><![CDATA[概述一个工程的源文件很多，按类型、功能、模块分别放在若干个目录中，makefile定义了一系列规则来指定，包括文件的编译的先后顺序，甚至进行更复杂的功能操作。makefile类似一个shell脚本，其中可执行操作系统的命令。 makefile写好，需要一个make命令，整个工程完全自动编译，提高软件开发的效率。make是一个解释makefile中指令的命令工具，一般大多数的IDE都有这个命令，比如：Delphi的make，Visual C++的nmake，Linux下GNU的make。【GNU是关于Linux内核的标准，一切有关Linux的开发必须符合GNU的规定】 关于程序的编译和链接程序的编译，无论c/c++，还是pascal，首先把源文件编译成中间代码文件，win下是.obj文件，UNIX下是.o文件，即object File，编译（compile）操作。然后把大量的Object File合成可执行文件，即作链接（link)。 编译时，编译器需要的语法是正确的，函数和变量的声明的正确。函数和变量的声明是需要告诉编译器的头文件所在位置，【头文件只是声明，定义在c/c++文件中】，只要所有语法正确，编译器就可以编译出中间目标文件，一般来说，每个源文件都应该对应于一个中间目标文件（o文件或是obj文件）。 链接时，主要是链接函数和全局变量，可以使用这些中间目标文件（o文件或是obj文件）来链接应用程序。链接器不管函数所在的源文件，只负责中间文件（o文件或是obj文件）。大多数时候，由于源文件太多，编译生成的中间目标文件太多，链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，要给这些目标文件打个包，在win下这种包叫做“库文件”（Library File），即.lib文件，在UNIX下，是“Archive File”，即.a文件。 总结一下： 源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。若未声明，编译器会给出警告，但可以生成Object File。 链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，就会报Linker Error。在VC下，这种错误一般是Link 2001错误。 Makefile介绍make命令执行时，需要一个Makefile文件，告诉make命令需要如何去编译和链接程序。 给定一个示例，工程有8个c文件，和3个头文件，需要写一个makefile来告诉make命令如何编译和链接这几个文件，规则是： 如果这个工程没有被编译过，所有的c文件都要被编译并被链接 若某几个文件被修改，只编译被修改的c文件，并链接目标程序 若工程的头文件被改变了，需要编译引用这几个头文件的c文件，并链接目标程序 写好Makefile文件，make命令会自动智能地根据当前文件的修改情况来确定哪些文件需要重新编译，从而自己编译所需要的文件和链接目标程序。 Makefile的规则1234target ... : prerequisites ... (冒号两边有空格) command (用tab键开头) ... ... target可以是一个或多个目标文件，可以是Object File，也可以是执行文件，甚至是一个标签（Label）。 prerequisites是要生成那个target所需要的文件或目标（即所需条件） 命令是生成目标所需要执行的脚本。（任意的shell命令） 这是一个文本依赖关系，target依赖于prerequisites中的文件，其生成规则定义在command中。 即：prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。 123456789101112131415161718192021222324edit : main.o kbd.o command.o display.o / insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o / insert.o search.o files.o utils.omain.o : main.c defs.h (目标文件 ： 依赖文件) cc -c main.c (定义如何生成目标文件的操作系统命令，tab键开头)kbd.o : kbd.c defs.h command.h cc -c kbd.ccommand.o : command.c defs.h command.h cc -c command.cdisplay.o : display.c defs.h buffer.h cc -c display.cinsert.o : insert.c defs.h buffer.h cc -c insert.csearch.o : search.c defs.h buffer.h cc -c search.cfiles.o : files.c defs.h buffer.h command.h cc -c files.cutils.o : utils.c defs.h cc -c utils.cclean : rm edit main.o kbd.o command.o display.o / insert.o search.o files.o utils.o 反斜杠表示换行符，Makefile易读，内容保存在名为Makefile或makefile的文件中，然后该目录下直接输入命令make生成可执行文件edit。如果要删除执行文件和所有的中间目标文件，执行make clean操作。 其中，目标文件（target）包含：执行文件edit和中间目标文件.o文件。依赖文件（prerequisites）即冒号后面的.c和.h文件。 每一个.o文件都有一组依赖文件，而本身.o文件又是执行文件edit的依赖文件。依赖关系的实质就是说明目标文件是由哪些文件生成的，即目标文件是哪些文件更新的。 定义好依赖关系，后续的那一行定义如何生成目标文件的操作系统命令，以一个Tab键作为开头。make只管执行所定义的命令，会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期比targets文件日期要新，或者target不存在，make会执行后续定义的命令。 clean是一个操作名称，不是一个文件，若其冒号后面没有，make就不会自动去找文件的依赖性，不会执行其后所定义的命令。若要执行其后的命令，需要在make命令后明显得指出命令标签的名字，比如用make clean指令。这样可以在一个makefile中定义不用的编译或是和编译无关的命令，比如程序的打包，程序备份等。 Makefile工作方式默认方式，只输入make命令。 make在当前目录下找到makefile或makefile的文件 找到后，会找文件中的第一个目标文件（target），上例中找到edit文件，并把这个文件作为最终的目标文件 若edit文件不存在，或是edit所依赖的后面.o文件的文件修改时间要比edit这个文件新，那么，会执行后面的定义的命令来生成edit这个文件 若edit所依赖的.o文件也存在，make会在当前文件中找目标.o文件的依赖性，如果找到则再根据那一个规则生成.o文件（类似一个堆栈的过程） C文件和H文件存在，make会生成.o文件，然后再用.o文件声明，make所处理的最终任务，就是执行edit文件 make会一层一层的去找文件的依赖关系，直到最终编译出第一个目标文件，make只管文件的依赖性，不负责所定义的命令的错误或是编译不成功。找到依赖关系后，冒号后面的文件还是不存在，make指令不工作。 上例中，若修改了其中一个源文件，如file.c，根据依赖性，目标file.o会被重编译，变为最新的，file.o文件修改时间要比edit要新，所以edit会被重新链接。 如果修改了command.h，那么kdb.o、command.o和files.o都会被重编译，并且，edit会被重链接。 Makefile变量使用12345edit : main.o kbd.o command.o display.o / insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o / insert.o search.o files.o utils.o .o文件的字符串被重复了两次，如果要加入一个新的.o文件，需要在三个地方添加（包括clean）。当makefile复杂时，可能会漏，导致编译失败，便于makefile维护，在makefile中使用变量，makefile的变量是一个字符串（相当于c语言的宏）。 例如： 声明一个变量： 12objects = main.o kbd.o command.o display.o\ insert.o search.o files.o utils.o 在makefile中以$(object)的方式来使用这个变量。 参考文章——makefile陈皓]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu使用教程]]></title>
    <url>%2Fblog%2F2020%2F05%2F10%2Fubuntu%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Ubuntu磁盘操作需要root权限 12sudo susu *** 用命令fdisk -l查看系统识别磁盘情况 12345678910111213Device Boot Start End Sectors Size Id Type/dev/sda1 * 2048 960335871 960333824 457.9G 83 Linux/dev/sda2 960337918 976771071 16433154 7.9G 5 ExtendedDisk /dev/sdb: 2.7 TiB, 3000592982016 bytes, 5860533168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: gptDisk identifier: 1AB82561-C5FD-47A9-9A29-31835617EBD3Device Start End Sectors Size Type/dev/sdb1 2048 5860532223 5860530176 2.7T Microsoft basic data 可以看到磁盘的挂载点。 ubuntu配置nVidia驱动一、禁用现有的开源驱动 Ubuntu默认安装了一个开源的驱动：nouveau。安装官方的驱动需要先禁用这个开源驱动，依次执行： sudo bash -c &quot;echo blacklist nouveau &gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf&quot; sudo bash -c &quot;echo options nouveau modeset=0 &gt;&gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf&quot; 执行完两条指令，使用命令：cat /etc/modprobe.d/blacklist-nvidia-nouveau.conf是否成功禁用了开源驱动，出现如下，表示成功： 123$ cat /etc/modprobe.d/blacklist-nvidia-nouveau.confblacklist nouveauoptions nouveau modeset=0 二、获取显卡型号 终端输入：ubuntu-drivers devices查看显卡硬件型号，会显示推荐安装驱动版本。 三、获取对应显卡驱动 去NVIDIA driver search page查看支持对应显卡的驱动的最新版本的版本号。 为防止最新版本不稳定，可以从geforce drivers处查询支持当前显卡的所有驱动版本。 四、安装 终端命令：sudo apt-get install nvidia-384（当前安装的是384版本）。 如果命令出错，执行下面命令： sudo apt-get upgrade sudo apt-get update 五、测试nvidia driver是否安装成功 首先需要重新启动系统，然后在终端输入：nvidia-smi查看驱动是否安装成功。 Ubuntu16.04配置cuda9.0+cudnn7.0安装GPU加速必须在cuda9以上才可以完成加速，cuda9以后的版本在Linux系统上只支持Ubuntu16.04和Ubuntu17.04。 一、设备要求系统：Ubuntu16.04及以上 显卡驱动：NVIDIA系列（384以上） 驱动检查：终端命令nvidia-smi 新安装的Ubuntu系统安装NVIDIA驱动步骤: a.删除旧驱动： `sudo apt-get remove –purge nvidia* b.禁用自带的nouveau nvidia驱动： 看下Nouveau是否已经被禁用 12&gt; lsmod | grep nouveau &gt; 如果已经没有任何显示说明不用禁用了，否则继续下面操作 12&gt; sudo vim /etc/modprobe.d/blacklist-nouveau.conf #创建一个文件（注：按一下i键，表示现在进行内容插入）&gt; 并添加如下内容： 123&gt; blacklist nouveau&gt; options nouveau modeset=0 &gt; 二、安装cuda9.0官网下载。有对应的deb和.run文件，有具体的安装命令。 推荐.run文件，因为deb文件安装，可能会修改已安装好的驱动和。 进入下载目录，给文件添加运行权限： 1chmod +x ./cuda_9.0.176_384.81_linux.run 运行安装 1sudo ./cuda_9.0.176_384.81_linux.run 启动安装程序，一直按空格到最后（可以选择Ctrl+c跳过），不用担心，到99%的时候，输入accept接受条款 注意：第一个提醒你是否安装驱动时，选“n”，其余都“y”，安装路径选择默认的路径。 以上为两种cuda的安装方法，不过并没有结束，安装完毕后必须添加环境 1gedit ~/.bashrc 1234# 在文件末尾添加环境变量export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64export PATH=$PATH:/usr/local/cuda-9.0/binexport CUDA_HOME=$CUDA_HOME:/usr/local/cuda-9.0 添加完后，终端命令：source ～/.bashrc 终端命令查看是否安装成功：nvcc -V，若出现nvcc未安装， 执行了 sudo apt-get install nvidia-cuda-toolkit 命令，nvcc命令可以正常执行，但是运行nvcc –version发现版本为5.5，与原来安装的CUDA 10.0不匹配，这将会导致一系列不兼容问题（比如在python中使用sk-cuda库就报错，因为nvcc版本不一致），遂通过如下步骤解决： 1、使用sudo apt-get autoremove nvidia-cuda-toolkit 卸载5.5版本 2、查看/usr/local/cuda/bin下是否有nvcc可执行程序，如果没有说明cuda没有正常安装，需要重新安装。 3、如果有nvcc可执行程序，切换到管理员权限，重新执行上面步骤，添加环境。 三、安装cudnn7.3.1去官网下载，此处下载的是cuDNN v7.3.1 Library for Linux版本。是一个.tgz文件。 下载完直接解压，解压会出现一个cuda文件夹，里面有两个文件include 和lib64，把里面的文件copy到/usr/local/cuda-9.0里面相应的目录里，具体命令如下： 123sudo cp cuda/include/cudnn.h /usr/local/cuda-9.0/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda-9.0/lib64sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn* 查看cudnn版本，检查是否安装好： 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 四、卸载（1）卸载CUDA 1sudo /usr/local/cuda-9.0/bin/uninstall_cuda_9.0.pl （2）卸载CUDNN 删除原来的cudnn文件： 12sudo rm -rf /usr/local/cuda-9.0/lib64/libcudnn*sudo rm -rf /usr/local/cuda-9.0/include/cudnn.h Ubuntu16.04下cuda9.0+cudnn7.0安装参考教程1 Ubuntu 多版本Cuda(8.0,9.0)以及CuDnn安装参考教程2 pytorch_gpu环境配置前提是已经配置好cuda9.0和cudnn7.0 在Anaconda环境下，新建一个pytorch-gpu环境，安装pytorch。 一、创建新的虚拟环境 12conda create -n pytorch_gpu python=3.7conda activate pytorch_gpu /source activate pytorch_gpu #激动虚拟环境 二、安装对应的cuda和cudnn 三、安装pytorch 官网下载对应版本的安装脚本，考虑操作系统、包、cuda版本、语言版本等。 1conda install pytorch torchvision cudatoolkit=9.0 -n pytorch_gpu -c pytorch Anaconda环境安装GPU版本Pytorch参考教程 tensorflow_gpu_keras环境配置Keras其实就是TensorFlow和Keras的接口（Keras作为前端，TensorFlow或theano作为后端），它也很灵活，且比较容易学。可以把keras看作为tensorflow封装后的一个API。 目前Keras是tensorflow官方的API。 查看已经安装的Python版本1ls | grep python 将pip服务器更换为国内的镜像服务器123# 如果不存在此文件夹，则创建之mkdir ~/.pipvim ~/.pip/pip.conf 内容如下 1234[global]index-url=http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com tensorflow-gpu和keras安装一、新建虚拟环境1conda create -n tensorflow_gpu python=3.6 (ipykernel) #选择加如jupyter notebook内核 找到环境支持的对应的版本： 二、安装tensorflow-gpu终端命令： 1pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp36-cp36m-linux_x86_64.whl 三、测试tensorflow-gpu是否安装成功测试代码如下： 1234567import tensorflow as tfa = tf.constant([1.,2.,3.,4.,5.,6.], shape=[2,3], name='a')b = tf.constant([1.,2.,3.,4.,5.,6.], shape=[3,2], name='b')c = tf.matmul(a,b)with tf.Session(config= tf.ConfigProto(log_device_placement=True)) as sess: print(sess.run(c)) 成功检测到GPU，表明安装成功，且可以调用GPU运行。 GPU使用率：terminal下run：nvidia-smi -q -g 0 -d UTILIZATION -lor run：watch -n 2 nvidia-smi 注意可能会出现tensorflow-gpu版本和cuda，cudnn版本的不匹配问题，需要卸载 ubuntu下tensorflow-gpu环境搭建 四、查看tensorflow-gpu安装版本1234567在python环境中输入：import tensorflow as tftf.__version__ (查看版本）tf.__path__ （查看路径） 五、卸载命令12sudo pip uninstall tensorflowsudo pip3 uninstall tensorflow 六、安装Keras1pip install keras -U --pre 七、验证安装123进入python命令环境import keras 1234567891011Keras测试：Keras中mnist数据集测试 下载Keras开发包conda install gitgit clone https://github.com/fchollet/keras.gitcd keras/examples/python mnist_mlp.py 在《Tensorflow技术解析与实战》一书中提到，keras安装后需要选择依赖的后端，在~/.Keras/keras.json下修改最后一行backend对应的值，（默认后端是tensorflow，不用修改）也就是： 1234567891011｛"image_dim_ordering":"tf","epsilon":le-07,"floatx":"float32""backend": "tensorflow"｝ 八、出现问题安装keras出现：ImportError: cannot import name &#39;tf_utils&#39; 123456789101112131415161718&gt;&gt;&gt; import tensorflow&gt;&gt;&gt; import kerasUsing TensorFlow backend.Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/__init__.py", line 3, in &lt;module&gt;from . import utils File "/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in &lt;module&gt;from . import conv_utils File "/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in &lt;module&gt;from .. import backend as K File "/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/__init__.py", line 1, in &lt;module&gt;from .load_backend import epsilon File "/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/load_backend.py", line 90, in &lt;module&gt;from .tensorflow_backend import * File "/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 13, in &lt;module&gt;from tensorflow.python.keras.utils import tf_utilsImportError: cannot import name 'tf_utils' 安装的keras版本是2.3.1的，卸载安装keras2.1.5版本，解决该问题，首先卸载现有keras版本： 1pip uninstall keras 安装keras2.1.5版本： 1pip install keras==2.1.5]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[受限玻尔兹曼机和深度置信网络]]></title>
    <url>%2Fblog%2F2020%2F04%2F20%2FRBM%26DBN%2F</url>
    <content type="text"><![CDATA[受限玻尔兹曼机梯度下降法（相关的L-BFGS(拟牛顿法)等）在使用初始化权重的深度网络上效果不好的原因： 在使用BP算法计算导数时，随着网络的深度增加，反向传播的梯度（从输出层到网络的最初几层）的幅度值急剧减小，造成整体的损失函数相对于最初的几层的权重的导数非常小，使得最初几层的权重变化非常缓慢，以至于不能从样本中进行有效的学习。称为“梯度弥散”。 与此相关，当神经网络的最后几层含有足够的神经元后，可能单独这几层就可以对有标签数据进行建模，而不用最初几层的帮助。因此，对所有层都使用随机初始化方法训练得到的整个网络的性能将会与训练得到的浅层网络（仅有最后几层构成的浅层网络）的性能类似。 Hinton提出了的RBM，是一类具有两层结构的、对称链接无自反馈的随机神经网络模型(一种特殊的马尔科夫随机场)。 一个RBM中，v表示所有可见单元，h表示所有隐藏单元，确定模型，需要模型的三个参数$\theta = \{W,A,B\}$即可。分别是权重矩阵$W$，可见层单元偏置$A$，隐藏单元偏置$B$。 一个RBM包含一个由随机隐单元构成的隐藏层（一般是伯努利分布）和一个随机的观测单元构成的可见层（一般是伯努利分布或高斯分布）。RBM可以表示成一个二分图模型，所有的可见单元和隐单元之间存在连接，隐单元两两之间和可见单元两两之间不存在连接，即层间全连接，层内无连接（区别：BM模型层间、层内全连接）。 每一个可见层节点和隐藏层节点都有两种状态，激活和未激活。节点激活的概率由可见层和隐藏层节点的分布函数计算。 假设一个RBM有n个可见单元和m个隐藏单元，$v_i$表示第i个可见单元，$h_j$表示第j个隐单元，参数形式为： $W = \{w_{i,j} \in R^{n \times m}\}，w_{i,j}$表示第i个可见单元和第j个隐单元之间的权值 $A = \{a_i\} \in R^m,a_i$表示第i个可见单元的偏置阈值 $B = \{b_j\} \in R^n,b_j$表示第i个隐单元的偏置阈值 给定一组状态下的$(v,h)$的值，假设可见单元和隐藏单元均服从伯努利分布，RBM的能力公式为：$$E(v,h|\theta) = - \sum\limits_{i=1}^n a_i v_i - \sum\limits_{j=1}^m b_jh_j - \sum\limits_{i=1}^n \sum\limits_{j=1}^mv_i W_{ij}h_j$$其中，令$\theta = \{W_{ij},a_i,b_j\}$是RBM模型的参数，能量函数表示在每一个可见节点的取值和每一个隐藏层节点的取值之间都存在一个能量值。 对能量函数的指数化和正则化后可以得到可见层节点集合和隐藏节点集合分别处于某一种状态下$(v,h)$联合概率分布公式：$$P(v,h|\theta) = \frac{e^{-E(v,h|\theta)}}{Z(\theta)}$$ $$Z(\theta) = \sum\limits_{v,h} e^{-E(v,h|\theta)}$$ $Z(\theta)$为归一化因子或配分函数(partition function)，表示对可见层和隐藏层节点集合的所有可能状态的(能量指数)的求和。 对于参数的求解使用似然函数求导的方法，已知联合概率分布$P(v,h|\theta)$，通过对隐藏层节点集合的所有状态求和，可以得到可见层节点集合的边缘分布$P(v|\theta)$：$$P(v|\theta) = \frac{1}{Z(\theta)} \sum\limits_h e^{-E(v,h|\theta)}$$边缘分布表示的是可见层节点集合处于某一种状态分布下的概率，也被称为似然函数。 RBM模型性质： （1）在给定可见单元的状态时，各个隐含层单元的激活状态之间是条件独立的，此时，第j个隐单元的激活概率为：$$P(h_j = 1|v) = \sigma(b_j + \sum\limits_iv_i W_{ij})$$（2）当给定隐单元的状态时，可见单元的激活概率同样是条件独立的：$$P(v_i = 1|h) = \sigma(a_i + \sum\limits_j W_{ij}h_j)$$其中，$\sigma(x) = \frac{1}{1+\exp(-x)}$，sigmoid函数可以使得无论模型的可见层输入节点数据处于一个多大的范围内，都可以通过sigmoid函数求得它相应的函数值，即节点的激活概率值。 模型参数求解训练一个RBM主要任务是调参，以拟合给定的训练样本，使得在该参数下RBM表示的可见层节点概率分布尽可能与训练数据相符合。 首先，确定可见层和隐含层节点个数，可见层节点数等于输入数据的维数，隐藏层节点个数一般需要根据使用而定，或者参数一定的情况下，使得模型的能量最小时的隐藏层节点个数。 其次，求解模型的三个参数，$\theta = \{W_{ij},a_i,b_j\}$，参数求解使用到似然函数的对数对参数求导。 具体求解过程：$$P(v|\theta) = \frac{1}{Z(\theta)} \sum\limits_h e^{-E(v,h|\theta)}$$能力E与概率P是成反比的关系，最大化P，使得能量E最小，最大化似然函数方法是 梯度上升法。$$\theta = \theta + \mu \frac{\partial \ln P(v^t)}{\partial \theta}$$其中$v^t$表示输入数据，对原来$\theta$进行修改，迭代使得似然函数$P$最大。$$\frac{\partial \ln P(v^t)}{\partial w_{i,j}} = P(h_j =1|v^t)v_i^t - \sum\limits_v P(v)P(h_j = 1|v)v_i$$ $$\frac{\partial \ln P(v^t)}{\partial a_i} = v_i^t - \sum\limits_v P(v)v_i$$ $$\frac{\partial \ln P(v^t)}{\partial b_j} =P(h_j = 1|v^t) - \sum\limits_v P(v)P(h_j = 1|v)$$ 式中第二项$P(v)$仍然含有参数，$P(v)$表示可见层节点的联合概率，很多提出通过采样逼近的方法来求。 模型训练算法1、Gibbs采样算法求解可见层节点的联合概率$P(v)$，逼近其近似值。 Gibbs采样思想： 不知道一个样本数据$\pmb x = (x_1,x_2,\dots,x_N)$的联合概率$P(x)$，但是知道样本中的每一个数据的条件概率$p(x_i|\pmb x)$（假设每一个变量都服从一种概率分布），可以先求出每一个数据的条件概率值，得到$x$的任一状态$[x_1(0),\dots,x_i(0),\dots,x_N(0)]$，然后用条件概率公式迭代对每一个数据求条件概率，最终，迭代$k$次，$x$的某一个状态$[x_1(k),\dots,x_i(k),\dots,x_{N}(k)]$将收敛于$x$的联合概率分布$P(x)$。 假设一个训练样本$v_0$，根据公式$P(h_j=1 |v)$求$h_0$中每个节点的条件概率，再由$P(v_i = 1|h)$，求$v_1$中每个节点的条件概率，依次迭代，直到执行$K$步（足够大），此时$P(v|h)$的概率将收敛于$P(v)$的概率：$$h_0 \sim P(h|v_0) \ v_1 \sim P(v|h_0) \ h_1 \sim P(h|v_1) \ v_2 \sim P(v|h_1) \ \dots \ v_{k+1} \sim P(v|h_k)$$ 对于RBM执行过程如下： 2、CD-K算法CD算法是需要K次Gibbs采样对可见层节点进行重构得到可见层节点的概率分布。 算法思想：假设给模型一个样本$v_0$，通过$P(h_j =1| v) = \sigma (b_j + \sum\limits_i v_i W_{ij} )$，求出所有隐藏层节点的概率值，然后每一个概率值和随机数进行比较得到每一个隐藏层节点的状态，然后通过:$$P(v_i=1 | h) = \sigma (a_i + \sum\limits_j W_{ijh_j})$$求出每一个可见层节点的概率值，再由：$$P(h_j =1|v) = \sigma(b_j + \sum\limits_i v_iW_{ij})$$求出每一个隐含层节点的概率值，最后参梯度的计算公式变为：$$\Delta W_{ij} = \mu(&lt;v_ih_j&gt;_{data} - &lt;v_ih_j&gt;_{recon}) \ \Delta a_i = \mu (&lt;v_i&gt; _{data} - &lt;v_i&gt; _{recon}) \ \Delta b_j =\mu(&lt;h_j&gt;_{data} - &lt;h_j&gt;_{recon})$$其中，$\mu$是学习率，data和recon分别表示训练数据的概率分布和重构后的概率分布。 依此求出参数的梯度，由每一个参数的梯度对原参数进行修改来使得模型的能量减小。 3、模型的评估一般采用近似的方法对模型进行评估，常用的近似方法是重构误差，即以训练样本作为初始状态，经过RBM模型的分布进行一次Gibbs采样后与原数据的差异值。 对于一个给定样本$P(h_j =1 |v) = \sigma (b_j + \sum\limits_i v_i W_{ij})$，通过$P(v_i = 1|h) = \sigma(a_i + \sum\limits_j W_{ij}h_j)$对所有隐含层节点的条件概率进行采样，再通过对所有可见层节点的条件概率进行采样，最后由样本值和采样出的可见层概率值做差取绝对值，作为该模型的评估。 4、节点状态确定对于每一层节点状态(0 or 1)的确定采用与随机数（0-1之间）进行比较的方法，如果节点被激活的概率值大于一个随机产生的数，则认为该节点被激活。 原理：给定可见层节点$v$，隐含层第$j$个节点状态为1的激活概率为$p_j^v$，此时，在区间$[0,1]$上产生一个随机数$r_j$落在子区间$[0,p_j^v)$的概率也是$p_j^v$，两个事件的概率是相等的，若随机数$r_j$落在子区间$[0,p_j^v)$，认为第$j$个节点的状态为1，否则为0。 总结RBM网络模型的最大的目的是最大可能的拟合输入数据，最大化$P(v)$，Hinton提出一种快速训练算法contrastive divergence（CD）算法，需要迭代K次，一般K=1。 CD算法开始用训练数据去初始化可见层，然后用条件分布计算隐藏层，再根据隐藏层状态，同样，用条件分布计算可见层，这样产生的结果就是对训练数据的一个重构。 根据CD算法可得模型对网络权值的梯度：$$\Delta W_{ij} = \mu (&lt;v_ih_j&gt;_{data} - &lt;v_ih_j&gt;_{recon})$$其中，$\mu$为学习率，$&lt;v_ih_j&gt;_{data}$是样本数据的期望，$&lt;v_ih_j&gt;_{recon}$是重构后可见层数据的期望。 深度置信网络深度置信网络(Deep Belief Network,DBN)是由RBM堆叠而成的，从而作为DBN预训练的基础模型。 不是只有RBM可以堆叠为一个深度生成式（或判别式）网络，其他类型的网络也可以使用相同的方法来生成网络，比如自动编码器(autoencoder)的变形。 将一定的数量的RBM堆叠成一个DBN，然后从底向上逐层预训练，堆叠过程如下： 训练一个高斯-伯努利RBM（对于语音应用使用连续的特征）或伯努利-伯努利RBM（对于正态分布或二项式分布特征应用，如黑白图像或者编码后的文本） 将隐单元的激活概率（activation probabilities）作为下一层伯努利-伯努利RBM的输入数据。 第二层伯努利-伯努利RBM的激活概率作为第三层伯努利-伯努利RBM的可见层输入数居，以后各层以此类推。 关于这种有效的逐层贪婪学习策略达到了近似的最大似然学习。这个学习过程是无监督的，所以不需要标签信息。 当应用到分类任务时，生成式预训练可以和其他算法结合使用，典型的是判别式方法，他通过有效地调整所有权值来改善网络的性能。 判别式精调（fine-tune）通常是在现有网络的最后一层上在增加一层节点，用来表示想要的输出或者训练数据提供的标签，它与标准的前馈神经网络（feed-forward neural network）一样，可以使用反向传播算法（back-propagation algorithm）来调整或精调网络的权值。DBN最后一层即标签层的内容，根据不同的任务和应用来确定。 研究已经表明其他种类的预训练策略的有效性。比如，在执行逐层贪婪训练时，可以在每一层的生成损失函数中增加一项判别项。 如果不适用生成式预训练，只使用随机梯度下降方法来对随机初始化DBN进行判别式训练，研究结果表明，当非常仔细的选取初始权值并且谨慎的选取适合于随机梯度下降的“迷你批量”（mini-batch）的大小（例如，随着训练轮数增加大小），也将会获得很好的效果。 “迷你批量”用于在收敛速度和噪声梯度之间进行折中。同时，在建立“迷你批量”时，对数据进行充分的随机化也是至关重要的。 另外，从一个只含有一层的浅层神经网络开始学习一个DBN是非常有效的。当折中方法用于训练区分式模型时（使用提前结束训练的策略以防止过拟合的出现），在第一个隐藏层和标签的softmax输出层之间插入第二个隐藏层，然后对整个网络应用反向传播来微调网络的权值。这种判别式预训练在实践中取得了比较好的效果，特别是在有大量的训练数据的情况下效果较好。当训练数据不断增多时，即使不使用上述预训练，一些特别设计的随机初始化方法也能够取得很好的效果。 将隐单元的激活概率（activation probabilities）作为下一层伯努利-伯努利RBM的输入数据。 第二层伯努利-伯努利RBM的激活概率作为第三层伯努利-伯努利RBM的可见层输入数据 RBM和深度置信网络原理]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python3教程]]></title>
    <url>%2Fblog%2F2020%2F04%2F20%2Fpython3_%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[print功能print 默认输出是换行，如果要实现不换行需要在变量末尾加上 end=”” 1print("hello world") hello world 1print('hello'+'world') helloworld 1print('hello'+4) #字符串不能直接和数字相加 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-4-3969ea1f57ce&gt; in &lt;module&gt; ----&gt; 1 print(&apos;hello&apos;+4) #字符串不能直接和数字相加 TypeError: can only concatenate str (not &quot;int&quot;) to str 1print(int(2.6)+3)#整型 5 1print(float('1.2')+3)#浮点型 4.2 基础数学运算 运算符 描述 实例 / 除 b/a输出结果为2.1 % 取模-返回除法的余数 b%a输出结果为1 ** 幂-返回x的y次幂 x的y次幂 // 取整除 9//2=4 1234567891011121314%matplotlib inline import matplotlib.pyplot as pltfrom numpy import pi,sin,linspacefrom matplotlib.mlab import stineman_interpx=linspace(0,2*pi,20)y=sinxyp=Nonexi=linspace(x[0],x[-1],100)yi=stineman_interp(xi,x,y,yp)fig.ax = plt.subplots()ax.plot(x,y,'ro',xi,yi,'-b.')plt.show() --------------------------------------------------------------------------- ImportError Traceback (most recent call last) &lt;ipython-input-1-1282f69f7ac5&gt; in &lt;module&gt;() ----&gt; 1 get_ipython().run_line_magic(&apos;matplotlib&apos;, &apos;inline&apos;) 2 import matplotlib.pyplot as plt 3 from numpy import pi,sin,linspace 4 from matplotlib.mlab import stineman_interp 5 C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\interactiveshell.py in run_line_magic(self, magic_name, line, _stack_depth) 2129 kwargs[&apos;local_ns&apos;] = sys._getframe(stack_depth).f_locals 2130 with self.builtin_trap: -&gt; 2131 result = fn(*args,**kwargs) 2132 return result 2133 &lt;C:\Anaconda3\envs\py3\lib\site-packages\decorator.py:decorator-gen-108&gt; in matplotlib(self, line) C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\magic.py in &lt;lambda&gt;(f, *a, **k) 185 # but it&apos;s overkill for just that one bit of state. 186 def magic_deco(arg): --&gt; 187 call = lambda f, *a, **k: f(*a, **k) 188 189 if callable(arg): C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\magics\pylab.py in matplotlib(self, line) 97 print(&quot;Available matplotlib backends: %s&quot; % backends_list) 98 else: ---&gt; 99 gui, backend = self.shell.enable_matplotlib(args.gui) 100 self._show_matplotlib_backend(args.gui, backend) 101 C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\interactiveshell.py in enable_matplotlib(self, gui) 3035 &quot;&quot;&quot; 3036 from IPython.core import pylabtools as pt -&gt; 3037 gui, backend = pt.find_gui_and_backend(gui, self.pylab_gui_select) 3038 3039 if gui != &apos;inline&apos;: C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\pylabtools.py in find_gui_and_backend(gui, gui_select) 271 &quot;&quot;&quot; 272 --&gt; 273 import matplotlib 274 275 if gui and gui != &apos;auto&apos;: ImportError: No module named &apos;matplotlib&apos; python中^符号，用两个** 1233**2 # **2表示2次方3**3 #**3表示3次方3**4 81 取余符号为% 18%3 2 比较运算符 运算符 描述 == 等于 != 不等于 赋值运算符 运算符 描述 %= 取模赋值 **= 幂赋值 //= 取整赋值 位运算符 运算符 描述 &amp; 按位与 \ 按位或 ^ 按位异或 ~ 按位取反 &lt;&lt; 左移动运算 &gt;&gt; 右移动运算 逻辑运算符 运算符 描述 and 布尔“与”-如果x为false，x and y返回false，否则返回y or 布尔“或”-如果x为true， x or y 返回x的值，否则返回y的值 not 布尔“非”-如果x为true，not x 返回false，反之 成员运算符 运算符 描述 in 如果在指定序列中找到，返回true，否则false not in 如果在指定的序列中没有找到，返回true，否则false 身份运算符 运算符 描述 is 判断两个标识符是不是引自一个对象，是返回true，否则false is not 判断两个标识符是不是引自不同对象，是返回true，否则false 运算优先级变量命名规则12apple=1 #赋值数字print(apple) 1 123apple='iphoneX'#赋值字符串print('apple')print(apple) apple iphoneX 12a,b,c=11,12,13#一次性定义多个变量print(a,b,c) 11 12 13 循环while循环while condition: expressionscondition为判断条件，即true和false中一个，如果为true执行expressions语句，否则跳出循环继续往下执行。会有意外缩进产生。 1234condition=0while condition &lt; 10: print(condition) #顶格写不在循环 condition = condition+1 0 1 2 3 4 5 6 7 8 9 1234condition=10while condition: print(condition) condition -=1 10 9 8 7 6 5 4 3 2 1 var = 1while var ==1: #表达式永远为True num = int(input(“输出一个数字：”)) print (“你输入的数字是：”，num) 小于(&lt;);等于(==);不等于(!=);不大于(&lt;=)会返回true和false值 none类型：如果while后面接着的语句类型none，将会返回false。 1234a=range(10) while a: print(a[-1])#打印出最后一位，将去除最后一位的列表a重新赋值给列表a a=a[:len(a)-1] 9 8 7 6 5 4 3 2 1 0 while循环使用else语句 123456count =0while count &lt; 5: print (count,"小于5") count=count +1else: print(count,"大于或等于5") 0 小于5 1 小于5 2 小于5 3 小于5 4 小于5 5 大于或等于5 for循环基本用法：for item in sequence: expressions其中:sequence为可迭代的对象，item为序列中的每个对象 12345example_list=[1,2,3,4,5,6,7,12,543,876,12,3,2,5]for i in example_list: print(i) print('inner of for')#每次循环都会输出inner of forprint('outer of for')#循环结束后输出一次out of for 1 inner of for 2 inner of for 3 inner of for 4 inner of for 5 inner of for 6 inner of for 7 inner of for 12 inner of for 543 inner of for 876 inner of for 12 inner of for 3 inner of for 2 inner of for 5 inner of for outer of for range使用python内置工厂函数，range函数会返回一个序列，有三种使用方法： range(start,stop)start是序列的起始值，stop是结束值，但不包括该值，类似的数学中的表达为：左闭右开区间。 range(stop)省略start，重0开始，相当于range(0,stop) range(start,stop,step)step表示步长，即相隔两个值的差值，从start开始，依次增加step值，直至等于或大于stop。 1234for i in range(1,10): print(i)for i in range(1,10,5): print(i) 1 2 3 4 5 6 7 8 9 1 6 集合range()和len()函数可以遍历一个序列的索引。 123a = ['google','baidu','runoob','taobao','qq']for i in range(len(a)): print(i,a[i]) 0 google 1 baidu 2 runoob 3 taobao 4 qq 使用range()函数创建一个列表 123list1=list( range(0,5) )print(list1) [0, 1, 2, 3, 4] break和continue语句及循环中的else子句12345678910111213for letter in 'Runoob': # 第一个实例 if letter == 'b': break print ('当前字母为 :', letter) var = 10 # 第二个实例while var &gt; 0: print ('当期变量值为 :', var) var = var -1 if var == 5: break print ("Good bye!") 当前字母为 : R 当前字母为 : u 当前字母为 : n 当前字母为 : o 当前字母为 : o 当期变量值为 : 10 当期变量值为 : 9 当期变量值为 : 8 当期变量值为 : 7 当期变量值为 : 6 Good bye! continue语句用于跳出当前循环块中的剩余语句，然后继续进行下一轮循环 123456789101112for letter in 'Runoob': # 第一个实例 if letter == 'o': # 字母为 o 时跳过输出 continue print ('当前字母 :', letter) var = 10 # 第二个实例while var &gt; 0: var = var -1 if var == 5: # 变量为 5 时跳过输出 continue print ('当前变量值 :', var)print ("Good bye!") 当前字母 : R 当前字母 : u 当前字母 : n 当前字母 : b 当前变量值 : 9 当前变量值 : 8 当前变量值 : 7 当前变量值 : 6 当前变量值 : 4 当前变量值 : 3 当前变量值 : 2 当前变量值 : 1 当前变量值 : 0 Good bye! 循环可以有else子句，在穷尽列表（以for循环）或条件变为false（以while循环）导致循环终止时被执行，但循环被break终止时不执行。 12345678for n in range(2,10): for x in range(2,n): if n%x == 0: print(n,'等于',x,'*',n //x) break else: #循环中没有找到元素 print(n,'是质数') 3 是质数 4 等于 2 * 2 5 是质数 5 是质数 5 是质数 6 等于 2 * 3 7 是质数 7 是质数 7 是质数 7 是质数 7 是质数 8 等于 2 * 4 9 是质数 9 等于 3 * 3 pass语句pass语句是空语句，为了保持程序结构的完整性；pass不做任何事情，一般用于占位语句。 1234567for letter in 'Runoob': if letter == 'o': pass print ('执行 pass 块') print ('当前字母 :', letter) print ("Good bye!") 当前字母 : R 当前字母 : u 当前字母 : n 执行 pass 块 当前字母 : o 执行 pass 块 当前字母 : o 当前字母 : b Good bye! 内置集合集合类型：python中有list,tuple（元组，数组）,dict,set等，如果该集合对象作为while判断语句，若集合元素为0 ，返回false，否则返回true。 1234tup=('python',2.7,64)#tuple类型for i in tup: print(i)#程序按行输出 python 2.7 64 123456dic=&#123;&#125; #dictionary模型dic['lan']='python'dic['version']=2.7dic['platform']=64for key in dic: print(key,dic[key])#key是乱序的 lan python version 2.7 platform 64 123s=set(['python','python2','python3','python']) #set类型for item in s: print(item) #输出乱序，去除重复项 python2 python3 python 迭代器python中for语句实际上实现了设计模式中的迭代器模式，可以按照迭代器的要求生成迭代器对象，以便于在for语句中使用。 只要在类中实现了iter()和next()函数，对象就可以在for语句中使用。 字符串，列表或元组对象都可用于创建迭代器。 123456list = [1,2,3,4]it = iter(list) #创建迭代器print (next(list)) #输出迭代器的下一个元素for x in it: print (x,end="") --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-1-0fb025b0a758&gt; in &lt;module&gt;() 1 list = [1,2,3,4] 2 it = iter(list) #创建迭代器 ----&gt; 3 print (next(list)) #输出迭代器的下一个元素 TypeError: &apos;list&apos; object is not an iterator 12345678910import sys #引入 sys 模块list = [1,2,3,4]it = iter(list) #创建迭代器对象while True: try: print(next(it)) except StopIteration: sys.exit() 1 2 3 4 An exception has occurred, use %tb to see the full traceback. SystemExit C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\interactiveshell.py:2969: UserWarning: To exit: use &apos;exit&apos;, &apos;quit&apos;, or Ctrl-D. warn(&quot;To exit: use &apos;exit&apos;, &apos;quit&apos;, or Ctrl-D.&quot;, stacklevel=1) 创建一个迭代器把一个类作为一个迭代器需要在类中实现两个方法__iter__()与__next__()。类都有一个构造函数，python的构造函数为__init__()，它会在对象初始化的时候执行。 __iter__()方法返回一个特殊的迭代器对象，这个迭代器实现了__next__()方法并通过StopIteration异常标识迭代的完成。 __next__()方法（python2里是next()）会返回下一个迭代对象。 1234567891011121314151617181920# define a Fib classclass Fib(object): def _init_(self, max): #注意init左右有两个_ self.max = max self.n, self.a, self,b=0, 0, 1 def _iter_(self): return self def _next_(self): if self.n &lt; self.max: r = self.b self.a, self.b = self.b, self.a + self.b self.n = self.n + 1 return r raise StopIteration() #using fib objectfor i in Fib(5): print(i) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-50-3c5b777610b9&gt; in &lt;module&gt; 17 18 #using fib object ---&gt; 19 for i in Fib(5): 20 print(i) TypeError: Fib() takes no arguments 1234567891011121314151617181920# define a Fib classclass Fib(object): def __init__(self, max): self.max = max self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def __next__(self): if self.n &lt; self.max: r = self.b self.a, self.b = self.b, self.a + self.b self.n = self.n + 1 return r raise StopIteration()# using Fib objectfor i in Fib(5): print(i) 1 1 2 3 5 StopIteration异常用于 标识迭代的完成，防止出现无线循环的情况，在__next()__方法中可以设置在完成指定循环次数后触发StopIteration异常结束迭代。 123456789101112131415161718class MyNumbers: def __iter__(self): self.a = 1 #初始值为1 return self def __next__(self): if self.a &lt;=20: x = self.a self.a += 1 #逐步增1 return x else: raise StopIteration #myiter = iter(MyNumbers())myiter = list(MyNumbers())for x in myiter: print(x) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 生成器python中使用yield关键字也能实现类似迭代的效果，yield语句每次执行时，立即返回结果给上层调用者，而当前的状态仍然保留，以便于迭代器下一次循环调用。 生成器是一个返回迭代器的函数，只能用于迭代操作，简单理解就是生成器就是一个迭代器。 调用生成器运行时，每次遇到yield时函数会暂停并保存当前所有的运行信息，返回yield的值，并在下一次执行 next()方法时从当前位置继续运行。 节约硬件资源，在需要的时候才会执行，并且只执行一次。 调用一个生成器函数，返回的是一个迭代器对象。 1234567891011def fib(max): a,b=0,1 while max: r=b a,b=b,a+b max-=1 yield r #using generator for i in fib(5): print(i) 1 1 2 3 5 1234567891011121314151617import sys def fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成 while True: try: print (next(f), end=" ") except StopIteration: sys.exit() 0 1 1 2 3 5 8 13 21 34 55 An exception has occurred, use %tb to see the full traceback. SystemExit C:\Anaconda3\envs\py3\lib\site-packages\IPython\core\interactiveshell.py:2969: UserWarning: To exit: use &apos;exit&apos;, &apos;quit&apos;, or Ctrl-D. warn(&quot;To exit: use &apos;exit&apos;, &apos;quit&apos;, or Ctrl-D.&quot;, stacklevel=1) if判断if condition: expression若condition的值为true，将会执行expressions语句内容，否则跳出该语句往下执行 1234567x=1y=2z=3if x&lt;y: print('x is less than y')if x&lt;y&lt;z: #最好写成x&lt;y and y&lt;z print('x is less than y,ang y is less than z') x is less than y x is less than y,ang y is less than z 1234x=2y=2if x==y: print('x is equal to y') x is equal to y if else判断if condition: true_expressionselse: false_expressions 1234567x =1y=2z=3if x&gt;y: print('x is greater than y')else: print('x is less or equal to y') x is less or equal to y 高级主题var = var1 if condition else var2如果condition的值为true，那么将var1赋给var；如果为false，则将var2的值赋给var。 123worked = Trueresult = 'done' if worked else 'not yet'print(result) done if elif else判断使用规则：if condition1: true1_expressionselif condition2: true2_expressionselif condtion3: true3_expressionselif … …else: else_expressions有 多个判断条件，可以通过elif语句添加多个判断条件，一旦某个条件为true，那么将执行对应的expression。并在代码执行完毕后跳出该if-elif-else语句块，往下执行。 12345678910x=4y=2z=3if x&gt;1: print('x&gt;1')elif x&lt;1: print('x&lt;1')else: print('x=1')print('finish') x&gt;1 finish 向量化例子（去除代码中的for循环）123import numpy as np #导入numpy库a = np.array([1,2,3,4]) #创建一个数据aprint(a) [1 2 3 4] 123456789101112131415import time #导入时间库a = np.random.rand(1000000)b = np.random.rand(1000000)#通过round随机得到两个一百万维度的数组tic = time.time()#测量当前时间c = np.dot(a,b)toc = time.time()#测量时间print(c)print("vectorized version:" + str(1000*(toc-tic)) + "ms")c=0tic = time.time()for i in range(1000000): c += a[i]*b[i]toc = time.time()print(c)print("for loop :" + str(1000*(toc-tic)) + "ms") 250120.5894239894 vectorized version:1.9960403442382812ms 250120.5894239858 for loop :386.0652446746826ms python中的广播123456789import numpy as npA= np.array([[56.0,0.0,4.4,68.0], [1.2,104.0,52.0,8.0], [1.8,135.0,99.0,0.9]])print(A)cal = A.sum(axis =0) #求和运算，按列执行 axis用来指明进行的运算是沿着哪个轴执行，在numpy中0轴是列，1轴是水平的，即行。print(cal)percentage = 100*A/cal.reshape(1,4) #将3*4矩阵A除以一个1*4矩阵，得到一个3*4矩阵print(percentage) [[ 56. 0. 4.4 68. ] [ 1.2 104. 52. 8. ] [ 1.8 135. 99. 0.9]] [ 59. 239. 155.4 76.9] [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。 15 关于python_numpy向量的说明123456import numpy as npa = np.random.randn(5) #生成5个高斯随机数变量存储在数组a中print(a) #数组a的形状shape是一个(5,)形状，一个一维数组，不是行/列向量。不直观np.shape(a) #输出数组的shape导入numpy模块，里面有一个shape函数，要使用这个函数，numpy.shape()即可，但是不加numpy就不行，因为只有在numpy的命名空间里面才有个shapeprint(a.T) #输出a的转置print(np.dot(a,a.T))#输出数组a和a的转置的内积，结果不是矩阵，是一个数。 [ 0.19718626 -0.55938823 -0.14788369 -1.8337937 0.63666497] [ 0.19718626 -0.55938823 -0.14788369 -1.8337937 0.63666497] 4.141808818121443 12345import numpy as npa = np.random.randn(5,1) #如果设置a为(5,1)，则将其置于5行1列向量中，print(a)print(a.T) #此时a的转置是一个行向量，上面只有一对方括号，现在有两对，即1行5列的矩阵和一维数组的差别。print(np.dot(a,a.T)) #此时输出是一个矩阵 [[ 0.13184372] [-1.72236441] [ 0.89974126] [ 0.28644756] [ 0.35497523]] [[ 0.13184372 -1.72236441 0.89974126 0.28644756 0.35497523]] [[ 0.01738277 -0.22708294 0.11862524 0.03776631 0.04680126] [-0.22708294 2.96653915 -1.54968232 -0.49336707 -0.6113967 ] [ 0.11862524 -1.54968232 0.80953434 0.25772868 0.31938586] [ 0.03776631 -0.49336707 0.25772868 0.0820522 0.10168179] [ 0.04680126 -0.6113967 0.31938586 0.10168179 0.12600741]] a = np.random.randn(5,1) a.shape=(5,1)，column vector（列向量）a = np.random.randn(1,5) a.shape=(1,5), row vector（行向量）要去简化你的代码，而且不要使用一维数组。总是使用 $n \times 1$ 维矩阵（基本上是列向量），或者 $1 \times n$ 维矩阵（基本上是行向量），这样你可以减少很多assert语句来节省核矩阵和数组的维数的时间。另外，为了确保你的矩阵或向量所需要的维数时，不要羞于 reshape 操作。 TensorFlow假设有一个损失函数$J(w)= w^2 -10w +25$，最小化，可见$w=5$。TensorFlow实现过程： 12345678910111213141516171819202122import numpy as npimport tensorflow as tf#导入TensorFloww= tf.Variable(0,dtype = tf.float32)#定义参数w，在TensorFlow中，使用tf.Variable()来定义cost = tf.add(tf.add(w**2,tf.multiply(- 10.,w)),25)#定义损失函数Jtrain = tf.train.GradientDescentOptimizer(0.01).minimize(cost)#学习率为0.01，目标是最小化损失init = tf.global_variables_initializer()session = tf.Session()#开启一个TensorFlow sessionsession.run(init)#初始化全局变量session.run(w)#让TensorFlow评估一个变量，将w初始化为0，并定义损失函数，定义train为学习算法#使用梯度下降法使得损失函数最小化#此时还没有运行学习算法。print(session.run(w)) 0.0 加载本地python文件执行命令：%load python文件的绝对路径ctrl+enter执行第一次执行，是将本地的Python文件内容加载到单元格内。此时，Jupyter Notebook会自动将“%load”命令注释掉（即在前边加井号“#”），以便在执行已加载的文件代码时不重复执行该命令；第二次执行，则是执行已加载文件的代码。 12# %load C:/Users/JXJ/Desktop/test.pyprint("hello world") hello world 直接运行python文件%run Python文件的绝对路径或!python3 Python文件的绝对路径或!python Python文件的绝对路径 ctrl+enter执行 1%run ~/Desktop/test.py hello world importimport和from…import来导入相应的模块。 数据类型py3中有6哥标准的数据类型： number（数字） string（字符串） list（列表） tuple（元组） set（集合） dictionary（字典） 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 数字Python3 支持 int、float、bool、complex（复数）。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。 12a,b,c,d=20,5.5,True,4+3jprint(type(a),type(b),type(c),type(d)) &lt;class &apos;int&apos;&gt; &lt;class &apos;float&apos;&gt; &lt;class &apos;bool&apos;&gt; &lt;class &apos;complex&apos;&gt; 还可以用isinstance来判断 12a=11isinstance(a,int) True isinstance 和 type 的区别在于： type()不会认为子类是一种父类类型。isinstance()会认为子类是一种父类类型。 还可以使用del语句删除一些对象引用。del语句的语法是：del var1[,var2[,var3[……,varN]]] stringPython中的字符串用单引号 ‘ 或双引号 “ 括起来，同时使用反斜杠 转义特殊字符。 字符串的截取的语法格式如下：变量[头下标：尾下标] 加号 + 是字符串的连接符， 星号 * 表示复制当前字符串，紧跟的数字为复制的次数 1234567891011str = 'Runoob'print(str) #输出字符串print(str[0:-1]) #输出第一个到倒数第二个的所有字符print (str[0]) # 输出字符串第一个字符print (str[2:5]) # 输出从第三个开始到第五个的字符print (str[2:]) # 输出从第三个开始的后的所有字符print (str * 2) # 输出字符串两次print (str + "TEST") # 连接字符串print('Ru\noob')print(r'Ru\noob') #字符串前加r（不能用空格），反斜杠不发生转义，表示原始字符串 Runoob Runoo R noo noob RunoobRunoob RunoobTEST Ru oob Ru\noob list(列表)列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号 [] 之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下：变量[头下标：尾下标] 加号 + 是列表连接运算符，星号 * 是重复操作 123456789list = [ 'abcd', 786 , 2.23, 'runoob', 70.2 ]tinylist = [123, 'runoob'] print (list) # 输出完整列表print (list[0]) # 输出列表第一个元素print (list[1:3]) # 从第二个开始输出到第三个元素print (list[2:]) # 输出从第三个元素开始的所有元素print (tinylist * 2) # 输出两次列表print (list + tinylist) # 连接列表 [&apos;abcd&apos;, 786, 2.23, &apos;runoob&apos;, 70.2] abcd [786, 2.23] [2.23, &apos;runoob&apos;, 70.2] [123, &apos;runoob&apos;, 123, &apos;runoob&apos;] [&apos;abcd&apos;, 786, 2.23, &apos;runoob&apos;, 70.2, 123, &apos;runoob&apos;] List 内置了有很多方法，例如 append()、pop() 等等，这在后面会讲到。 注意： 1、List写在方括号之间，元素用逗号隔开。2、和字符串一样，list可以被索引和切片。3、List可以使用+操作符进行拼接。4、List中的元素是可以改变的。 123456789101112131415161718192021222324#翻转字符串def reverseWords(input): # 通过空格将字符串分隔符，把各个单词分隔为列表 inputWords = input.split(" ") # 翻转字符串 # 假设列表 list = [1,2,3,4], # list[0]=1, list[1]=2 ，而 -1 表示最后一个元素 list[-1]=4 ( 与 list[3]=4 一样) # inputWords[-1::-1] 有三个参数 # 第一个参数 -1 表示最后一个元素 # 第二个参数为空，表示移动到列表末尾 # 第三个参数为步长，-1 表示逆向 inputWords=inputWords[-1::-1] # 重新组合字符串 output = ' '.join(inputWords) return output if __name__ == "__main__": input = 'I like runoob' rw = reverseWords(input) print(rw) runoob like I tuple(元组)元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： 123456789tuple = ( 'abcd', 786 , 2.23, 'runoob', 70.2 )tinytuple = (123, 'runoob') print (tuple) # 输出完整元组print (tuple[0]) # 输出元组的第一个元素print (tuple[1:3]) # 输出从第二个元素开始到第三个元素print (tuple[2:]) # 输出从第三个元素开始的所有元素print (tinytuple * 2) # 输出两次元组print (tuple + tinytuple) # 连接元组 (&apos;abcd&apos;, 786, 2.23, &apos;runoob&apos;, 70.2) abcd (786, 2.23) (2.23, &apos;runoob&apos;, 70.2) (123, &apos;runoob&apos;, 123, &apos;runoob&apos;) (&apos;abcd&apos;, 786, 2.23, &apos;runoob&apos;, 70.2, 123, &apos;runoob&apos;) 1234tup = (1,2,3,4,5,6)print(tup[0])print(tup[1:5]) 1 (2, 3, 4, 5) string、list 和 tuple 都属于 sequence（序列）。 注意： 1、与字符串一样，元组的元素不能修改。2、元组也可以被索引和切片，方法一样。3、注意构造包含 0 或 1 个元素的元组的特殊语法规则。4、元组也可以使用+操作符进行拼接。 set（集合）集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。 基本功能是进行成员关系测试和删除重复元素。 可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 123456789101112131415161718192021222324student = &#123;'Tom', 'Jim', 'Mary', 'Tom', 'Jack', 'Rose'&#125; print(student) # 输出集合，重复的元素被自动去掉 # 成员测试if 'Rose' in student : print('Rose 在集合中')else : print('Rose 不在集合中') # set可以进行集合运算a = set('abracadabra')b = set('alacazam') print(a) print(a - b) # a 和 b 的差集 print(a | b) # a 和 b 的并集 print(a &amp; b) # a 和 b 的交集 print(a ^ b) # a 和 b 中不同时存在的元素 {&apos;Rose&apos;, &apos;Tom&apos;, &apos;Mary&apos;, &apos;Jack&apos;, &apos;Jim&apos;} Rose 在集合中 {&apos;b&apos;, &apos;a&apos;, &apos;d&apos;, &apos;c&apos;, &apos;r&apos;} {&apos;b&apos;, &apos;d&apos;, &apos;r&apos;} {&apos;a&apos;, &apos;b&apos;, &apos;l&apos;, &apos;r&apos;, &apos;d&apos;, &apos;m&apos;, &apos;z&apos;, &apos;c&apos;} {&apos;a&apos;, &apos;c&apos;} {&apos;b&apos;, &apos;l&apos;, &apos;r&apos;, &apos;m&apos;, &apos;z&apos;, &apos;d&apos;} dictionary列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。 键(key)必须使用不可变类型。 在同一个字典中，键(key)必须是唯一的。 123456789101112dict = &#123;&#125;dict['one'] = "1 - 菜鸟教程"dict[2] = "2 - 菜鸟工具" tinydict = &#123;'name': 'runoob','code':1, 'site': 'www.runoob.com'&#125; print (dict['one']) # 输出键为 'one' 的值print (dict[2]) # 输出键为 2 的值print (tinydict) # 输出完整的字典print (tinydict.keys()) # 输出所有键print (tinydict.values()) # 输出所有值 1 - 菜鸟教程 2 - 菜鸟工具 {&apos;code&apos;: 1, &apos;name&apos;: &apos;runoob&apos;, &apos;site&apos;: &apos;www.runoob.com&apos;} dict_keys([&apos;code&apos;, &apos;name&apos;, &apos;site&apos;]) dict_values([1, &apos;runoob&apos;, &apos;www.runoob.com&apos;]) 构造函数 dict() 可以直接从键值对序列中构建字典如下：dict([(‘Runoob’, 1), (‘Google’, 2), (‘Taobao’, 3)])x: x**2 for x in (2, 4, 6)}dict(Runoob=1, Google=2, Taobao=3) 另外，字典类型也有一些内置的函数，例如clear()、keys()、values()等。 注意： 1、字典是一种映射类型，它的元素是键值对。2、字典的关键字必须为不可变类型，且不能重复。3、创建空字典使用 { }。 python数据类型转换需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 函数语法函数代码块以 def管金子开头，后接函数标识符名称和圆括号()。 任何传入的参数和自变量必须放在圆括号内，括号内可以用于定义参数。 函数的第一行语句可以选择性使用文档字符串来存放函数说明。 函数内容以冒号起始，并且缩进。 return[表达式]结束函数。不带表达式的return相当于返回none。 def 函数名(参数列表): 函数体 1234567891011#计算面积的函数def area(width,height): return width*heightdef print_welcome(name): print("welcome",name) print_welcome("Roob")w=4h=5print("width=",w, "height = ",h, "area = ", area(w,h)) welcome Roob width= 4 height = 5 area = 20 函数调用定义一个函数：给定函数名称，指定函数里包含的参数，和代码块结构。 可以通过另一个函数调用执行，也可以直接从python命令提示符执行。 1234567def printme(str): #打印任何出入的字符串 print(str) return#调用函数printme("调用任何函数")printme("再次调用") 调用任何函数 再次调用 参数传递a =[1,2,3] list类型a=&quot;runoob&quot; string类型变量a没有类型，仅仅是一个对象的引用（一个指针），可以指向list类型对象，也可以指向string类型对象。 可更改(mutable)与不可更改(immutable)对象在 python 中，strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。 不可变类型：变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变a的值，相当于新生成了a。 可变类型：变量赋值 la=[1,2,3,4] 后再赋值la[2]=5则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了。 python 函数的参数传递： 不可变类型：类似 c++ 的值传递，如 整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响 1234567#传不可变对象def changeint(a): a =10b=2changeint(b)print(b) 2 结果是2，在传递给changeint函数时，按传值形式复制了变量b，相当于修改了另一个复制的对象，本身b的值不受影响。还是2 1234567891011121314#传可变对象实例#可写函数说明def changeme( mylist ): # "修改传入的列表" mylist.append([1,2,3,4]) print ("函数内取值: ", mylist) return#调用changeme函数mylist = [10,20,30]changeme( mylist )print ("函数外取值: ", mylist) 函数内取值: [10, 20, 30, [1, 2, 3, 4]] 函数外取值: [10, 20, 30, [1, 2, 3, 4]] 参数必需参数：必须以正确的顺序传入函数，调用时的数量必须和声明的一样。 关键字参数：函数调用使用关键字参数来确定传入的参数值。使用关键字参数允许函数调用时参数的顺序和声明时不一致，python解释器能够用参数名匹配参数值。 默认参数：调用函数时，如果没有传递参数，则会使用默认参数。 不定长参数：函数处理比起始声明时更多的参数。声明时不会命名。基本语法如下：123456789101112131415161718def functionname([formal_args,] *var_args_tuple): "函数_文档字符串"function_suitereturn [expression]​```python#参数使用不需要使用指定顺序，关键字参数#可写函数说明def printinfo(name,age): #打印任何传入的字符串 print("名字：", name) print("年龄：",age) return#调用printinfo函数printinfo(age = 50,name = "runoob") 名字： runoob 年龄： 50 123456789101112#不定长参数# 可写函数说明def printinfo( arg1, *vartuple ):#加星号的参数会以元组tuple的形式，存放所有未命名的变量参数 # "打印任何传入的参数" print ("输出: ") print (arg1) print (vartuple) # 调用printinfo 函数printinfo( 70, 60, 50 ) 输出: 70 (60, 50) 如果在函数调用时没有指定参数，它就是一个空元组。也可以不向函数传递未命名的变量 123456789101112# 可写函数说明def printinfo( arg1, *vartuple ): "打印任何传入的参数" print ("输出: ") print (arg1) for var in vartuple: print (var) return # 调用printinfo 函数printinfo( 10 )printinfo( 70, 60, 50 ) 输出: 10 输出: 70 60 50 加了两个星号 ** 的参数会以字典的形式导入。 123456789# 可写函数说明def printinfo( arg1, **vardict ): "打印任何传入的参数" print ("输出: ") print (arg1) print (vardict) # 调用printinfo 函数printinfo(1, a=2,b=3) 输出: 1 {&apos;b&apos;: 3, &apos;a&apos;: 2} 匿名函数python使用lambda来创建匿名函数。 即不使用def语句这样的标准的形式定义一个函数 lambda只是一个表达式 lambda的主体是一个表达式，而不是一个代码块。仅能在lambda表达式中封装有限的逻辑进去。 lambda函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间中的参数。 lambda函数语法只包含一个语句，如下：lambda [arg1[,arg2,...argn]]:expression 123456# 可写函数说明sum = lambda arg1, arg2: arg1 + arg2 # 调用sum函数print ("相加后的值为 : ", sum( 10, 20 ))print ("相加后的值为 : ", sum( 20, 20 )) 相加后的值为 : 30 相加后的值为 : 40 变量作用域python中变量不是在任何位置都可以访问，访问权限决定于这个变量是在哪里赋值的。 作用域有四种：L （Local） 局部作用域E （Enclosing） 闭包函数外的函数中G （Global） 全局作用域B （Built-in） 内置作用域（内置函数所在模块的范围） 以 L –&gt; E –&gt; G –&gt;B 的规则查找，即：在局部找不到，便会去局部外的局部找（例如闭包），再找不到就会去全局找，再者去内置中找。 1234567891011total = 0 # 这是一个全局变量# 可写函数说明def sum( arg1, arg2 ): #返回2个参数的和." total = arg1 + arg2 # total在这里是局部变量. print ("函数内是局部变量 : ", total) return total #调用sum函数sum( 10, 20 )print ("函数外是全局变量 : ", total) 函数内是局部变量 : 30 函数外是全局变量 : 0 global和nonlocal关键字当内部作用域想修改外部作用域的变量时，用到。 123456789101112#修改全局变量num#用到关键字globalnum = 1def fun1(): global num print(num) num = 123 print(num) fun1()print(num) 1 123 123 12345678910111213#修改嵌套作用域（enclosing作用域，外层非全局作用域）中的变量#使用nonlocal关键字def outer(): num = 10 def inner(): nonlocal num num = 100 print(num) inner() print(num)outer() 100 100 1234567891011121314"""a = 10def test(): a = a + 1 print(a)test()#test函数中的a使用的是局部，未定义，无法修改。报错"""#修改a为全局变量，通过函数参数传递，可正常输出a = 10def test(a): a = a + 1 print(a)test(a) 11 数据结构字符串使用&#39;或&quot;来创建字符串，只要为变量分配一个值即可。 python不支持单个字符类型，访问子字符串，使用方括号截取字符串。 12345var1 = 'Hello World!'var2 = "Runoob" print ("var1[0]: ", var1[0])print ("var2[1:5]: ", var2[1:5]) var1[0]: H var2[1:5]: unoo 转义字符需要在字符串中使用特殊字符时，用反斜杠（\）转义字符 字符串运算符 操作符 描述 实例 + 字符串连接 a+b输出 hello py * 重复输出字符串 a*2输出结果hellohello [] 通过索引获取字符串中字符 a[1]输出结果为e [:] 截取字符串一部分，遵循左闭右开原则，str[0:2]不包含第3个字符 a[1:4]输出结果为ell in 成员运算符,包含则输出true ‘H’in a输出true % 格式字符串 将一个值插入一个有字符串格式符%的字符串中 字符串格式化在 Python 中，字符串格式化使用与 C 中 sprintf 函数一样的语法。 1print ("我叫 %s 今年 %d 岁!" % ('小明', 10)) 我叫 小明 今年 10 岁! 符 号 描述 %c 格式化字符及其ASCII码 %s 格式化字符串 %d 格式化整数 %u 格式化无符号整型 %o 格式化无符号八进制数 %x 格式化无符号十六进制数 %X 格式化无符号十六进制数（大写） %f 格式化浮点数字，可指定小数点后的精度 %e 用科学计数法格式化浮点数 %E 作用同%e，用科学计数法格式化浮点数 %g %f和%e的简写 %G %f 和 %E 的简写 %p 用十六进制数格式化变量的地址 Python2.6 开始，新增了一种格式化字符串的函数 str.format()，它增强了字符串格式化的功能。 基本语法是通过 {} 和 :来代替以前的 %。 format 函数可以接受不限个参数，位置可以不按顺序。 格式化操作辅助指令： 符号 功能 * 定义宽度或者小数点精度 - 用做左对齐 + 在正数前面显示加号( + ) &lt;sp&gt; 在正数前面显示空格 # 在八进制数前面显示零(‘0’)，在十六进制前面显示’0x’或者’0X’(取决于用的是’x’还是’X’) 0 显示的数字前面填充’0’而不是默认的空格 % ‘%%’输出一个单一的’%’ (var) 映射变量(字典参数) m.n. m 是显示的最小总宽度,n 是小数点后的位数(如果可用的话) 1"&#123;&#125; &#123;&#125;".format("hello", "world") # 不设置指定位置，按默认顺序 &apos;hello world&apos; 1"&#123;0&#125; &#123;1&#125;".format("hello", "world") # 设置指定位置 &apos;hello world&apos; 1"&#123;1&#125; &#123;0&#125; &#123;1&#125;".format("hello", "world") # 设置指定位置 &apos;world hello world&apos; 12345678910#设置参数print("网站名：&#123;name&#125;, 地址 &#123;url&#125;".format(name="菜鸟教程", url="www.runoob.com"))# 通过字典设置参数site = &#123;"name": "菜鸟教程", "url": "www.runoob.com"&#125;print("网站名：&#123;name&#125;, 地址 &#123;url&#125;".format(**site)) # 通过列表索引设置参数my_list = ['菜鸟教程', 'www.runoob.com']print("网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;".format(my_list)) # "0" 是必须的 网站名：菜鸟教程, 地址 www.runoob.com 网站名：菜鸟教程, 地址 www.runoob.com 网站名：菜鸟教程, 地址 www.runoob.com 1234567#可以向str.format()传入对象class AssignValue(object): def __init__(self, value): self.value = valuemy_value = AssignValue(6)print('value 为: &#123;0.value&#125;'.format(my_value)) # "0" 是可选的 value 为: 6 三引号python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 一个典型的用例是，当你需要一块HTML或者SQL时，这时用字符串组合，特殊字符串转义将会非常的繁琐。 123456para_str = """这是一个多行字符串的实例多行字符串可以使用制表符TAB ( \t )。也可以使用换行符 [ \n ]。"""print (para_str) 这是一个多行字符串的实例 多行字符串可以使用制表符 TAB ( )。 也可以使用换行符 [ ]。 ​ Unicode字符串在Python2中，普通字符串是以8位ASCII码进行存储的，而Unicode字符串则存储为16位unicode字符串，这样能够表示更多的字符集。使用的语法是在字符串前面加上前缀 u。 在Python3中，所有的字符串都是Unicode字符串。 字符串内建函数序号 方法及描述1capitalize()将字符串的第一个字符转换为大写 2center(width, fillchar) 返回一个指定的宽度 width 居中的字符串，fillchar 为填充的字符，默认为空格。3count(str, beg= 0,end=len(string)) 返回 str 在 string 里面出现的次数，如果 beg 或者 end 指定则返回指定范围内 str 出现的次数4bytes.decode(encoding=”utf-8”, errors=”strict”) Python3 中没有 decode 方法，但我们可以使用 bytes 对象的 decode() 方法来解码给定的 bytes 对象，这个 bytes 对象可以由 str.encode() 来编码返回。5encode(encoding=’UTF-8’,errors=’strict’) 以 encoding 指定的编码格式编码字符串，如果出错默认报一个ValueError 的异常，除非 errors 指定的是’ignore’或者’replace’6endswith(suffix, beg=0, end=len(string))检查字符串是否以 obj 结束，如果beg 或者 end 指定则检查指定的范围内是否以 obj 结束，如果是，返回 True,否则返回 False. 7expandtabs(tabsize=8) 把字符串 string 中的 tab 符号转为空格，tab 符号默认的空格数是 8 。8find(str, beg=0, end=len(string)) 检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含返回开始的索引值，否则返回-19index(str, beg=0, end=len(string)) 跟find()方法一样，只不过如果str不在字符串中会报一个异常.10isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返 回 True,否则返回 False11isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True, 否则返回 False12isdigit() 如果字符串只包含数字则返回 True 否则返回 False..13islower() 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False14isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False15isspace() 如果字符串中只包含空白，则返回 True，否则返回 False.16istitle() 如果字符串是标题化的(见 title())则返回 True，否则返回 False17isupper() 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True，否则返回 False18join(seq) 以指定字符串作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串19len(string) 返回字符串长度20ljust(width[, fillchar]) 返回一个原字符串左对齐,并使用 fillchar 填充至长度 width 的新字符串，fillchar 默认为空格。21lower() 转换字符串中所有大写字符为小写.22lstrip() 截掉字符串左边的空格或指定字符。23maketrans() 创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。24max(str) 返回字符串 str 中最大的字母。25min(str) 返回字符串 str 中最小的字母。26replace(old, new [, max]) 把 将字符串中的 str1 替换成 str2,如果 max 指定，则替换不超过 max 次。27rfind(str, beg=0,end=len(string)) 类似于 find()函数，不过是从右边开始查找.28rindex( str, beg=0, end=len(string)) 类似于 index()，不过是从右边开始.29rjust(width,[, fillchar]) 返回一个原字符串右对齐,并使用fillchar(默认空格）填充至长度 width 的新字符串30rstrip() 删除字符串字符串末尾的空格.31split(str=””, num=string.count(str)) num=string.count(str)) 以 str 为分隔符截取字符串，如果 num 有指定值，则仅截取 num+1 个子字符串32splitlines([keepends]) 按照行(‘\r’, ‘\r\n’, \n’)分隔，返回一个包含各行作为元素的列表，如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。33startswith(substr, beg=0,end=len(string)) 检查字符串是否是以指定子字符串 substr 开头，是则返回 True，否则返回 False。如果beg 和 end 指定值，则在指定范围内检查。34strip([chars]) 在字符串上执行 lstrip()和 rstrip()35swapcase() 将字符串中大写转换为小写，小写转换为大写36title() 返回”标题化”的字符串,就是说所有单词都是以大写开始，其余字母均为小写(见 istitle())37translate(table, deletechars=””) 根据 str 给出的表(包含 256 个字符)转换 string 的字符, 要过滤掉的字符放到 deletechars 参数中38upper() 转换字符串中的小写字母为大写39zfill (width) 返回长度为 width 的字符串，原字符串右对齐，前面填充040isdecimal() 检查字符串是否只包含十进制字符，如果是返回 true，否则返回 false。 列表列表可以修改，则字符串和元组不能。 创建一个列表：用逗号分隔不同的数据项，使用方括号括起来。 list1=[1,2,3,4,5] list2=[&#39;google&#39;,&#39;baidu&#39;] 列表索引从0开始，列表可以进行截取、组合等。 访问列表的值： 使用下标索引来访问列表的值，可以用使用方括号截取字符。 更新列表： 可以使用append()方法来添加列表项。 删除列表元素： 使用del语句来删除列表的元素。还有remove()函数方法 123456#访问列表的值list1 = ['Google', 'Runoob', 1997, 2000];list2 = [1, 2, 3, 4, 5, 6, 7 ]; print ("list1[0]: ", list1[0])print ("list2[1:5]: ", list2[1:5]) #列表截取和拼接 list1[0]: Google list2[1:5]: [2, 3, 4, 5] 123456#更新列表list = ['Google', 'Runoob', 1997, 2000] print ("第三个元素为 : ", list[2])list[2] = 2001 print ("更新后的第三个元素为 : ", list[2]) 第三个元素为 : 1997 更新后的第三个元素为 : 2001 123456#删除列表元素list = ['Google', 'Runoob', 1997, 2000] print ("原始列表 : ", list)del list[2]print ("删除第三个元素 : ", list) 原始列表 : [&apos;Google&apos;, &apos;Runoob&apos;, 1997, 2000] 删除第三个元素 : [&apos;Google&apos;, &apos;Runoob&apos;, 2000] 列表脚本操作符 +用于组合列表,拼接，*用于重复列表 Python表达式 结果 描述 len([1, 2, 3]) 3 长度 [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] 组合 [‘Hi!’] * 4 [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’] 重复 3 in [1, 2, 3] True 元素是否存在于列表中 for x in [1, 2, 3]: print(x, end=” “) 1 2 3 迭代 方法 描述 len(list) 列表元素个数 max(list) 返回列表最大值 min(list) 最小值 list(seq 将元组转换为列表 list.append(x) 把一个元素添加到列表的结尾，相当于 a[len(a):] = [x]。 list.extend(L) 通过添加指定列表的所有元素来扩充列表，相当于 a[len(a):] = L。 list.insert(i, x) 在指定位置插入一个元素。第一个参数是准备插入到其前面的那个元素的索引，例如 a.insert(0, x) 会插入到整个列表之前，而 a.insert(len(a), x) 相当于 a.append(x) 。 list.remove(x) 删除列表中值为 x 的第一个元素。如果没有这样的元素，就会返回一个错误。 list.pop([i]) 从列表的指定位置移除元素，并将其返回。如果没有指定索引，a.pop()返回最后一个元素。元素随即从列表中被移除。（方法中 i 两边的方括号表示这个参数是可选的，而不是要求你输入一对方括号，你会经常在 Python 库参考手册中遇到这样的标记。） list.clear() 移除列表中的所有项，等于del a[:]。 list.index(x) 返回列表中第一个值为 x 的元素的索引。如果没有匹配的元素就会返回一个错误。 list.count(x) 返回 x 在列表中出现的次数。 list.sort() 对列表中的元素进行排序。 list.reverse() 倒排列表中的元素。 list.copy() 返回列表的浅复制，等于a[:]。 1234567891011121314a = [66.25, 333, 333, 1, 1234.5]print(a.count(333), a.count(66.25), a.count('x'))a.append(333)print(a)a.remove(333)print(a)a.reverse()print(a)a.index(333)print(a) 2 1 0 [66.25, 333, 333, 1, 1234.5, 333] [66.25, 333, 1, 1234.5, 333] [333, 1234.5, 1, 333, 66.25] [333, 1234.5, 1, 333, 66.25] 将列表当做堆栈使用 堆栈 特定的数据结构，最先进入的元素最后一个被释放（后进先出）。 用append()方法可以把一个元素添加到堆栈项。 用不指定索引的pop()方法可以把一个元素从堆栈顶释放出来。 12345stack = [3,4,5]stack.append(6)stack.append(7)print(stack)stack.pop() [3, 4, 5, 6, 7] 7 将列表当作队列使用在队列中加入的第一个元素，第一个被取出来；列表做这样的目的效率不高。 在列表的最后添加或弹出元素速度快，但是在头部插入或弹出速度不快。（因为所有元素需要一个个地移动） 列表推导式提供了从序列创建列表的简单途径。应用程序将一些操作应用于某个序列的每个元素，用获得的元素结果作为生成新列表的元素，或者根据确定的判定条件创建子序列。 每个列表推导式都在 for 之后跟一个表达式，然后零到多个 for 或 if 子句。 返回结果是一个根据表达从其后的for和if上下文环境中生成出来的列表。 如果希望表示推导出一个元组，必须使用括号。 12vec =[2,4,6][3*x for x in vec] [6, 12, 18] 12vec = [2,4,6][[x,x**2] for x in vec] [[2, 4], [4, 16], [6, 36]] 12345678#对序列里的每一个元素逐个调用某方法freshfruit = ['banana','loganberry','passion fruit'][weapon.strip() for weapon in freshfruit]#Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。#注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。 [&apos;banana&apos;, &apos;loganberry&apos;, &apos;passion fruit&apos;] 123#用if子句作为过滤器[3*x for x in vec if x &gt;3] [12, 18] 1[3*x for x in vec if x &lt;2] [] 123456#关于循环和其它技巧vec1 = [2,4,6]vec2 = [4,3,-9][x*y for x in vec1 for y in vec2] [8, 6, -18, 16, 12, -36, 24, 18, -54] 123vec1 = [2,4,6]vec2 = [4,3,-9][x+y for x in vec1 for y in vec2] [6, 5, -7, 8, 7, -5, 10, 9, -3] 123vec1 = [2,4,6]vec2 = [4,3,-9][ vec1[i]*vec2[i] for i in range(len(vec1)) ] [8, 12, -54] 列表推导式可以使用复杂表达式或嵌套函数 12[str(round(355/113,i)) for i in range(1,6) ]#round()返回浮点数四舍五入的值，i表示几位小数 [&apos;3.1&apos;, &apos;3.14&apos;, &apos;3.142&apos;, &apos;3.1416&apos;, &apos;3.14159&apos;] del语句使用del语句可以从一个列表中依索引而不是值来删除一个元素，与使用pop()返回一个值不同。 可以使用del语句从列表中删除一个切割，或清空整个列表。 元组和序列元组由若干逗号分隔的值组成。 元组在输出时总是有括号的，以便于正确的表达嵌套结构。 12t = 12345,54312,'hello!'print(t) (12345, 54312, &apos;hello!&apos;) 12t = 12345,54312,'hello!'print(t[0]) 12345 123t = 12345,54312,'hello!'u = t,(1,2,3,4,6)print(u) ((12345, 54312, &apos;hello!&apos;), (1, 2, 3, 4, 6)) 集合集合是一个无序不重复元素的集，基本功能包括关系测试和消除重复元素。 可以用（{}）创建集合。 如果创建空集，必须使用set()而不是{}。相当于创建一个空的字典。 12basket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;print(basket) # 删除重复的 {&apos;banana&apos;, &apos;apple&apos;, &apos;orange&apos;, &apos;pear&apos;} 12basket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;'orange' in basket #检测成员 True 1234567891011121314#两个集合的操作a = set('abracadabra')b = set('alacazam')print(a) #唯一字母print(a-b) #在集合a而不在集合b中print(a|b) #在a或b中print(a &amp; b) #在a和b中都有的字母print(a^b) #在a或b中，但不同时在a和b中 {&apos;a&apos;, &apos;b&apos;, &apos;d&apos;, &apos;c&apos;, &apos;r&apos;} {&apos;b&apos;, &apos;d&apos;, &apos;r&apos;} {&apos;l&apos;, &apos;r&apos;, &apos;a&apos;, &apos;m&apos;, &apos;b&apos;, &apos;z&apos;, &apos;d&apos;, &apos;c&apos;} {&apos;a&apos;, &apos;c&apos;} {&apos;l&apos;, &apos;d&apos;, &apos;m&apos;, &apos;b&apos;, &apos;z&apos;, &apos;r&apos;} 1234#集合也支持推导式a= &#123;x for x in 'abracadabra' if x not in 'abc'&#125;print (a) {&apos;d&apos;, &apos;r&apos;} 字典序列是以连续的整数为索引，与此不同，字典以关键字为索引，关键字可以是任意不可变类型，通常用字符串或数值。 字典可以看成 无序的键=》值对集合。 在一个字典中，关键字必须互不相同。 一对大括号创建一个空的字典：{}。 1234tel = &#123;'jack': 4098, 'sape': 4139&#125;tel['guido'] = 4127print(tel)tel['jack'] {&apos;sape&apos;: 4139, &apos;guido&apos;: 4127, &apos;jack&apos;: 4098} 4098 1234567891011tel = &#123;'jack': 4098, 'sape': 4139&#125;del tel['sape']tel['irv'] = 4127print(tel)list(tel.keys()) 'irv' in tel {&apos;irv&apos;: 4127, &apos;jack&apos;: 4098} True 构造 dict()直接从键值对元组列表中构建字典。 如果有固定的模式，列表推导式指定特定的键值对 1dict([('sape', 4139), ('guido', 4127), ('jack', 4098)]) {&apos;guido&apos;: 4127, &apos;jack&apos;: 4098, &apos;sape&apos;: 4139} 12# 字典可以创建任意键和值的表达式词典&#123;x: x**2 for x in (2, 4, 6)&#125; {2: 4, 4: 16, 6: 36} 遍历技巧在字典中遍历时，关键字和对应的值可以使用items() 方法同时解读出来： 123knights = &#123;'gallahad': 'the pure', 'robin': 'the brave'&#125;for k, v in knights.items(): print(k, v) gallahad the pure robin the brave 在序列中遍历时，索引位置和对应值可以使用 enumerate() 函数同时得到： 12for i, v in enumerate(['tic', 'tac', 'toe']): print(i, v) 0 tic 1 tac 2 toe 同时遍历两个或更多的序列，可以使用 zip() 组合： 1234questions = ['name', 'quest', 'favorite color']answers = ['lancelot', 'the holy grail', 'blue']for q, a in zip(questions, answers): print('What is your &#123;0&#125;? It is &#123;1&#125;.'.format(q, a)) What is your name? It is lancelot. What is your quest? It is the holy grail. What is your favorite color? It is blue. 要反向遍历一个序列，首先指定这个序列，然后调用 reversed() 函数： 12for i in reversed(range(1, 10, 2)): print(i) 9 7 5 3 1 要按顺序遍历一个序列，使用 sorted()函数返回一个已排序的序列，并不修改原值： 123basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']for f in sorted(set(basket)): print(f) apple banana orange pear 模块是一个包含所有定义的函数和变量的文件，后缀名.py。可以被别的程序引入，使用。也是使用python标注库的方法。 python本身带有一些标准的模块库。 import语句使用python源文件，需要在另一个源文件执行import语句。语法如下： import module1[,module2[,...moduleN] 当解释器遇到import语句时，如果模块在当前搜索路径就会被导入。 搜索路径是一个解释器会先进行搜索的所有目录的列表。如想要导入模块 support，需要把命令放在脚本的顶端： support.py文件代码： filename: support.py def print_func(par) print(“hello:”,par) return test.py引入support模块： import support 导入模块 support.print_func(“ruboot”) 调用模块 输出结果： hello : ruboot 一个模块只会被导入一次，不管执行多少次import操作。 from-import语句from语句把从模块中导入一个指定的部分到当前命名空间，语法如下： from modname import name1[,...nameN]] 1234567891011121314151617def fib(n): # 定义到 n 的斐波那契数列 a, b = 0, 1 while b &lt; n: print(b, end=' ') a, b = b, a+b print() def fib2(n): # 返回到 n 的斐波那契数列 result = [] a, b = 0, 1 while b &lt; n: result.append(b) a, b = b, a+b return result#from fibo import fib,fib2 print(fib(500)) 1 1 2 3 5 8 13 21 34 55 89 144 233 377 None from modname import *把一个模块的所有内容全部导入当前的命名空间内。 name属性当一个模块被另一个程序第一次引入时，其主程序将运行。 如果想在模块被引入时，模块中的某一程序块不执行，可以使用__name__属性来使得该程序块仅在该模块自身运行时执行。 每一个模块都有一个__name__属性，当其值为__main__时，表明该模块自身在运行，否则是被引入。（注意：是双下划线） dir()函数内置的函数dir()可以找到模块内定义的所有名称。以一个字符串列表的形式返回。 包包是一个管理python模块命名空间的形式，采用“点模块名称”。 如一个包的名称是A.B，表示一个包A中的子模块B。 在导入一个包的时候，Python 会根据 sys.path 中的目录来寻找这个包中包含的子目录。 目录只有包含一个叫做__init__.py的文件才会被认作是一个包。为了避免影响到搜索路径中的有效模块。 from sound.effects import *会发生什么？ Python 会进入文件系统，找到这个包里面所有的子模块，一个一个的把它们都导入进来。 Windows平台上工作的就不是非常好，因为Windows是一个大小写不区分的系统。 需要包作者提供一个精确的包的索引。 导入语句遵循如下规则：如果包定义文件 __init__.py 存在一个叫做__all__的列表变量，那么在使用from package import *的时候就把这个列表中的所有名字作为包内容导入。 在更新包之后，需要保证__all__也更新，如果没有更新，如在在:file:sounds/effects/__init__.py中包含如下代码: __all__ = [&quot;echo&quot;, &quot;surround&quot;, &quot;reverse&quot;] 这表示当你使用from sound.effects import *这种用法时，你只会导入包里面这三个子模块。 如果__all__没有定义，则使用from sound.effects import *这种用法时不会导入包sound.effects里的任何子模块。只是把包和其里面定义的所有内容导入进来。 记住，使用from Package import specific_submodule这种方法永远不会有错。事实上，这也是推荐的方法。除非是你要导入的子模块有可能和其他包的子模块重名。 如果在结构中包是一个子包（比如这个例子中对于包sound来说），而你又想导入兄弟包（同级别的包）你就得使用导入绝对的路径来导入。 比如，如果模块sound.filters.vocoder要使用包sound.effects中的模块echo，你就要写成 from sound.effects import echo。 无论是隐式的还是显式的相对导入都是从当前模块开始的。主模块的名字永远是__main__，一个Python应用程序的主模块，应当总是使用绝对路径引用。 包还提供一个额外的属性__path__。这是一个目录列表，里面每一个包含的目录都有为这个包服务的__init__.py，你得在其他__init__.py被执行前定义哦。可以修改这个变量，用来影响包含在包里面的模块和子包。 输入和输出python输出值的方式中有两种是： 表达式语句和print()语句。 第三种是：使用文件对象的write()方法，标准输出文件可以用sys.stdout引用。 可以使用str.format()函数来格式化输出值，进而输出多样的形式。 可以使用repr()或str()函数来实现将输出的值转成字符串。 str()： 函数返回一个用户易读的表达形式。 repr()： 产生一个解释器易读的表达形式。 12s = 'hello,runb'str(s) &apos;hello,runb&apos; 12s = 'hello,runb'repr(s) &quot;&apos;hello,runb&apos;&quot; 1234x = 10*3.25y = 200*200s = 'x的值为：' + repr(x) + ', y的值为：' + repr(y)+ '...'print(s) x的值为：32.5, y的值为：40000... 有两种方式输出平方和立方的表： 1234for x in range(1,11): print(repr(x).rjust(2), repr(x*x).rjust(3),end=' ') #end的使用 print(repr(x*x*x).rjust(4)) 1 1 1 2 4 8 3 9 27 4 16 64 5 25 125 6 36 216 7 49 343 8 64 512 9 81 729 10 100 1000 每列间的空格由print()添加。 rjust()将字符串靠右，并在左边填充空格。类似的有ljust()和center()。这方仅仅返回新的字符串。 还有一个zfill()，会在数字的左边填充0。 12for x in range(1,11): print('&#123;0:2d&#125;&#123;1:3d&#125;&#123;2:4d&#125;'.format(x,x*x,x*x*x)) 1 1 1 2 4 8 3 9 27 4 16 64 5 25 125 6 36 216 7 49 343 8 64 512 9 81 729 101001000 str.format()使用123#基本使用如下print( '&#123;&#125;网址："&#123;&#125;!"'.format('菜鸟教程','www.runbob.com') ) 菜鸟教程网址：&quot;www.runbob.com!&quot; 括号及其里面的字符（称为格式化字段）将会被format()中的参数替换。 1print('&#123;0&#125; 和&#123;1&#125;'.format('google','runboo') ) google 和runboo 1print('&#123;1&#125; 和&#123;0&#125;'.format('google','runboo') ) runboo 和google 括号内的数字用于指向传入对象在format()中的位置。 1print('&#123;name&#125;网址： &#123;site&#125;'.format(name='菜鸟教程', site='www.runoob.com')) 菜鸟教程网址： www.runoob.com 使用关键参数，值会指向使用该名字的参数 1print('站点列表 &#123;0&#125;, &#123;1&#125;, 和 &#123;other&#125;。'.format('Google', 'Runoob', other='Taobao')) 站点列表 Google, Runoob, 和 Taobao。 1234567import mathprint('常量PI的值近似为：&#123;&#125;。'.format(math.pi))print('常量 PI 的值近似为： &#123;!r&#125;。'.format(math.pi))#可选项（：）和格式标识符可以跟着字段名，允许更好的格式化print('常量 PI 的值近似为 &#123;0:.3f&#125;。'.format(math.pi)) 常量PI的值近似为：3.141592653589793。 常量 PI 的值近似为： 3.141592653589793。 常量 PI 的值近似为 3.142。 ‘!a’ (使用 ascii()), ‘!s’ (使用 str()) 和 ‘!r’ (使用 repr()) 可以用于在格式化某个值之前对其进行转化 如果你有一个很长的格式化字符串, 而你不想将它们分开, 那么在格式化时通过变量名而非位置会是很好的事情。 最简单的就是传入一个字典, 然后使用方括号 &#39;[]&#39; 来访问键值 12table = &#123;'Google':1,'Runoob':2,'Taobao':3&#125;print('Runoob: &#123;0[Runoob]:d&#125;; Google: &#123;0[Google]:d&#125;; Taobao: &#123;0[Taobao]:d&#125;'.format(table)) Runoob: 2; Google: 1; Taobao: 3 在 : 后传入一个整数, 可以保证该域至少有这么多的宽度。 用于美化表格 也可以通过在 table 变量前使用 ** 来实现相同的功能 12table = &#123;'Google':1,'Runoob':2,'Taobao':3&#125;print('Runoob: &#123;Runoob:d&#125;; Google: &#123;Google:d&#125;; Taobao: &#123;Taobao:d&#125;'.format(**table)) Runoob: 2; Google: 1; Taobao: 3 旧式字符串格式化:使用%操作符将坐标的参数类似printf()格式化字符串，将右边的代入。 12import mathprint('常量PI的值近似为：%5.3f。' %math.pi) 常量PI的值近似为：3.142。 读取键盘的输入Python提供了input()内置函数从标准输入读入一行文本，默认的标准输入是键盘。 input 可以接收一个Python表达式作为输入，并将运算结果返回。 str = input(&quot;请输入：&quot;) print(&quot;你输入的内容是：&quot;,str) 读和写文件open()将会返回一个file对象，基本语法格式如下： open(filename,mode) filename:包含要访问的文件名称和字符串值。 mode:决定打开文件的模式：只读、写入、追加等。非强制性参数，默认文件访问模式为只读（r）。 模 式 描述 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 将字符串写入放在桌面的文件foo.txt中： f = open(&quot;foo.txt&quot;,&quot;w&quot;) 打开一个文件，注意路径 第一个参数表示要打开文件名；第二个参数描述文件如何使用。mode可以是r只读；w只用于写（如果存在同名文件则将被删除），a用于追加文件内容，所写的任何数据会被自动添加到末尾；r+同时用于读写。mode的参数是可选的，默认是r只读。 f.write( &quot;Python 是一个非常好的语言。\n是的，的确非常好!!\n&quot; ) f.close() 关闭文件 文件对象的方法假设创建的文件对象为f。 f.read()读取一个文件的内容，调用f.read(size),读取一定数目的数据，然后作为字符串或字节对象返回。 size是一个可选的数字类型的参数。当size被忽略或为负，则该文件的所有内容都将被读取并且返回。 f.readline()会从文件中读取单独一行，换行符为\n。 f.readline()如果返回一个空字符，说明已经读取到最后一行。 f.readlines()将返回该文件中包含的所有行。如果设置可选参数sizehint，则读取指定长度的字节，并且将这些字节按行分割。 另一种方式是迭代一个文件对象然后读取每行。 for line in f: `print(line, end=&apos; &apos;)` f.write()f.write(string)将string写入到文件中，然后返回写入的字符数。如： f = open(&quot;foo.txt&quot;,&quot;w&quot;) num = f.write(&quot;Python 是一个非常好的语言。\n是的，的确非常好!!\n&quot;) print(num) f.close() 输出结果为：29 如果写入一些不是字符的东西，需要先进行转换： f = open(&quot;foo.txt&quot;,&quot;w&quot;) value = (&#39;www.runboo.com&#39;,14) s = str(value) f.write(s) f.close() f.tell()返回文件对象当前所处的位置，它是从文件开头开始算起字节数。 f.seek()如果要改变文件当前位置，可以用f.seek(offset,from_what)函数。 from_what的值，如果是0表示开头，如果是1表示当前位置，2表示文件的结尾。默认值为1，即文件开头。如： seek(x,0) ： 从起始位置即文件首行首字符开始移动 x 个字符 seek(x,1) ： 表示从当前位置往后移动x个字符 seek(-x,2)：表示从文件的结尾往前移动x个字符 f.close()在文本文件中（打开文件模式没有b的），只会相对于文件起始位置进行定位。 当处理完文件后，调用f.close()来关闭文件并释放系统的资源，如果尝试再调用该文件，则会抛出异常。 当处理一个文件对象时，使用with关键字要比try-finally语句要简短： with open(&#39;foo.txt&#39;,&#39;r&#39;) as f: `read_data = f.read()` f.close() pickle模块python的pickle模块实现了基本的数据序列的反序列化。 通过pickle模块的序列化操作能够将程序中运行的对象信息保存到文件中，永久存储。 通过pickle模块的反序列化操作，能够从文件中创建上一次程序保存的对象。 基本接口： pickle.dump(obj,file,[,protocol]) 然后对file以读取的形式打开： x = pickle.load(file) 其中file是类文件对象，有read()和readline()接口。 实例1： import pickle #使用pickle模块将数据对象保存到文件 data1 = {&#39;a&#39;:[1,2.0,3,4+6j], &#39;b&#39;:(&#39;string&#39;, u&#39;Unicode string&#39;), &#39;c&#39;: None} selfref_list=[1,2,3] selfref_list.append(selfref_list) output = open(&#39;data.txt&#39;,&#39;wb&#39;)二进制写入文件 #pickle dictionary using protocol 0. pickle.dump(data1,output) #pickle the list using highest protocol available pickle.dump(selfref_list,output,-1) output.close() 实例2： import pprint, pickle #使用pickle模块从文件中重构python对象 pkl_file = open(&#39;data.pkl&#39;, &#39;rb&#39;) data1 = pickle.load(pkl_file) pprint.pprint(data1) data2 = pickle.load(pkl_file) pprint.pprint(data2) pkl_file.close() 文件(file)方法open()open()方法用于打开一个文件，并返回文件对象，在对文件进行处理过程都需要使用这个函数，如果文件无法打开，抛出 OSError。 使用open()一定要保证关闭文件对象，即调用close()方法 。 open()函数常用形式是接收两个参数：文件名（file）和模式（mode）。即： open(file,mode = &#39;r&#39;) 完整格式为： open(file, mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 参数说明： ile: 必需，文件路径（相对或者绝对路径）。 mode: 可选，文件打开模式 buffering: 设置缓冲 encoding: 一般使用utf8 errors: 报错级别 newline: 区分换行符 closefd: 传入的file参数类型 opener: mode参数有： 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（不推荐）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 默认方式是文本模式，如果要以二进制打开，加上b file对象file对象使用open()函数创建，常用file函数有： 序号 方法及描述 file.close() 关闭文件。关闭后文件不能再进行读写操作。 file.flush() 刷新文件内部缓冲，直接把内部缓冲区的数据立刻写入文件, 而不是被动的等待输出缓冲区写入。 file.fileno() 返回一个整型的文件描述符(file descriptor FD 整型), 可以用在如os模块的read方法等一些底层操作上。 file.isatty() 如果文件连接到一个终端设备返回 True，否则返回 False。 file.next() Python 3 中的 File 对象不支持 next() 方法。返回文件下一行。 file.read([size]) 从文件读取指定的字节数，如果未给定或为负则读取所有。 file.readline([size]) 读取整行，包括 “\n” 字符。 file.readlines([sizeint]) 读取所有行并返回列表，若给定sizeint&gt;0，返回总和大约为sizeint字节的行, 实际读取值可能比 sizeint 较大, 因为需要填充缓冲区。 file.seek(offset[, whence]) 设置文件当前位置 file.tell() 返回文件当前位置。 file.truncate([size]) 从文件的首行首字符开始截断，截断文件为 size 个字符，无 size 表示从当前位置截断；截断之后后面的所有字符被删除，其中 Widnows 系统下的换行代表2个字符大小。 file.write(str) 将字符串写入文件，返回的是写入的字符长度。 file.writelines(sequence) 向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符。 OS 文件/目录os 模块提供了非常丰富的方法用来处理文件和目录。如： os.chdir(path)改变当前工作目录。 os.access(path,mode)检验权限模式 os.chmod(path,mode)更改权限 等等。 语法错误和异常处理语法错误称为解析错误。 SyntaxError : invalid syntax 异常即便语法正确，但运行时，也可能发生错误。运行时检测到的错误称为异常。 异常以不同的类型出现，这些类型都作为信息的一部分打印出来: 例子中的类型有 ZeroDivisionError，NameError 和 TypeError。 错误信息的前面部分显示了异常发生的上下文，并以调用栈的形式显示具体信息。 异常-try-except语句while True: try: `x = int(input(&quot;Please enter a number: &quot;))` `break` except ValueError: `print(&quot;Oops! That was no valid number. Try again &quot;)` try语句工作方式： 首先执行try子句（try和except之间语句） 如果没有异常，忽略except子句，try子句执行结束。 如执行try子句有异常，try子句剩余部分忽略，如果异常的类型和except之后的名称相符，对应的except子句将被执行。最后执行try语句之后的代码。 如果异常没有与任何except匹配，那么这个异常会被传递给上层的try中。 一个 try 语句可能包含多个except子句，分别来处理不同的特定的异常。最多只有一个分支会被执行。 处理程序将只针对对应的try子句中的异常进行处理，而不是其他的 try 的处理程序中的异常。 一个except子句可以同时处理多个异常，这些异常将被放在一个括号里成为一个元组。例如： except (RuntimeError, TypeError, NameError): pass try except 语句还有一个可选的else子句，如果使用这个子句，那么必须放在所有的except子句之后。这个子句将在try子句没有发生任何异常的时候执行。 for arg in sys.argv[1:]: try: f = open(arg, &apos;r&apos;) except IOError: print(&apos;cannot open&apos;, arg) else: print(arg, &apos;has&apos;, len(f.readlines()), &apos;lines&apos;) f.close() 抛出异常-raise语句python使用raise语句抛出一个指定的异常。例如： raise NameError(&#39;HiThere&#39;) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in ? NameError: HiThere raise唯一的一个参数指定了要抛出的异常。它必须是一个异常的实例或者是异常的类（即except的子类）。 定义清理行为-try-finally语句try-finally定义了无论在任何情况下都会执行的清理行为。 无论try子句里面有没有发生异常，finally子句都会执行。 如果一个异常在try子句中（或except或else子句里）被抛出，而没有任何的except把它截住，那么这个异常在finally子句执行后被抛出。 12345678910def divide(x,y): try: result = x/y except ZeroDivisionError: print("division by zero!") else: print("result is",result) finally: print("executing finally clause")divide(2,1) result is 2.0 executing finally clause 12345678910def divide(x,y): try: result = x/y except ZeroDivisionError: print("division by zero!") else: print("result is",result) finally: print("executing finally clause")divide(2,0) division by zero! executing finally clause 12345678910def divide(x,y): try: result = x/y except ZeroDivisionError: print("division by zero!") else: print("result is",result) finally: print("executing finally clause")divide("2","1") executing finally clause --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-28-e99a7014f207&gt; in &lt;module&gt;() 8 finally: 9 print(&quot;executing finally clause&quot;) ---&gt; 10 divide(&quot;2&quot;,&quot;1&quot;) &lt;ipython-input-28-e99a7014f207&gt; in divide(x, y) 1 def divide(x,y): 2 try: ----&gt; 3 result = x/y 4 except ZeroDivisionError: 5 print(&quot;division by zero!&quot;) TypeError: unsupported operand type(s) for /: &apos;str&apos; and &apos;str&apos; 面向对象面向对象技术简介 类（class）：描述具有相同的属性和方法的对象的集合。定义了集合中每个对象所共有的属性和方法。对象是类的实例。 方法：类中定义的函数。 类变量：在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员：类变量或者实例变量用于处理类及其实例对象的相关的数据。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写。称为覆盖（override），也称为方法重写。 局部变量：定义在方法中的变量，只作用于当前实例中的类。 实例变量：在类的声明中，属性是用变量来表示的，称为实例变量，是在类声明内部但是在类的其它成员方法之外声明。 继承：一个派生类（derived class） 继承 基类（base class）的字段和方法。继承允许把一个派生类的对象作为一个基类对象对待。如一个dog类型的对象派生自animal类。 实例化：创建一个类的实例，类的具体对象。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 python类提供了面向对象编程的所有基本功能：类的继承机制允许多个基类，派生类可以覆盖基类中的任何方法，方法中可以调整基类的同名方法。 对象可以包含任意数量和类型的数据。 类定义语法格式： class Classname: `&lt;statement-1&gt;` `&lt;statement-N&gt;` ​创建一个类之后，可以通过类名访问其属性。 类对象类对象支持两种操作：属性引用和实例化 属性引用使用标准语法：obj.name 类对象创建后。类命名空间中所有命名都是有效属性。 12345678910111213class Myclass: """一个简单的类实例""" i= 12345 def f(): return 'hello world' #实例化x = Myclass()#访问类的属性和方法print("MyClass 类的属性 i为：",x.i)print("MyClass 类的方法 f输出为：",x.f()) MyClass 类的属性 i为： 12345 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-2-29be863acd9b&gt; in &lt;module&gt;() 11 12 print(&quot;MyClass 类的属性 i为：&quot;,x.i) ---&gt; 13 print(&quot;MyClass 类的方法 f输出为：&quot;,x.f()) TypeError: f() takes 0 positional arguments but 1 was given 12345678910111213class Myclass: """一个简单的类实例""" i= 12345 def f(self): #每个与类相关联的方法调用都自动传递实参self，指向实例本身的引用，f()在定义时没有形参，会报错 return 'hello world' #类实例化操作x = Myclass()#访问类的属性和方法print("MyClass 类的属性 i为：",x.i)print("MyClass 类的方法 f输出为：",x.f()) MyClass 类的属性 i为： 12345 MyClass 类的方法 f输出为： hello world 类有一个名为__init__()方法（构造方法），该方法在类实例化时会自动调用，如： def __init__(self): `self.data=[]` 类定义了__init__()方法，类的实例化操作会自动调用__init__()方法。 1234567class Complex: def __init__(self,realpart,imagpart): #__init__可以有参数 self.r = realpart self.i = imagpart x = Complex(3.0,-4.5)print(x.r,x.i) 3.0 -4.5 【注意】： 类的方法和普通函数只有一个特殊的区别——必须有一个额外的第一个参数,按照惯例为self。 12345678class Test: def prt(self): print(self) print(self.__class__) t = Test()t.prt() &lt;__main__.Test object at 0x000002561FC0ADA0&gt; &lt;class &apos;__main__.Test&apos;&gt; self代表的是类的实例，代表当前对象的地址，而self.class则指向类。 self不是python关键字，换成其它的如runb之类的也能正常运行。 类的方法在类的内部，使用def关键字来定义一个方法，与一般函数的定义不同。类方法必须包含第一个参数self，代表的是类的实例。 123456789101112131415161718192021#类的定义class people: #定义基本属性 name = '' age = 0 #定义私有属性，私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说：我 %d 岁。" % (self.name,self.age)) #实例化类p = people('runboo',10,30)p.speak() runboo 说：我 10 岁。 继承派生类的定义如下： class DerivedClassName(BaseClassName): &lt;statement-1&gt; &lt;statement-N&gt; ​【注意】： 需要注意圆括号内的基类的顺序，若是基类有相同的方法名，而在子类使用时未指定，python从左至右搜索，即方法在子类中未找到时，从左到右查找基类中是否包含方法。 BaseClassName(实例中的基类名)必须与派生类定义在一个作用域内。 除了类，还可以用表达式。（这在基类定义在另一个模块中时非常有用。） 123456789101112131415161718192021222324252627282930class people: #定义属性 name = '' age = 0 #定义私有属性，私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说：我 %d 岁。" % (self.name,self.age)) #单继承示例class student(people): grade = '' def __init__(self,n,a,w,g): #调用父类的构造函数 people.__init__(self,n,a,w) self.grade = g #覆写父类的方法 def speak(self): print("%s 说： 我 %d 岁了，我在读 %d 年级" %(self.name,self.age,self.grade)) s = student('ken',10,60,3)s.speak() ken 说： 我 10 岁了，我在读 3 年级 多继承class DerivedClassName(Base1, Base2, Base3): &lt;statement-1&gt; . . . &lt;statement-N&gt; 需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，python从左至右搜索 即方法在子类中未找到时，从左到右查找父类中是否包含方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class people: #定义属性 name = '' age = 0 #定义私有属性，私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说：我 %d 岁。" % (self.name,self.age)) #单继承示例class student(people): grade = '' def __init__(self,n,a,w,g): #调用父类的构造函数 people.__init__(self,n,a,w) self.grade = g #覆写父类的方法 def speak(self): print("%s 说： 我 %d 岁了，我在读 %d 年级" %(self.name,self.age,self.grade)) #另一类，多重继承之前的准备class speaker(): topic = '' name = '' def __init__(self,n,t): self.name = n self.topic = t def speak(self): print("我叫 %s，我是一个演说家，我演讲的主题是 %s"%(self.name,self.topic))#多重继承class sample(speaker,student): a = '' def __init__(self,n,a,w,g,t): student.__init__(self,n,a,w,g) speaker.__init__(self,n,t)test = sample("Tim",25,80,4,"python")test.speak() #方法同名。默认调用的是括号内排在前面的父类方法 我叫 Tim，我是一个演说家，我演讲的主题是 python 方法重写12345678910111213class Parent: # 定义父类 def myMethod(self): print ('调用父类方法') class Child(Parent): # 定义子类 def myMethod(self): print ('调用子类方法') c = Child() # 子类实例c.myMethod() # 子类调用重写方法super(Child,c).myMethod() #用子类对象调用父类已被覆盖的方法#super()函数用于调用父类（超类）的一个方法 调用子类方法 调用父类方法 子类继承父类的构造函数说明如果在子类中需要父类的构造方法需要显示地调用父类的构造方法，或者不重写父类的构造方法。 子类不重写__init__()，实例化子类时，会自动调用父类定义的__init__()。 1234567891011121314class Father(object): def __init__(self, name): self.name=name print ( "name: %s" %( self.name) ) def getName(self): return 'Father ' + self.name class Son(Father): def getName(self): return 'Son '+self.name if __name__=='__main__': son=Son('runoob') print ( son.getName() ) name: runoob Son runoob 如果重写了__init__函数，实例化子类时，就不会调用父类已经定义的__init__。 1234567891011121314151617class Father(object): def __init__(self, name): self.name=name print ( "name: %s" %( self.name) ) def getName(self): return 'Father ' + self.name class Son(Father): def __init__(self, name): print ( "hi" ) self.name = name def getName(self): return 'Son '+self.name if __name__=='__main__': son=Son('runoob') print ( son.getName() ) hi Son runoob 如果重写__init__时，要继承父类的构造方法，可以使用关键字super。 super(子类,self).__init__(参数1,参数2,...) 另一种经典写法： 父类名称.__init__(self,参数1,参数2,...) 123456789101112131415161718class Father(object): def __init__(self, name): self.name=name print ( "name: %s" %( self.name)) def getName(self): return 'Father ' + self.name class Son(Father): def __init__(self, name): super(Son, self).__init__(name) print ("hi") self.name = name def getName(self): return 'Son '+self.name if __name__=='__main__': son=Son('runoob') print ( son.getName() ) name: runoob hi Son runoob 类属性和方法类的私有属性__private_attrs:两个下划线开头，声明该属性是私有的，不能在类的外部使用或直接访问。在类的内部的方法中使用时self.__private_attrs。 类的方法在类的内部，使用def关键字来定义一个方法，类方法必须包含参数self作为第一个参数。self代表的是类的实例。 一般按照约定用self，也可以用this。 类的私有方法__private_methods:两个下划线开头，声明该方法是私有的，不能在类的外部使用或直接访问。在类的内部的方法中使用时self.__private_mathods。 实例如下： 1234567891011121314class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print (self.__secretCount) counter = JustCounter()counter.count()counter.count()print (counter.publicCount)print (counter.__secretCount) # 报错，实例不能访问私有变量 1 2 2 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-21-eb413b755b77&gt; in &lt;module&gt;() 12 counter.count() 13 print (counter.publicCount) ---&gt; 14 print (counter.__secretCount) # 报错，实例不能访问私有变量 AttributeError: &apos;JustCounter&apos; object has no attribute &apos;__secretCount&apos; 1234567891011121314151617181920class Site: def __init__(self, name, url): self.name = name # public self.__url = url # private def who(self): print('name : ', self.name) print('url : ', self.__url) def __foo(self): # 私有方法 print('这是私有方法') def foo(self): # 公共方法 print('这是公共方法') self.__foo() x = Site('菜鸟教程', 'www.runoob.com')x.who() # 正常输出x.foo() # 正常输出，内部可以调用私有化方法x.__foo() # 报错,外部不能调用私有化方法 name : 菜鸟教程 url : www.runoob.com 这是公共方法 这是私有方法 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-23-9d93b50ee324&gt; in &lt;module&gt;() 18 x.who() # 正常输出 19 x.foo() # 正常输出 ---&gt; 20 x.__foo() # 报错 AttributeError: &apos;Site&apos; object has no attribute &apos;__foo&apos; 类的专有方法 方法 描述 __init__ 构造函数，在生成对象时调用 __del__ 析构函数，释放对象时使用 __repr__ 打印，转换 __setitem__ 按照索引赋值 __getitem__ 按照索引获取值 __len__ 获得长度 __cmp__ 比较运算 __call__ 函数调用 __add__ 加运算 __sub__ 减运算 __mul__ 乘运算 __truediv__ 除运算 __mod__ 求余运算 __pow__ 乘方 运算符重载python支持运算符重载，可以对类的专有方法进行重载。 类重载可以进行加减运算、打印、函数调用、索引等内置运算，运算符重载使得对象的行为与内置对象一样python在调用操作符时会自动调用这样的方法。 例如：如果类实现了__add__方法，当类的对象出现在+运算符中时会调用这个方法。 1234567891011121314class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print (v1 + v2) Vector (7, 8) 1234567891011class computation(): def __init__(self,value): self.value = value def __add__(self,other): return self.value + other def __sub__(self,other): return self.value -other c = computation(5)c + 5c-3 2 标准库操作系统接口OS模块提供了不少与操作系统相关联的函数。 os.getcwd() 返回当前的工作目录 os.chdir()修改当前的工作目录 os.system(&#39;mkdir today&#39;) 执行系统命令mkdir 建议使用import os导入模块。这样可以保证随操作系统不同而有所变化的os.open()不会覆盖内置函数open()。 使用OS这类大型模块的内置的dir()和help()函数非常有用。 针对于日常的文件和目录管理任务:mod:shutil模块提供了一个易于使用的高级接口。 import shutil shutil.copyfile(&#39;data.db&#39;, &#39;archive.db&#39;) shutil.move(&#39;/build/executables&#39;, &#39;installdir&#39;) 123import osdir(os)# returns a list of module functions [&apos;F_OK&apos;, &apos;MutableMapping&apos;, &apos;O_APPEND&apos;, &apos;O_BINARY&apos;, &apos;O_CREAT&apos;, &apos;O_EXCL&apos;, &apos;O_NOINHERIT&apos;, &apos;O_RANDOM&apos;, &apos;O_RDONLY&apos;, &apos;O_RDWR&apos;, &apos;O_SEQUENTIAL&apos;, &apos;O_SHORT_LIVED&apos;, &apos;O_TEMPORARY&apos;, &apos;O_TEXT&apos;, &apos;O_TRUNC&apos;, &apos;O_WRONLY&apos;, &apos;P_DETACH&apos;, &apos;P_NOWAIT&apos;, &apos;P_NOWAITO&apos;, &apos;P_OVERLAY&apos;, &apos;P_WAIT&apos;, &apos;R_OK&apos;, &apos;SEEK_CUR&apos;, &apos;SEEK_END&apos;, &apos;SEEK_SET&apos;, &apos;TMP_MAX&apos;, &apos;W_OK&apos;, &apos;X_OK&apos;, &apos;_DummyDirEntry&apos;, &apos;_Environ&apos;, &apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__spec__&apos;, &apos;_dummy_scandir&apos;, &apos;_execvpe&apos;, &apos;_exists&apos;, &apos;_exit&apos;, &apos;_get_exports_list&apos;, &apos;_putenv&apos;, &apos;_unsetenv&apos;, &apos;_wrap_close&apos;, &apos;abort&apos;, &apos;access&apos;, &apos;altsep&apos;, &apos;chdir&apos;, &apos;chmod&apos;, &apos;close&apos;, &apos;closerange&apos;, &apos;cpu_count&apos;, &apos;curdir&apos;, &apos;defpath&apos;, &apos;device_encoding&apos;, &apos;devnull&apos;, &apos;dup&apos;, &apos;dup2&apos;, &apos;environ&apos;, &apos;errno&apos;, &apos;error&apos;, &apos;execl&apos;, &apos;execle&apos;, &apos;execlp&apos;, &apos;execlpe&apos;, &apos;execv&apos;, &apos;execve&apos;, &apos;execvp&apos;, &apos;execvpe&apos;, &apos;extsep&apos;, &apos;fdopen&apos;, &apos;fsdecode&apos;, &apos;fsencode&apos;, &apos;fstat&apos;, &apos;fsync&apos;, &apos;ftruncate&apos;, &apos;get_exec_path&apos;, &apos;get_handle_inheritable&apos;, &apos;get_inheritable&apos;, &apos;get_terminal_size&apos;, &apos;getcwd&apos;, &apos;getcwdb&apos;, &apos;getenv&apos;, &apos;getlogin&apos;, &apos;getpid&apos;, &apos;getppid&apos;, &apos;isatty&apos;, &apos;kill&apos;, &apos;linesep&apos;, &apos;link&apos;, &apos;listdir&apos;, &apos;lseek&apos;, &apos;lstat&apos;, &apos;makedirs&apos;, &apos;mkdir&apos;, &apos;name&apos;, &apos;open&apos;, &apos;pardir&apos;, &apos;path&apos;, &apos;pathsep&apos;, &apos;pipe&apos;, &apos;popen&apos;, &apos;putenv&apos;, &apos;read&apos;, &apos;readlink&apos;, &apos;remove&apos;, &apos;removedirs&apos;, &apos;rename&apos;, &apos;renames&apos;, &apos;replace&apos;, &apos;rmdir&apos;, &apos;scandir&apos;, &apos;sep&apos;, &apos;set_handle_inheritable&apos;, &apos;set_inheritable&apos;, &apos;spawnl&apos;, &apos;spawnle&apos;, &apos;spawnv&apos;, &apos;spawnve&apos;, &apos;st&apos;, &apos;startfile&apos;, &apos;stat&apos;, &apos;stat_float_times&apos;, &apos;stat_result&apos;, &apos;statvfs_result&apos;, &apos;strerror&apos;, &apos;supports_bytes_environ&apos;, &apos;supports_dir_fd&apos;, &apos;supports_effective_ids&apos;, &apos;supports_fd&apos;, &apos;supports_follow_symlinks&apos;, &apos;symlink&apos;, &apos;sys&apos;, &apos;system&apos;, &apos;terminal_size&apos;, &apos;times&apos;, &apos;times_result&apos;, &apos;truncate&apos;, &apos;umask&apos;, &apos;uname_result&apos;, &apos;unlink&apos;, &apos;urandom&apos;, &apos;utime&apos;, &apos;waitpid&apos;, &apos;walk&apos;, &apos;write&apos;] 123import oshelp(os)# returns an extensive manual page created from the module's docstring Help on module os: NAME os - OS routines for NT or Posix depending on what system we&apos;re on. DESCRIPTION This exports: - all functions from posix, nt or ce, e.g. unlink, stat, etc. - os.path is either posixpath or ntpath - os.name is either &apos;posix&apos;, &apos;nt&apos; or &apos;ce&apos;. - os.curdir is a string representing the current directory (&apos;.&apos; or &apos;:&apos;) - os.pardir is a string representing the parent directory (&apos;..&apos; or &apos;::&apos;) - os.sep is the (or a most common) pathname separator (&apos;/&apos; or &apos;:&apos; or &apos;\\&apos;) - os.extsep is the extension separator (always &apos;.&apos;) - os.altsep is the alternate pathname separator (None or &apos;/&apos;) - os.pathsep is the component separator used in $PATH etc - os.linesep is the line separator in text files (&apos;\r&apos; or &apos;\n&apos; or &apos;\r\n&apos;) - os.defpath is the default search path for executables - os.devnull is the file path of the null device (&apos;/dev/null&apos;, etc.) Programs that import and use &apos;os&apos; stand a better chance of being portable between different platforms. Of course, they must then only use functions that are defined by all platforms (e.g., unlink and opendir), and leave all pathname manipulation to os.path (e.g., split and join). CLASSES builtins.Exception(builtins.BaseException) builtins.OSError builtins.tuple(builtins.object) nt.times_result nt.uname_result stat_result statvfs_result terminal_size error = class OSError(Exception) | Base class for I/O related errors. | | Method resolution order: | OSError | Exception | BaseException | object | | Methods defined here: | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | helper for pickle | | __str__(self, /) | Return str(self). | | ---------------------------------------------------------------------- | Data descriptors defined here: | | characters_written | | errno | POSIX exception code | | filename | exception filename | | filename2 | second exception filename | | strerror | exception strerror | | winerror | Win32 exception code | | ---------------------------------------------------------------------- | Methods inherited from BaseException: | | __delattr__(self, name, /) | Implement delattr(self, name). | | __getattribute__(self, name, /) | Return getattr(self, name). | | __repr__(self, /) | Return repr(self). | | __setattr__(self, name, value, /) | Implement setattr(self, name, value). | | __setstate__(...) | | with_traceback(...) | Exception.with_traceback(tb) -- | set self.__traceback__ to tb and return self. | | ---------------------------------------------------------------------- | Data descriptors inherited from BaseException: | | __cause__ | exception cause | | __context__ | exception context | | __dict__ | | __suppress_context__ | | __traceback__ | | args class stat_result(builtins.tuple) | stat_result: Result from stat, fstat, or lstat. | | This object may be accessed either as a tuple of | (mode, ino, dev, nlink, uid, gid, size, atime, mtime, ctime) | or via the attributes st_mode, st_ino, st_dev, st_nlink, st_uid, and so on. | | Posix/windows: If your platform supports st_blksize, st_blocks, st_rdev, | or st_flags, they are available as attributes only. | | See os.stat for more information. | | Method resolution order: | stat_result | builtins.tuple | builtins.object | | Methods defined here: | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | ---------------------------------------------------------------------- | Data descriptors defined here: | | st_atime | time of last access | | st_atime_ns | time of last access in nanoseconds | | st_ctime | time of last change | | st_ctime_ns | time of last change in nanoseconds | | st_dev | device | | st_file_attributes | Windows file attribute bits | | st_gid | group ID of owner | | st_ino | inode | | st_mode | protection bits | | st_mtime | time of last modification | | st_mtime_ns | time of last modification in nanoseconds | | st_nlink | number of hard links | | st_size | total size, in bytes | | st_uid | user ID of owner | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | n_fields = 17 | | n_sequence_fields = 10 | | n_unnamed_fields = 3 | | ---------------------------------------------------------------------- | Methods inherited from builtins.tuple: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self&gt;=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(self, key, /) | Return self[key]. | | __getnewargs__(...) | | __gt__(self, value, /) | Return self&gt;value. | | __hash__(self, /) | Return hash(self). | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self&lt;=value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self&lt;value. | | __mul__(self, value, /) | Return self*value.n | | __ne__(self, value, /) | Return self!=value. | | __rmul__(self, value, /) | Return self*value. | | count(...) | T.count(value) -&gt; integer -- return number of occurrences of value | | index(...) | T.index(value, [start, [stop]]) -&gt; integer -- return first index of value. | Raises ValueError if the value is not present. class statvfs_result(builtins.tuple) | statvfs_result: Result from statvfs or fstatvfs. | | This object may be accessed either as a tuple of | (bsize, frsize, blocks, bfree, bavail, files, ffree, favail, flag, namemax), | or via the attributes f_bsize, f_frsize, f_blocks, f_bfree, and so on. | | See os.statvfs for more information. | | Method resolution order: | statvfs_result | builtins.tuple | builtins.object | | Methods defined here: | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | ---------------------------------------------------------------------- | Data descriptors defined here: | | f_bavail | | f_bfree | | f_blocks | | f_bsize | | f_favail | | f_ffree | | f_files | | f_flag | | f_frsize | | f_namemax | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | n_fields = 10 | | n_sequence_fields = 10 | | n_unnamed_fields = 0 | | ---------------------------------------------------------------------- | Methods inherited from builtins.tuple: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self&gt;=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(self, key, /) | Return self[key]. | | __getnewargs__(...) | | __gt__(self, value, /) | Return self&gt;value. | | __hash__(self, /) | Return hash(self). | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self&lt;=value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self&lt;value. | | __mul__(self, value, /) | Return self*value.n | | __ne__(self, value, /) | Return self!=value. | | __rmul__(self, value, /) | Return self*value. | | count(...) | T.count(value) -&gt; integer -- return number of occurrences of value | | index(...) | T.index(value, [start, [stop]]) -&gt; integer -- return first index of value. | Raises ValueError if the value is not present. class terminal_size(builtins.tuple) | A tuple of (columns, lines) for holding terminal window size | | Method resolution order: | terminal_size | builtins.tuple | builtins.object | | Methods defined here: | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | ---------------------------------------------------------------------- | Data descriptors defined here: | | columns | width of the terminal window in characters | | lines | height of the terminal window in characters | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | n_fields = 2 | | n_sequence_fields = 2 | | n_unnamed_fields = 0 | | ---------------------------------------------------------------------- | Methods inherited from builtins.tuple: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self&gt;=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(self, key, /) | Return self[key]. | | __getnewargs__(...) | | __gt__(self, value, /) | Return self&gt;value. | | __hash__(self, /) | Return hash(self). | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self&lt;=value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self&lt;value. | | __mul__(self, value, /) | Return self*value.n | | __ne__(self, value, /) | Return self!=value. | | __rmul__(self, value, /) | Return self*value. | | count(...) | T.count(value) -&gt; integer -- return number of occurrences of value | | index(...) | T.index(value, [start, [stop]]) -&gt; integer -- return first index of value. | Raises ValueError if the value is not present. class times_result(builtins.tuple) | times_result: Result from os.times(). | | This object may be accessed either as a tuple of | (user, system, children_user, children_system, elapsed), | or via the attributes user, system, children_user, children_system, | and elapsed. | | See os.times for more information. | | Method resolution order: | times_result | builtins.tuple | builtins.object | | Methods defined here: | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | ---------------------------------------------------------------------- | Data descriptors defined here: | | children_system | system time of children | | children_user | user time of children | | elapsed | elapsed time since an arbitrary point in the past | | system | system time | | user | user time | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | n_fields = 5 | | n_sequence_fields = 5 | | n_unnamed_fields = 0 | | ---------------------------------------------------------------------- | Methods inherited from builtins.tuple: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self&gt;=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(self, key, /) | Return self[key]. | | __getnewargs__(...) | | __gt__(self, value, /) | Return self&gt;value. | | __hash__(self, /) | Return hash(self). | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self&lt;=value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self&lt;value. | | __mul__(self, value, /) | Return self*value.n | | __ne__(self, value, /) | Return self!=value. | | __rmul__(self, value, /) | Return self*value. | | count(...) | T.count(value) -&gt; integer -- return number of occurrences of value | | index(...) | T.index(value, [start, [stop]]) -&gt; integer -- return first index of value. | Raises ValueError if the value is not present. class uname_result(builtins.tuple) | uname_result: Result from os.uname(). | | This object may be accessed either as a tuple of | (sysname, nodename, release, version, machine), | or via the attributes sysname, nodename, release, version, and machine. | | See os.uname for more information. | | Method resolution order: | uname_result | builtins.tuple | builtins.object | | Methods defined here: | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | ---------------------------------------------------------------------- | Data descriptors defined here: | | machine | hardware identifier | | nodename | name of machine on network (implementation-defined) | | release | operating system release | | sysname | operating system name | | version | operating system version | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | n_fields = 5 | | n_sequence_fields = 5 | | n_unnamed_fields = 0 | | ---------------------------------------------------------------------- | Methods inherited from builtins.tuple: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self&gt;=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(self, key, /) | Return self[key]. | | __getnewargs__(...) | | __gt__(self, value, /) | Return self&gt;value. | | __hash__(self, /) | Return hash(self). | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self&lt;=value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self&lt;value. | | __mul__(self, value, /) | Return self*value.n | | __ne__(self, value, /) | Return self!=value. | | __rmul__(self, value, /) | Return self*value. | | count(...) | T.count(value) -&gt; integer -- return number of occurrences of value | | index(...) | T.index(value, [start, [stop]]) -&gt; integer -- return first index of value. | Raises ValueError if the value is not present. FUNCTIONS _exit(status) Exit to the system with specified status, without normal exit processing. abort() Abort the interpreter immediately. This function &apos;dumps core&apos; or otherwise fails in the hardest way possible on the hosting operating system. This function never returns. access(path, mode, *, dir_fd=None, effective_ids=False, follow_symlinks=True) Use the real uid/gid to test for access to a path. path Path to be tested; can be string or bytes mode Operating-system mode bitfield. Can be F_OK to test existence, or the inclusive-OR of R_OK, W_OK, and X_OK. dir_fd If not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. effective_ids If True, access will use the effective uid/gid instead of the real uid/gid. follow_symlinks If False, and the last element of the path is a symbolic link, access will examine the symbolic link itself instead of the file the link points to. dir_fd, effective_ids, and follow_symlinks may not be implemented on your platform. If they are unavailable, using them will raise a NotImplementedError. Note that most operations will use the effective uid/gid, therefore this routine can be used in a suid/sgid environment to test if the invoking user has the specified access to the path. chdir(path) Change the current working directory to the specified path. path may always be specified as a string. On some platforms, path may also be specified as an open file descriptor. If this functionality is unavailable, using it raises an exception. chmod(path, mode, *, dir_fd=None, follow_symlinks=True) Change the access permissions of a file. path Path to be modified. May always be specified as a str or bytes. On some platforms, path may also be specified as an open file descriptor. If this functionality is unavailable, using it raises an exception. mode Operating-system mode bitfield. dir_fd If not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. follow_symlinks If False, and the last element of the path is a symbolic link, chmod will modify the symbolic link itself instead of the file the link points to. It is an error to use dir_fd or follow_symlinks when specifying path as an open file descriptor. dir_fd and follow_symlinks may not be implemented on your platform. If they are unavailable, using them will raise a NotImplementedError. close(fd) Close a file descriptor. closerange(fd_low, fd_high, /) Closes all file descriptors in [fd_low, fd_high), ignoring errors. cpu_count() Return the number of CPUs in the system; return None if indeterminable. device_encoding(fd) Return a string describing the encoding of a terminal&apos;s file descriptor. The file descriptor must be attached to a terminal. If the device is not a terminal, return None. dup(fd, /) Return a duplicate of a file descriptor. dup2(fd, fd2, inheritable=True) Duplicate file descriptor. execl(file, *args) execl(file, *args) Execute the executable file with argument list args, replacing the current process. execle(file, *args) execle(file, *args, env) Execute the executable file with argument list args and environment env, replacing the current process. execlp(file, *args) execlp(file, *args) Execute the executable file (which is searched for along $PATH) with argument list args, replacing the current process. execlpe(file, *args) execlpe(file, *args, env) Execute the executable file (which is searched for along $PATH) with argument list args and environment env, replacing the current process. execv(path, argv, /) Execute an executable path with arguments, replacing current process. path Path of executable file. argv Tuple or list of strings. execve(path, argv, env) Execute an executable path with arguments, replacing current process. path Path of executable file. argv Tuple or list of strings. env Dictionary of strings mapping to strings. execvp(file, args) execvp(file, args) Execute the executable file (which is searched for along $PATH) with argument list args, replacing the current process. args may be a list or tuple of strings. execvpe(file, args, env) execvpe(file, args, env) Execute the executable file (which is searched for along $PATH) with argument list args and environment env , replacing the current process. args may be a list or tuple of strings. fdopen(fd, *args, **kwargs) # Supply os.fdopen() fsdecode(filename) Decode filename from the filesystem encoding with &apos;surrogateescape&apos; error handler, return str unchanged. On Windows, use &apos;strict&apos; error handler if the file system encoding is &apos;mbcs&apos; (which is the default encoding). fsencode(filename) Encode filename to the filesystem encoding with &apos;surrogateescape&apos; error handler, return bytes unchanged. On Windows, use &apos;strict&apos; error handler if the file system encoding is &apos;mbcs&apos; (which is the default encoding). fstat(fd) Perform a stat system call on the given file descriptor. Like stat(), but for an open file descriptor. Equivalent to os.stat(fd). fsync(fd) Force write of fd to disk. ftruncate(fd, length, /) Truncate a file, specified by file descriptor, to a specific length. get_exec_path(env=None) Returns the sequence of directories that will be searched for the named executable (similar to a shell) when launching a process. *env* must be an environment variable dict or None. If *env* is None, os.environ will be used. get_handle_inheritable(handle, /) Get the close-on-exe flag of the specified file descriptor. get_inheritable(fd, /) Get the close-on-exe flag of the specified file descriptor. get_terminal_size(...) Return the size of the terminal window as (columns, lines). The optional argument fd (default standard output) specifies which file descriptor should be queried. If the file descriptor is not connected to a terminal, an OSError is thrown. This function will only be defined if an implementation is available for this system. shutil.get_terminal_size is the high-level function which should normally be used, os.get_terminal_size is the low-level implementation. getcwd() Return a unicode string representing the current working directory. getcwdb() Return a bytes string representing the current working directory. getenv(key, default=None) Get an environment variable, return None if it doesn&apos;t exist. The optional second argument can specify an alternate default. key, default and the result are str. getlogin() Return the actual login name. getpid() Return the current process id. getppid() Return the parent&apos;s process id. If the parent process has already exited, Windows machines will still return its id; others systems will return the id of the &apos;init&apos; process (1). isatty(fd, /) Return True if the fd is connected to a terminal. Return True if the file descriptor is an open file descriptor connected to the slave end of a terminal. kill(pid, signal, /) Kill a process with a signal. link(src, dst, *, src_dir_fd=None, dst_dir_fd=None, follow_symlinks=True) Create a hard link to a file. If either src_dir_fd or dst_dir_fd is not None, it should be a file descriptor open to a directory, and the respective path string (src or dst) should be relative; the path will then be relative to that directory. If follow_symlinks is False, and the last element of src is a symbolic link, link will create a link to the symbolic link itself instead of the file the link points to. src_dir_fd, dst_dir_fd, and follow_symlinks may not be implemented on your platform. If they are unavailable, using them will raise a NotImplementedError. listdir(path=None) Return a list containing the names of the files in the directory. path can be specified as either str or bytes. If path is bytes, the filenames returned will also be bytes; in all other circumstances the filenames returned will be str. If path is None, uses the path=&apos;.&apos;. On some platforms, path may also be specified as an open file descriptor;\ the file descriptor must refer to a directory. If this functionality is unavailable, using it raises NotImplementedError. The list is in arbitrary order. It does not include the special entries &apos;.&apos; and &apos;..&apos; even if they are present in the directory. lseek(fd, position, how, /) Set the position of a file descriptor. Return the new position. Return the new cursor position in number of bytes relative to the beginning of the file. lstat(path, *, dir_fd=None) Perform a stat system call on the given path, without following symbolic links. Like stat(), but do not follow symbolic links. Equivalent to stat(path, follow_symlinks=False). makedirs(name, mode=511, exist_ok=False) makedirs(name [, mode=0o777][, exist_ok=False]) Super-mkdir; create a leaf directory and all intermediate ones. Works like mkdir, except that any intermediate path segment (not just the rightmost) will be created if it does not exist. If the target directory already exists, raise an OSError if exist_ok is False. Otherwise no exception is raised. This is recursive. mkdir(path, mode=511, *, dir_fd=None) Create a directory. If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. The mode argument is ignored on Windows. open(path, flags, mode=511, *, dir_fd=None) Open a file for low level IO. Returns a file descriptor (integer). If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. pipe() Create a pipe. Returns a tuple of two file descriptors: (read_fd, write_fd) popen(cmd, mode=&apos;r&apos;, buffering=-1) # Supply os.popen() putenv(name, value, /) Change or add an environment variable. read(fd, length, /) Read from a file descriptor. Returns a bytes object. readlink(...) readlink(path, *, dir_fd=None) -&gt; path Return a string representing the path to which the symbolic link points. If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. remove(path, *, dir_fd=None) Remove a file (same as unlink()). If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. removedirs(name) removedirs(name) Super-rmdir; remove a leaf directory and all empty intermediate ones. Works like rmdir except that, if the leaf directory is successfully removed, directories corresponding to rightmost path segments will be pruned away until either the whole path is consumed or an error occurs. Errors during this latter phase are ignored -- they generally mean that a directory was not empty. rename(src, dst, *, src_dir_fd=None, dst_dir_fd=None) Rename a file or directory. If either src_dir_fd or dst_dir_fd is not None, it should be a file descriptor open to a directory, and the respective path string (src or dst) should be relative; the path will then be relative to that directory. src_dir_fd and dst_dir_fd, may not be implemented on your platform. If they are unavailable, using them will raise a NotImplementedError. renames(old, new) renames(old, new) Super-rename; create directories as necessary and delete any left empty. Works like rename, except creation of any intermediate directories needed to make the new pathname good is attempted first. After the rename, directories corresponding to rightmost path segments of the old name will be pruned until either the whole path is consumed or a nonempty directory is found. Note: this function can fail with the new directory structure made if you lack permissions needed to unlink the leaf directory or file. replace(src, dst, *, src_dir_fd=None, dst_dir_fd=None) Rename a file or directory, overwriting the destination. If either src_dir_fd or dst_dir_fd is not None, it should be a file descriptor open to a directory, and the respective path string (src or dst) should be relative; the path will then be relative to that directory. src_dir_fd and dst_dir_fd, may not be implemented on your platform. If they are unavailable, using them will raise a NotImplementedError.&quot; rmdir(path, *, dir_fd=None) Remove a directory. If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. scandir(...) scandir(path=&apos;.&apos;) -&gt; iterator of DirEntry objects for given path set_handle_inheritable(handle, inheritable, /) Set the inheritable flag of the specified handle. set_inheritable(fd, inheritable, /) Set the inheritable flag of the specified file descriptor. spawnl(mode, file, *args) spawnl(mode, file, *args) -&gt; integer Execute file with arguments from args in a subprocess. If mode == P_NOWAIT return the pid of the process. If mode == P_WAIT return the process&apos;s exit code if it exits normally; otherwise return -SIG, where SIG is the signal that killed it. spawnle(mode, file, *args) spawnle(mode, file, *args, env) -&gt; integer Execute file with arguments from args in a subprocess with the supplied environment. If mode == P_NOWAIT return the pid of the process. If mode == P_WAIT return the process&apos;s exit code if it exits normally; otherwise return -SIG, where SIG is the signal that killed it. spawnv(mode, path, argv, /) Execute the program specified by path in a new process. mode Mode of process creation. path Path of executable file. argv Tuple or list of strings. spawnve(mode, path, argv, env, /) Execute the program specified by path in a new process. mode Mode of process creation. path Path of executable file. argv Tuple or list of strings. env Dictionary of strings mapping to strings. startfile(...) startfile(filepath [, operation]) Start a file with its associated application. When &quot;operation&quot; is not specified or &quot;open&quot;, this acts like double-clicking the file in Explorer, or giving the file name as an argument to the DOS &quot;start&quot; command: the file is opened with whatever application (if any) its extension is associated. When another &quot;operation&quot; is given, it specifies what should be done with the file. A typical operation is &quot;print&quot;. startfile returns as soon as the associated application is launched. There is no option to wait for the application to close, and no way to retrieve the application&apos;s exit status. The filepath is relative to the current directory. If you want to use an absolute path, make sure the first character is not a slash (&quot;/&quot;); the underlying Win32 ShellExecute function doesn&apos;t work if it is. stat(path, *, dir_fd=None, follow_symlinks=True) Perform a stat system call on the given path. path Path to be examined; can be string, bytes, or open-file-descriptor int. dir_fd If not None, it should be a file descriptor open to a directory, and path should be a relative string; path will then be relative to that directory. follow_symlinks If False, and the last element of the path is a symbolic link, stat will examine the symbolic link itself instead of the file the link points to. dir_fd and follow_symlinks may not be implemented on your platform. If they are unavailable, using them will raise a NotImplementedError. It&apos;s an error to use dir_fd or follow_symlinks when specifying path as an open file descriptor. stat_float_times(...) stat_float_times([newval]) -&gt; oldval Determine whether os.[lf]stat represents time stamps as float objects. If value is True, future calls to stat() return floats; if it is False, future calls return ints. If value is omitted, return the current setting. strerror(code, /) Translate an error code to a message string. symlink(src, dst, target_is_directory=False, *, dir_fd=None) Create a symbolic link pointing to src named dst. target_is_directory is required on Windows if the target is to be interpreted as a directory. (On Windows, symlink requires Windows 6.0 or greater, and raises a NotImplementedError otherwise.) target_is_directory is ignored on non-Windows platforms. If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. system(command) Execute the command in a subshell. times() Return a collection containing process timing information. The object returned behaves like a named tuple with these fields: (utime, stime, cutime, cstime, elapsed_time) All fields are floating point numbers. truncate(path, length) Truncate a file, specified by path, to a specific length. On some platforms, path may also be specified as an open file descriptor. If this functionality is unavailable, using it raises an exception. umask(mask, /) Set the current numeric umask and return the previous umask. unlink(path, *, dir_fd=None) Remove a file (same as remove()). If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. dir_fd may not be implemented on your platform. If it is unavailable, using it will raise a NotImplementedError. urandom(size, /) Return a bytes object containing random bytes suitable for cryptographic use. utime(path, times=None, *, ns=None, dir_fd=None, follow_symlinks=True) Set the access and modified time of path. path may always be specified as a string. On some platforms, path may also be specified as an open file descriptor. If this functionality is unavailable, using it raises an exception. If times is not None, it must be a tuple (atime, mtime); atime and mtime should be expressed as float seconds since the epoch. If ns is specified, it must be a tuple (atime_ns, mtime_ns); atime_ns and mtime_ns should be expressed as integer nanoseconds since the epoch. If times is None and ns is unspecified, utime uses the current time. Specifying tuples for both times and ns is an error. If dir_fd is not None, it should be a file descriptor open to a directory, and path should be relative; path will then be relative to that directory. If follow_symlinks is False, and the last element of the path is a symbolic link, utime will modify the symbolic link itself instead of the file the link points to. It is an error to use dir_fd or follow_symlinks when specifying path as an open file descriptor. dir_fd and follow_symlinks may not be available on your platform. If they are unavailable, using them will raise a NotImplementedError. waitpid(pid, options, /) Wait for completion of a given process. Returns a tuple of information regarding the process: (pid, status &lt;&lt; 8) The options argument is ignored on Windows. walk(top, topdown=True, onerror=None, followlinks=False) Directory tree generator. For each directory in the directory tree rooted at top (including top itself, but excluding &apos;.&apos; and &apos;..&apos;), yields a 3-tuple dirpath, dirnames, filenames dirpath is a string, the path to the directory. dirnames is a list of the names of the subdirectories in dirpath (excluding &apos;.&apos; and &apos;..&apos;). filenames is a list of the names of the non-directory files in dirpath. Note that the names in the lists are just names, with no path components. To get a full path (which begins with top) to a file or directory in dirpath, do os.path.join(dirpath, name). If optional arg &apos;topdown&apos; is true or not specified, the triple for a directory is generated before the triples for any of its subdirectories (directories are generated top down). If topdown is false, the triple for a directory is generated after the triples for all of its subdirectories (directories are generated bottom up). When topdown is true, the caller can modify the dirnames list in-place (e.g., via del or slice assignment), and walk will only recurse into the subdirectories whose names remain in dirnames; this can be used to prune the search, or to impose a specific order of visiting. Modifying dirnames when topdown is false is ineffective, since the directories in dirnames have already been generated by the time dirnames itself is generated. No matter the value of topdown, the list of subdirectories is retrieved before the tuples for the directory and its subdirectories are generated. By default errors from the os.scandir() call are ignored. If optional arg &apos;onerror&apos; is specified, it should be a function; it will be called with one argument, an OSError instance. It can report the error to continue with the walk, or raise the exception to abort the walk. Note that the filename is available as the filename attribute of the exception object. By default, os.walk does not follow symbolic links to subdirectories on systems that support them. In order to get this functionality, set the optional argument &apos;followlinks&apos; to true. Caution: if you pass a relative pathname for top, don&apos;t change the current working directory between resumptions of walk. walk never changes the current directory, and assumes that the client doesn&apos;t either. Example: import os from os.path import join, getsize for root, dirs, files in os.walk(&apos;python/Lib/email&apos;): print(root, &quot;consumes&quot;, end=&quot;&quot;) print(sum([getsize(join(root, name)) for name in files]), end=&quot;&quot;) print(&quot;bytes in&quot;, len(files), &quot;non-directory files&quot;) if &apos;CVS&apos; in dirs: dirs.remove(&apos;CVS&apos;) # don&apos;t visit CVS directories write(fd, data, /) Write a bytes object to a file descriptor. DATA F_OK = 0 O_APPEND = 8 O_BINARY = 32768 O_CREAT = 256 O_EXCL = 1024 O_NOINHERIT = 128 O_RANDOM = 16 O_RDONLY = 0 O_RDWR = 2 O_SEQUENTIAL = 32 O_SHORT_LIVED = 4096 O_TEMPORARY = 64 O_TEXT = 16384 O_TRUNC = 512 O_WRONLY = 1 P_DETACH = 4 P_NOWAIT = 1 P_NOWAITO = 3 P_OVERLAY = 2 P_WAIT = 0 R_OK = 4 SEEK_CUR = 1 SEEK_END = 2 SEEK_SET = 0 TMP_MAX = 2147483647 W_OK = 2 X_OK = 1 __all__ = [&apos;altsep&apos;, &apos;curdir&apos;, &apos;pardir&apos;, &apos;sep&apos;, &apos;pathsep&apos;, &apos;linesep&apos;, ... altsep = &apos;/&apos; curdir = &apos;.&apos; defpath = r&apos;.;C:\bin&apos; devnull = &apos;nul&apos; environ = environ({&apos;JPY_PARENT_PID&apos;: &apos;1688&apos;, &apos;TERM&apos;: &apos;xter....WSH;.MSC... extsep = &apos;.&apos; linesep = &apos;\r\n&apos; name = &apos;nt&apos; pardir = &apos;..&apos; pathsep = &apos;;&apos; sep = r&apos;\&apos; supports_bytes_environ = False FILE c:\anaconda3\envs\py3\lib\os.py ​​ 文件通配符glob模块提供了一个函数用于从目录通配符搜索中生成文件列表。 1234import globglob.glob('*.py')#比如输出为['prime.py','random.py']形式。 [] 命令行参数通用工具脚本经常调用命令行参数。这些命令行参数以链表的形式存储于sys模块的argv变量（argument vector参数矢量）。 错误输出重定向和程序终止sys还有stdin,stdout和stderr属性，即使在stdout被重定向时，后者也可以用于显示警告和错误信息。 大多数脚本的定向终止都使用sys.exit()。 字符串正则匹配re模块为高级字符串处理提供了正则表达式工具。对于复杂的匹配和处理，正则表达式提供了简洁、优化的解决方案。 如果只需要简单的功能，首先考虑字符串的方法。 12import re'tea for too'.replace('too','two') &apos;tea for two&apos; 数学math模块为浮点运算提供了底层C函数库的访问。 12import mathmath.log(1024,2) 10.0 random模块提供了生成随机数的工具 12import randomrandom.sample(range(100),10) #sample without replacement [45, 66, 56, 3, 97, 87, 35, 1, 38, 92] 访问互联网有几个模块用于访问互联网以及处理网络通信协议。其中最简单的两个是用于处理从 urls 接收的数据的urllib.request 以及用于发送电子邮件的 smtplib。 日期和时间datetime模块为日期和时间处理的同时提供了简单和复杂的方法。 支持日期和时间算法的同时，实现的重点放在更有效的处理和格式化的输出。 该模块还支持时区的处理。 123from datetime import datenow = date.today()print(now) 2019-05-29 数据压缩接支持通用的数据打包和压缩格式：zlib，gzip，bz2，zipfile，以及 tarfile。 1234567891011import zlibs = b'witch which has which witches wrist watch'print(len(s))t=zlib.compress(s)print(len(t))zlib.crc32(s) 41 37 226805979 测试模块开发高质量软件的方法之一是为每一个函数开发测试代码，并且在开发过程中经常进行测试 doctest模块提供了一个工具，扫描模块并根据程序中内嵌的文档字符串执行测试。 测试构造如同简单的将它的输出结果剪切并粘贴到文档字符串中。 通过用户提供的例子，它强化了文档，允许 doctest 模块确认代码的结果是否与文档一致。 12]]></content>
      <categories>
        <category>教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[马尔科夫随机场原理简介]]></title>
    <url>%2Fblog%2F2020%2F03%2F21%2F%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%9A%8F%E6%9C%BA%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[在机器视觉领域，一个图像分析问题通常被定义为建模问题，图像分析的过程就是从计算的观点求解模型的过程。一个模型除了可以表示成图像的形式外，通常使用一个目标函数来表示，建模的过程就是定义目标函数的过程，模型的求解过程就是利用各种优化工具或者知识来求解目标函数的过程。使用各种优化工具是因为处理过程中存在各种各样的不确定性，使用优化工具可以比较客观真实的模拟模型解。 在解释视觉信息式，contextual constraints（上下文约束）是必须的，可以从图像中的像素的空间和视觉上下文来理解场景；也可可以利用low-level来制作feature描述物体，再利用feature的上下文语义识别物体；高层特征可以通过对底层特征的抽象得到，底层特征可以通过图像中像素间最基本的位置关系得到。 马尔科夫随机场(Markov Random Field，MRF)理论提供了一个方便且具有一致性的建模方法，不管是上下文依赖关系的实体（如图像像素），还是具有内在联系的feature，MRF都可以实现。 图像分析中的labeling问题labeling problemlabeling问题是机器视觉和图像分析领域的基本问题。图像分割，目标识别等问题都可以看成是labeling问题的延伸，labeling问题就是将一组标签(label)赋给图像中的每个像素。 令$L$代表所有的标签集合(label set)，$S$代表需要被赋予标签的元素的集合，如一张图像。当需要进行边缘检测时，问题转化为从$L = \{\rm{edge,nonedge}\}$中选择一个标签赋予$S$中的任意一个元素$s$，其中$s$可以看看成是图像中的某个像素。 这样得到由$S$中所有像素$s$的标签构成的集合$f = \{f_1,\dots,f_n\}$，$n$为$S$中的像素个数。$f$的取值生成过程，可以看做是一个函数在离散定义域$S$上的求值过程：$f: S \rightarrow L$，即是一个映射(mapping)过程。 作为随机场的术语，labeling也被称为“configuration（配置，结构）”。 使用contextual constraints进行labelingcontextual constraints常被用来建模成条件概率，比如$P(f_i | f_i’)$，$f_i’$代表$i’$位置的label。因为局部信息更容易被直接观察到，可以使用局部属性(local properties)来做全局推理。 当所有的label都相互独立时，全局label的联合概率可以写成：$$P(f) = P(f_1) \dots P(f_n)$$这样一个全局的labeling问题$f$可以通过局部的labeling$f_i$来计算得到，这是contextual constraints优势。 基于优化的方法优化（optimization）能处理成像和视觉领域内众多的不确定性，一个问题可以表述为一些指标的优化过程。 优化可以隐式执行，如Hough transform常用来检测直线和曲线，是通过检测累积函数的峰值；模板匹配(template matching)将问题转化为最大化似然概率；边缘检测还被定义为使用一些简单算子，如Gaussian算子。Sobel算子等。 传感器成像和传输信号时，出现的不确定的噪声和干扰，利用优化模型可以解决这个问题。 优化包括三件事：问题表述、目标函数和优化算法。 问题表述：包括描述和形式化计算。描述重点在于如何表述(represent)图像的特征和物体形状；形式化计算主要关注解决方案，和S，L的选择有关。 目标函数：将解决方案映射到实际计算空间，并且可以衡量解决方案的损益。 优化目标：在问题空间寻找优化的解决方案，（1）局部极小值存在于非凸函数的问题；（2）算法的空间复杂度和时间复杂度。 能量函数作为解决方案的全局质量评价的衡量标准；作为一个最小化的解决方案的搜索过程的guide。 能量函数通常期望是一个最小又最全局的解决方案。这样设计能量函数的应朝着简单小巧方向。 鉴别两种不同类的问题，调试模型，如一个优化程序的输出不是期望，可能有两个原因：（1）目标函数不正确；（2）局部最小化时出错。 在实空间最小化时，如果能量函数是平滑的凸函数，全局最小化等价为局部极小能量函数的梯度。非凸，没有通用的方法获得有效解。 在模式识别领域，能量函数大致有两种形式：参数和非参数。 参数形式：基础分部的类型是已知的，并且分布的参数是有限固定，参数设定，能量函数的形式可以公式化描述。 非参数形式：分布树（distribution tree）方法，分布未知。分布要么根据数据估计，要么根据预先设定的具有若干未知参数的基函数估计得到。 因此，能量函数两个重要要素是：形式和参数。形式依赖于解$f$和观察数据$d$，记作$E(f|d)$，参数集合记作$\theta$，能量函数可以写成$E(f|d,\theta)$。 目标解记作$f^$，当参数作为能量函数定义的一部分时，目标解的求解过程看成是$f^ = \arg \min E(f|d)$，当参数不确定时，这个形式不完整。这些参数应当提前被设定或者在计算过程中能够通过某些方法估计。 目标函数的形式化表述成像系统的不确定性，当有关数据的一些分布知道，但是先验知识不知道时，可以使用$\rm{maximum likehood}$（ML，最大似然）准则。$f^* = \arg \max P(d|f)$。 若只有先验知识，则$\rm{maximun entropy}$（最大熵）更适合：$$f^* = \arg \max P(d|f) - \sum\limits_{i=1}^{m} P(f_i) \ln P(f_i)$$最大熵的准则是：熵越大，可能性越大。 当先验知识和似然分布都已知时，可以通过贝叶斯准则来获得最好的结果。 Bayes估计通常使用最小化风险的方法来获得优化估计，目标解$f^$的贝叶斯风险(Bayes risk)可以定义为：$$R(f^) = \int_\left\{f \in \mathbb{F}\right\} C(f^ ,f)P(f|d)df$$$d$是观测数据，$C(f^,f)$是代价函数，$p(f|d)$是后验分布。 首先需要根据先验知识和似然分布计算后验分布，用到贝叶斯定理：$P(f|d) = \frac{p(d|f)P(f)}{p(d)}$，其中$P(f)$是labeling f的先验概率，$p(d|f)$是观测数据$d$的条件概率密度函数(PDF)，也称为$f$对$d$的似然函数；$p(d)$是$d$的密度，给定$d$，则是一个常量。 代价函数$C(f^,f)$决定了当truth是$f^$时估计$f$的代价，$C$可以使用$f^$和$f$之间的距离作为度量：$$C(f^,f) =||f^ - f|| ^2$$通常，可以使用$\delta$作为阈值，如果$||f^ -f|| &lt; \delta$，则$C(f^,f) = 0$；反之，等于1。这样，贝叶斯风险可以记作：$$R(f^) = 1-\int_{f:||f^ -f|| \le \delta} P(f|d) df$$因此，最小化上式等价于：$$f^ = \arg \max P(f|d)$$即，最大化后验概率（MAP）估计。由于$p(d)$在$d$是固定时是常量，因此$P(f|d)$正比于联合概率分布P(f,d)。最终MAP公式化描述可以写成：$$f^ = \arg \max_\left\{f \in \mathbb{F}\right\} \left\{ p(d|f) P(f) \right\}$$ MAP-MRF labeling在MAP-MRF标记问题中，$P(f|d)$是MRF的后验分布，$f$一个典型的先验分布可以写成：$$P(f) = \frac{1}{Z} e ^{-U(f)}$$其中，$U(f) = \sum\limits_i \sum\limits_\left\{i’ \in \{i-1,i+1\}\right\} (f_i - f_{i’})^2 $ 是能量函数的先验。 假设观测值还需要加上一个高斯噪声，$d_i = f_i + e_i$,$e_i \sim \mathcal{N}(\mu,\sigma^2)$。$$p(d|f) = \frac{1}{\prod_{i=1}^{m} \sqrt\{2\pi\sigma^2}\} e^{-U(d|f)}$$ $$U(d|f) = \sum\limits_{i=1}^m (f_i - d_i)^2 /2\sigma^2 是先验能量$$ 后验概率可记作：$$P(f|d) \propto e^{-U(f|d)}$$ $$U(f|d) =U(d|f) + U(f) = \sum\limits_{i=1}^m (f_i -d_i)^2/2\sigma_i^2 + \sum\limits_{i=1}^m(f_i - f_{i-1})^2是后验能量$$ 这样，MAP估计等价为寻找最小的后延能量函数：$f^*= \arg \min U(f|d) $。这里只需要一个参数估计$\sigma_i$。当参数确定，$U(f|d)$确定，MAP-MRF的解确定。 MAP-MRF方法解决机器视觉问题的主要步骤如下： \1. 将视觉问题转换为labeling的问题，并确定是属于LP1-LP4中的哪一种，然后选择正确的MRF表示f。 \2. 定义MAP解的后验能量函数。 \3. 寻找MAP解。 MRF领域系统把一幅图像看作是PGM模型，图像中的每一个像素代表模型的顶点，PGM模型中的顶点之间的边的表示：领域系就是针对这种相邻位之间的概率依赖而设计的。 马尔科夫性是指一个事件当前的状态只与它直接的原因(cause)有关，而与其他事件无关。转移到随机场(Random Field)中，利用领域系统可以分析空间上的马尔科夫性。 即：一个像素点的特性，更可能与它周围的像素的影响，与它距离越远的像素，对它的特性影响越小。 领域系统建模，定义了图像中的一个中心像素，受周围哪些像素(Neighbourhood)影响。中心像素和相邻像素一起构成集合，称为“Cliques”（组团）。 对于图像$X$中任意像素$x$，$X$的领域系统记作$N$，$N = \{N_x , x \in X\}$，$N_x$代表$x$的相邻像素的集合，同时，相邻的关系还满足如下约束： （1）$x$不属于$N_x$ （2）相邻关系是相互的，即对于像素$x,y$，$y \in N_x,x \in N_y$。 相邻像素的集合可以定义为： $N_x = \{y \in X |dis(x,y) &lt; \tau,y \ne x\}$，$dis(x,y)$通常采用欧式距离。上图紫色代表4领域系统，N1-N7称为8领域系统。图像$X$和领域系统$N$一一对应，$X$包含所有的顶点，$N$根据相邻关系决定顶点的连线。 “Clique”是与x有关的属于X的一个子集，更多的分析的是Nx里的几种依赖关系：（1）单点，x；（2）成对点，x和y，比如x和N1；（3）三相邻点组：x，y，z，例如x，N1，N2。一般来说这三种情况形成的集合被记作C1，C2，C3： $C_1={x | x∈X}；$ $C_2=\{(x , y)|y∈N_x, x∈X\}；$ $C3=\{(x , y , z)|x,y,z∈X,并且三者互相邻近\}$。并且，由于x的邻域与y的邻域不完全相等，因此${x,y}$与${y,x}$不等价，以此类推。 综上所述，对于$(X , N)$的所有Clique为： $C=C1 \cup C2 \cup C3…..$ MRF定义令$F = \{F_1 ,\dots,F_m\}$是一组定义在集合$S$上的随机变量，其中$F_i$代表在标签集L上的一个取值$f_i$，$F_i = f_i$代表取值为$f_i$的事件。 $(F_1 = f_1,\dots,F_m = f_m)$代表联合事件(joint event)。假设$f = \{f_1,\dots,f_m\}$，即$F =f$。 对于离散的标签集合L，$F_i = f_i$的概率记作$P(F_i =f_i)$，简称$P(f_i)$；联合事件的概率记作$P(f)$。对于连续的标签集合L，知道条件概率密度函数PDF $p(F_i = f_i)$和$p(F=f)$。 对于一个领域系统$N$,如果满足一下两条性质： （1）$P(f) &gt; 0$，对任意的$f$。（非负性） （2）$P(f_i| f_{s-[i]}) = P(f_i | f_{N_i})$。（马尔科夫性） 这样称$F$是一个马尔科夫随机场。 其中，$s-{[i]}$表示除$i$之外的所有点。$f_{s-[i]}$表示$s-[i]$的所有点的标签集合，$f_{N_i} = \{f_{i’} |i’ \in N_i\}$表示点$i$的领域的点的标签集合。 在利用MRF建模解决实际问题时，往往使用多个MRF，如在边缘检测时，一般会用两个MRF模型，一个对像素建模，一个对边缘建模。 GRFGibbs Distribution是一个概率分布，公式化定义为：$$P(f) = Z^{-1} \times e^{-\frac{1}{T} U(f)} \ Z = \sum\limits_{ f \in F} e^{-\frac{1}{T} U(f)}是一个归一化常数，称为配分函数(\rm{partition function})$$$T$一般取值为1。 $U(f)$是能量函数，等于所有可能的Clique的集合C的潜在能量的和，这个潜在能量的大小根据具体设定有所不同。是MRF的核心。 两个label之间的上下文约束（Contextual constraints）可以算作是传递上下文信息的最低阶约束。这种约束，形式简单且计算量小。这种约束可以变成Gibbs的能量函数，一般被叫做“pair-site clique potentials”。之所以叫做这个名字，是因为：（1）这种约束的主要对象有两个；（2）对两个对象建模符合PGM对关系建模的思想；（3）以能量表示两个对象之间的关系，适用于GRF。 CRFMRF的核心思想：针对标签集合F中的任意元素$f_i$，其满足马尔科夫性$P(f_i| f_{s-[i]}) = P(f_i | f_{N_i})$，即$f_i$只和其领域系统的取值有关。 CRF核心思想：不仅考虑马尔科夫性，同时将抽象信息作为一个观察层，对于观察数据集合$D$中的任意元素$d$，标签集合$F$中任意元素$f_i$，如果每个$f_i$都满足马尔科夫性：$$P(f_i | d, f_{s-[i]}) = P(f_i |d, f_\left\{\mathcal{N_i}\right\})$$称$F$是一个条件随机场(Conditional Random Fields，CRF)。CRF更多是对后验建模，而不是对先验或者似然建模。根据Gibbs等价性，可以将CRF写成：$$P(f|d) = \frac{1}{Z} \exp \left( -\frac{1}{T} E(f|d)\right)$$ $$P(f|d) = \frac{1}{Z} \exp \left\{ -\sum\limits_{i \in S}V_1(f_i |d) - \sum\limits_{i \in S}\sum\limits_{i’ \in N_i}V_2(f_i ,f_{i’} |d) \right\}$$ CRF和MRF的主要区别： （1）对于单点势能(unary potential)，CRF包含所有的观测数据建模，而MRF仅对当前点计算； （2）对于成对点势能(pairwise potential)，CRF依旧包含所有点，而MRF仅包含观察数据$d_i$及其领域系统内点$d_{i’}$。 DRF判别随机场(Discriminative Random Fields，DRF)是CRF的延伸，一方面，DRF只用于二维场景，如图像；另一方面，DRF通过局部判别式分类器计算单点势能与成对点势能。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[回归问题简介]]></title>
    <url>%2Fblog%2F2020%2F01%2F11%2F%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[线性回归假设函数：$h_{\theta}(x) = \theta_0x_0+ \theta_1 x_1 + \dots + \theta_n x_n$ 损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$ 目标函数：$\min J(\theta_0,\theta_1,\dots,\theta_n)$ 线性回归中可能遇到的问题： 求解损失函数的最小值的两种方法：梯度下降法和正规方程。 特征缩放：对特征数据进行归一化操作，一是能够提升模型的收敛速度，二是能提升模型精度。 学习率$\alpha$的选取：过小，导致迭代次数变多，收敛速度变慢；过大，可能会跳过最优解，最终导致无法收敛。 过拟合问题： 解决方法：1）丢弃一些对最终预测影响不大的特征，可以通过PCA算法实现。 ​ 2）使用正则化，保留所有特征，但是减少特征前面的参数$\theta$的大小，具体即修改线性回归中的损失函数形式。岭回归和Lasso回归即如此。 岭回归和Lasso回归是为了解决线性回归出现的过拟合问题以及在通过正规方程求解$\theta$过程中出现的X转置乘以X转置不可逆这两类问题。 这两种回归均通过在损失函数中引入正则化项来达到目的。 线性回归损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$ 岭回归损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2 + \lambda\sum\limits_{j=1}^n \theta_j^2$ Lasso回归损失函数：$J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2 + \lambda \sum\limits_{j=1}^n|\theta_j|$ 其中$\lambda$是正则化参数，过大，会使得所有参数$\theta$均最小化，造成欠拟合；过小，会导致对过拟合处理不当。 两者主要区别： 岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中许多的$\theta$均变为0，岭回归要求所有的$\theta$均存在， Lasso回归的计算量要远远小于岭回归。 数学描述： 惩罚方法： Lasso方法： Ridge方法： 图像比较： 以二维数据为例，左图对应Lasso方法，右图对应Ridge方法，红色部分代表随着$\lambda$的变化所得到的残差平方和。$\hat{\beta}$为椭圆的中心点，对应普通线性模型的最小二乘估计。 两图的区别在于蓝色区域（约束域），等高线和约束域的切点就是目标函数的最优解，Ridge方法对应的约束域是圆，切点只会存在于圆周上，不会和坐标轴相切，在任一维度上的取值都不会为0，因此没有稀疏。 Lasso方法，约束域是正方形，会存在与坐标轴的切点，使得部分维度特征权重为0，容易产生稀疏的结果。 对比可知，Lasso方法可以达到变量选择的效果， 维数灾难高维数据：数据维度远大于样本量的个数。 维数灾难的特征 在空间中数据是非常稀疏的，与空间的维数相比样本量总是显得非常少 随着维数的增长，分析所需要的空间样本数会呈指数增长 在高维数据空间，预测将变得不再容易 导致模型过拟合。 curse of dimensionality，“维数灾难”，分类器的性能随着特征的个数的变化不断的增加，过了某个值后，性能不升反降。 特征数量越多，训练样本会越稀疏，分类器的参数估计越不准确，更加容易出现过拟合问题；“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的，处于中心位置的训练样本比四周的训练样本更加稀疏。 在高维特征空间内，大多数的训练样本位于超立方体的角落里。 在高维特征空间内，对于样本距离的度量失去意义，由于分类器的性能基本依赖于如Euclidean距离、Manhattan距离等，所以在特征数量过大时，分类器性能降低。 维数灾难的解决办法 理论上，如果训练样本数足够大，不会出现维数灾难，可以采用任意多的特征来训练分类器。 对于一些泛化能力不是很好的分类器，如Neural network ,knn,decision tree等，不应该采用过多的特征，这些需要精确的非线性决策边界。 对于一些泛化能力较好的分类器，如naive bayes,linear classifier等，可以适当增加特征的数量。 给定N个特征，从中选出M个最优的特征，有很多特征选择算法来帮助确定特征数量以及选择特征，有很多特征提取算法，如PCA等，交叉验证（cross-validation）也长被用于检测和避免过拟合问题。 机器学习中的维数灾难问题 数据降维 维数灾难带来的过拟合问题，解决思路：1）增加样本量；2）减少样本特征。 主成分分析是保留所有原变量的基础上，通过原变量的线性组合得到主成分，选取少数主成分就可保留原变量的绝大部分信息，这样用几个主成分来代替原变量，从而达到降维的目的。 主成分只适用于数据空间维度小于样本量的情况，当数据空间维度很高时，不再使用。 逻辑回归logistic regression（LR）主要用于解决分类问题，逻辑回归是一个非线性模型，sigmoid函数，又称为逻辑回归函数，但是本质上是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归。 逻辑回归首先把样本映射到[0,1]之间的数值（归功于sigmoid，可以把任何连续的值映射到[0,1]之间，数越大越趋向于0，越小越趋近于1。） sigmoid函数为：$g(z)= \frac{1}{1+e^{-z}}$ 判定边界：对多元线性回归方程求sigmoid函数。$h_{\theta}(x)= g(\theta_0+\theta_1x_1+ \dots + \theta_n x_n)$找到一组θ，把样本分为两类，得到判定边界。 逻辑回归的损失函数$$Cost(h_{\theta}(x),y) = \cases{-\log(h_{\theta}(x)), y=1 \ -\log(1-h_{\theta}(x)), y=0 }$$其中$h(x)$是一个概率值，$y=1$表示正样本，$y=0$表示负样本。当为正样本时，如果给定的概率特别小（预测为负样本），损失会很大；给定的概率很大（预测为正样本），损失会接近0。 正则化 使用正则化来避免过拟合，在经验风险最小化的基础上（训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度，如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。 正则化降低了特征的权重，使得模型更为简单，一般采用L1范式或者L2范式，形式分别为$\Phi(\omega) = ||x||_1$和$\Phi(x) = ||x||_2$。 带L2正则项的损失函数为：$$J(\theta) = \left[-\frac{1}{m} \sum\limits_{i=1}^{m} y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1-h_{\theta}(x^{(i)})) \right] +\frac{\lambda}{m} \sum\limits_{j=1}^n \theta_j^2$$ 给loss function加上正则项，能够使得优化目标函数$h = f + ||\omega||$，需要在两者之间做一个平衡，通过降低模型的复杂度，得到更小的泛化误差，降低过拟合程度。 L1正则化是在loss function后边加正则项为L1范数，容易得到稀疏解（0比较多）。L2正则项所加的为L2范数的平方，加上L2正则项相比于L1，得到解比较平滑（不是稀疏），但是同样能够保证解接近于0（但不等于0，所以相对平滑）的维度比较多，降低模型的复杂度。 逻辑回归 样本处理 如果样本太大，离散化后用one-hot编码处理成0，1值，再用LR处理会较快收敛，如果一定用连续值，可以scaling。 如果样本不均衡，样本充足的情况下可以做 下采样——抽样。样本不足的情况下做 上采样——对样本少的及做重复。修改损失函数，给不同的权重。 特征处理 离散化优点：映射到高维空间，用linear的LR（快，且兼具更好的分割性） 稀疏性：0，1向量内积乘法运算速度快，计算结果方便存储，容易扩展 离散化后，给线性模型带来一定的非线性，模型稳定性，收敛性高，鲁棒性好，在一定程度上降低了过拟合风险。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记_科大]]></title>
    <url>%2Fblog%2F2019%2F06%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0_%E7%A7%91%E5%A4%A7%2F</url>
    <content type="text"><![CDATA[What is machine learning and does it matter? Learning can be explained as a process which improves performances from experience（经验）, an extension(延伸，拓展) to this definition would be, the process of ML which can be explained or defined, as a method through which computer programs, that habitually （习惯地，惯常地）or spontaneously（自然地，自发地，不由自主地） improve their performance through experience. This would basically translate into machine’s learning to improve their performance based on limited programming interventions(干涉，介入). ML can be considered as an extension to AI ,which believes that machines should be able to adapt and learn through experience. ML is not a new innovation and has been around for years ,however with new computing technologies, ML has evolved, most of the algorithms have been around, however ,the ability to apply complex algorithms to big data ,in a loop and more rapidly, is a recent development. ML is quite integrated in our everyday lives, so much that we might not be consciously aware how frequently we are using the application. For example ,there is great excitement over google’s self drive car, a product of ML. Spam emails being diligently dumped away, or frequent recommendations while shopping online, or offers from particular brands(扔掉) of your interest being brought to your notice, are all direct outcomes of Machine Learning. Besides the basic applications, more recent complex uses of ML would be early fraud (欺诈)detection in banking, a lot of businesses are able to have a consolidated（巩固，加强） look at what their customers feel about them, emotional and sentimental analysis is possible through data mining（数据挖掘） techniques, again a direct product of ML. In current times ML matters, for the possiblities and advantages it offers. There are growing volumes(卷，册，容量，体积) of data available to us easily, computational processing is cost effective, and we have better data storage opportunities, all this indicates that we are right in the center of exciting times, where, we will be able to analyse bigger and complex data faster and more accurately. Which directly means that organisations will have a better vantage（优势，有利地位） point to make informed decisions, leading to better profits and avoiding risks. with a good investment of time in creating training data for machines, learning can then be expedited through experienced and learning through algorithms. Implementation and automation then become easy for machines, upon learning, a machine can process several images without any fatigue as oppased to human brain, which might deliver data with errors. With good training data input and intelligent processing, with an accurate algorithm, the output can be phenomenal(非凡的). Hence（因此） it is believed that big data and ML is a great combination, opening doors to various opportunities. Application of algorithms in building models may expose links which can help an entity(实体，独立存在体) make better decisions with minimal human interventions（人为干预）, keeping biases（偏见，偏置） away. Most organizations in recent times have understood the importance, benefits and value of Machine Learning technology, as most insights from the avaliable data can be received in real time, hence giving companies an edge over their competitors, and assisting them in better aligning（满足，匹配，使结盟，使成一行） their needs with those of their customers. Due to these paybacks, application of ML can be seen in Financial services, Healthcare , Marketing and Sales, Transportation and logistics（后勤，符号逻辑）, Goverment agencies like Utilities（公共事业，公用工程） and Publics Safety. So while machine learning has many advantages ,it has a few challenges, however, the benefits of the application outweigh the limitations（超过限制）, The ability to decipher（破译，辨认） big data, with minimal programming, faster and accurate results in real time ,will see ML be applied in various aspects of our daily lives. 汤姆米切尔（Mitchell）: 对于某一类任务T和性能度量P，如果一个计算机程序在某些任务T 上以P度量的性能随着经验E的增加而提高，那么称这个计算机程序是重经验E中学习[1]。 [1] Machine Learning, Tom Mitchell, McGraw Hill, 1997. http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html 伊恩·古德费罗（Ian Goodfellow）、约舒亚·本吉奥（Yoshua Bengio）和亚伦·库尔维尔（Aaron Courville）: 机器学习本质上属于应用统计学，更多的关注如何利用计算机对复杂函数进行统计估计，而不太关注如何估算这些函数的置信区间 [2] [2] Deep Learning, Ian Goodfellow, Yoshua Bengio &amp; Aaron Courville, MIT Press, 2016. https://www.deeplearningbook.org/ 伊恩·威腾（Ian Witten）、埃贝·弗兰克（Eibe Frank）和马克·霍尔（Mark Hall）： 我们感兴趣的是在新情境下性能的替身或者是性能提升的潜力。 当以一种可以使自身在未来表现更好的方式改变自己的行为时，就是在学习。 学习意味着思考和目标，必须有目标地去学习。 经验表明，在机器学习和数据挖掘的许多应用中，获得清晰的知识结构，即结构化描述，以及在新实例预测中表现良好的能力，通常使用数据挖掘来获取知识，而不是仅仅用来预测[3]。 [3] Data Mining: Practical Machine Learning Tools and Techniques (3rd ed.), Ian Witten, Eibe Frank &amp; Mark Hall, Morgan Kaufmann, 2011.（很少涉及数学，多实用性解释，去除数据挖掘部分，适用机器学习） https://www.cs.waikato.ac.nz/ml/weka/book.html 克里斯托弗·毕肖普（Christopher Bishop）： 以算法为中心，间接定义机器学习。 机器学习算法的结果可以表示为一个函数y(x)，输入新的数字图像x，产生向量y，用同样的方法编码来作为目标向量。 在训练阶段（即学习阶段），根据训练数据确定y(x)精确的形式。 一旦训练完成模型，就可以用它来确认测试集中新数字图像的类别，正确分类新数字图像的能力被称为泛化，这些新数字图像不同于训练时的数字图像。 在实际应用中，输入向量的多样性使得训练数据只能包含所有可能输入向量的一小部分，因此泛化是模式识别的核心目标[4]。 [4] Pattern Recognition and Machine Learning, Christopher M. Bishop, Springer, 2006. https://www.springer.com/gp/book/9780387310732 吴恩达机器学习笔记 机器学习的几种定义这里有四种机器学习定义的方法： 根据优化过程，抽象定义机器学习 更具规范性的定义，指出计算力在机器学习中的重要性 关注“学习”哪些方面在机器学习过程中是相似的和重要的 从算法角度概述机器学习 都不是完整的，但便于我们自己对机器学习的定义的扩展。 机器学习中的数学基础梯度下降法假设一个函数$J(w)$，如下图： 要求得当$w$为何值时，$J(w)$能够取得最小值。初始位置的切线的斜率为$a&gt;0$（即该点导数大于0），$w=w-a$能使得让$w$的值减小，循环求导更新$w$直到$J(w)$取得最小值。如果函数$J(w)$包含多个变量，需要分别对不同的变量求偏导来更新不同变量的值。 链式法则即复合函数求导 $f’[g(x)] = f’[g(x)]g’(x)$ 常用距离度量方法欧式距离（Euclidean distance） 二维平面上点$a(x_1,y_1),b(x_2,y_2)$间的欧式距离为： ​ $d_{12} = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$。 n维度空间点$a(x_{11},x_{12},…,x_{1n}),b(x_{21},x_{22},…,x_{2n})$间的欧式距离为： $d_{12}= \sqrt{\sum_{k=1}^n(x_{1k}-x_{2k})^2}$。 二维空间欧式距离python实现： 1234def euclidean2(a,b): distance = sqrt( (a[0]-b[0])**2+ (a[1]-b[1])**2 ) return distanceprint('a,b两点之间的欧式距离为：'，euclidean2((1,1),(2,2))) 三维空间欧式距离实现： 1234def euclidean3(a,b): distance = sqrt( (a[0]-b[0])**2+ (a[1]-b[1])**2 +(a[2]-b[2])**2 ) return distanceprint('a,b两点之间的欧式距离为：'，euclidean3((1,1,1),(2,2,2))) 多维空间的欧式距离实现： 123456789101112131415def euclideann(a,b): sum =0 for i in range(len(a)): sun+=(a[i]-b[i])**2 distance = np.sqrt(sum) return distanceprint('a,b两点之间的欧式距离为：'，euclideann((1,1,2,2),(2,2,4,4)))def euclieann2(a,b): A = np.array(a) B = np.array(b) C = (A-B)**2 distance = np.sqrt(sum(c)) return distanceprint('a,b两点之间的欧式距离为：'，euclidean2((1,1,2,2),(2,2,4,4))) 标准化欧式距离（standardized Euclidean distance）​ 针对欧式距离的缺点做的一种改进。 ​ 标准欧式距离思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差等。假设样本集X的均值（mean）为m ,标准差（standard deviation）为s。 X的“标准化变量”表示为： $X^* = \frac {X-m}s$。 标准化欧式距离公式为： $d_{12} = \sqrt{\sum_{k=1}^n (\frac {x_{1k}-x_{2k}}{s_k})^2}$。 如果将方差的倒数看成一个权重，也称为加权欧式距离(weighted Euclidean distance)。 标准化欧式距离实现： 123456789def euclideans(a,b): sumnum = 0 for i in range(len(a)): avg = (a[i] - b[i]) / 2 si = np.sqrt( (a[i]-avg)**2 + (b[i] - avg)**2 ) sumnum += ((a[i]-b[i]) /si)**2 distance = np.sqrt(sumnum) return distanceprint('a,b两点之间的标准化欧式距离为：'，euclideans((1,2,1,2),(3,3,3,4))) 曼哈顿距离（Manhattan distance）​ “驾驶距离”不是两点间的直线距离， 也称为“城市街区距离（city block distance）” 二维平面上点$a(x_1,y_1),b(x_2,y_2)$间的曼哈顿距离为： $d_{12}=|x_1-x_2| + |y_1-y_2|$。 n维度空间点$a(x_{11},x_{12},…,x_{1n}),b(x_{21},x_{22},…,x_{2n})$间的曼哈顿距离为： $d_{12} = \sum_{k=1}^n |x_{1k}-x_{2k}|$。 二维空间曼哈顿距离实现： 1234def manhattan2(a,b): distance = np.abs(a[0]-b[0])+np.abs(a[1]-b[1]) return distanceprint('二维空间a,b两点之间的曼哈顿距离为：'，manhattan((1,1),(2,2))) 多维空间曼哈顿距离的实现： 12345678910111213141516171819def manhattann(a, b): """ n维空间曼哈顿距离 """ distance = 0 for i in range(len(a)): distance += np.abs(a[i]-b[i]) return distanceprint ('n维空间a, b两点之间的曼哈顿距离为： ', manhattann((1,1,2,2),(2,2,4,4))) def manhattann2(a, b): """ n维空间曼哈顿距离, 不使用循环 """ A = np.array(a) B = np.array(b) distance = sum(np.abs(A-B)) return distanceprint ('n维空间a, b两点之间的曼哈顿距离为： ', manhattann2((1,1,2,2),(2,2,4,4))) 切比雪夫距离（chebyshev distance）​ 国际象棋中，国王可以直行、横行、斜行，所以这个国王走一步可以移动到相邻的8个方格任意一个，国王从格子$(x_1,y_1)$走到格子$(x_2,y_2)$最少需要走多少步，这个距离称为切比雪夫距离。 二维平面上点$a(x_1,y_1),b(x_2,y_2)$间的切比雪夫距离为： $d_{12}=max(|x_1-x_2| + |y_1-y_2|)$。 n维度空间点$a(x_{11},x_{12},…,x_{1n}),b(x_{21},x_{22},…,x_{2n})$间的切比雪夫距离为： $d_{12}=\underset{i}max(|x_{1i}-x_{2i}| )$。 该公式等价于： $d_{12}=\lim\limits_{k\rightarrow\infty}(\sum_{i=1}^{n}\left|x_{1i}-x_{2i}\right|^k)^{\frac{1}{k}}$ 二维切比雪夫距离实现： 1234567def chebyshev2(a, b): """ 二维空间切比雪夫距离 """ distance = max(abs(a[0]-b[0]), abs(a[1]-b[1])) return distanceprint ('二维空间a, b两点之间的欧式距离为： ', chebyshev2((1,2),(3,4))) 多维切比雪夫距离实现： 12345678910111213141516171819202122def chebyshevn(a, b): """ n维空间切比雪夫距离 """ distance = 0 for i in range(len(a)): if (abs(a[i]-b[i]) &gt; distance): distance = abs(a[i]-b[i]) return distanceprint ('n维空间a, b两点之间的切比雪夫距离为：' , chebyshevn((1,1,1,1),(3,4,3,4))) def chebyshevn2(a, b): """ n维空间切比雪夫距离, 不使用循环 """ distance = 0 A = np.array(a) B = np.array(b) distance = max(abs(A-B)) return distance print ('n维空间a, b两点之间的切比雪夫距离为：' , chebyshevn2((1,1,1,1),(3,4,3,4))) 闵科夫斯基距离（minkowski distance）​ 闵氏距离不是一种距离，而是一组距离的定义，是对多个距离公式的概括性的表述。 闵氏距离定义： 两个n维度空间点$a(x_{11},x_{12},…,x_{1n}),b(x_{21},x_{22},…,x_{2n})$间的闵氏距离定义为： $d_{12} = \sqrt[\uproot{10}\leftroot{-2}p]{\sum_{k=1}^n |x_{1k}-x_{2k}|^p}$。 其中p是一个参数： 当$p=1$时，曼哈顿距离 当$p=2$时，欧式距离 当$p \to \infty$时，切比雪夫距离 因此，根据参数不同，闵氏距离可以表示为某一类/种距离。但是存在缺点： 将各个分量的量纲（scale），即“单位”相同看待 未考虑各个分量的分布（期望、方差等）可能是不同的 python实现： 123456789101112131415def minkowski(a, b): """ 闵可夫斯基距离 """ A = np.array(a) B = np.array(b) #方法一：根据公式求解 distance1 = np.sqrt(np.sum(np.square(A-B))) #方法二：根据scipy库求解 from scipy.spatial.distance import pdist X = np.vstack([A,B]) distance2 = pdist(X)[0] return distance1, distance2print ('二维空间a, b两点之间的闵可夫斯基距离为：' , minkowski((1,1),(2,2))[0]) 马氏距离（mahalanobis distance）​ 概念：马氏距离是基于样本分布的一种距离。 ​ 物理意义：在规范化的主成分空间种的欧式距离。 ​ 规范化的主成分空间就是利用主成分分析对一些数据进行主成分分解。在对所有的主成分分解轴做归一化，形成新的坐标轴。由这些坐标轴张成的空间就是规范化的主成分空间。【马氏距离椭圆图】 ​ 定义：有M个样本向量$X_1-X_m$，协方差矩阵记为S，均值记为向量$\mu$,则： 其中的样本向量$X$到$\mu$的马氏距离表示为： ​ $D(X) = \sqrt{(X-\mu)^TS^{-1}(X-\mu)}$。 向量$X_i$与$X_j$之间的马氏距离定义为： ​ $D(X_i,X_j) = \sqrt{(X_i-X_j)^TS^{-1}(X_i-X_j)}$。 若协方差矩阵是单位阵（各个样本向量之间独立同分布），则$X_i$与$X_j$之间的马氏距离等于他们之间的欧式距离： ​ $D(X_i,X_j) = \sqrt{(X_i-X_j)^T(X_i-X_j)}$。 若协方差矩阵式对角矩阵，则是标准化欧式距离。 马氏距离特点： 量纲无关，排除变量之间的相关性干扰 马氏距离的计算是建立在总体样本的基础上的，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，排除这两个总体的协方差矩阵也相同。 计算马氏距离过程中，要求总体的样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况，用欧式距离计算即可。 python实现： 12345678910111213141516171819202122232425262728def mahalanobis (a, b): """ 马氏距离 """ A = np.array(a) B = np.array(b) #马氏距离要求样本数要大于维数，否则无法求协方差矩阵 #此处进行转置，表示10个样本，每个样本2维 X = np.vstack([A,B]) XT = X.T #方法一：根据公式求解 S = np.cov(X) #两个维度之间协方差矩阵 SI = np.linalg.inv(S) #协方差矩阵的逆矩阵 #马氏距离计算两个样本之间的距离，此处共有10个样本，两两组合，共有45个距离。 n = XT.shape[0] distance1 = [] for i in range(0, n): for j in range(i+1, n): delta = XT[i] - XT[j] d = np.sqrt(np.dot(np.dot(delta,SI),delta.T)) distance1.append(d) #方法二：根据scipy库求解 from scipy.spatial.distance import pdist distance2 = pdist(XT,'mahalanobis') return distance1, distance2print ('(1, 2)，(1, 3)，(2, 2)，(3, 1)两两之间的闵可夫斯基距离为：' , mahalanobis((1, 1, 2, 3),(2, 3, 2, 1))[0]) ​ 余弦距离（cosine distance)夹角余弦用于衡量两个向量方向的差异，机器学习中，借用此来衡量样本向量之间的差异。 二维空间上向量$A(x_1,y_1),B(x_2,y_2)$间的夹角余弦公式为： $\cos \theta = \frac{x_1x_2+y_1y_2}{\sqrt{x_1^2+y_1^2}\sqrt{x_2^2+y_2^2}}$ 两个n维度样本点$a(x_{11},x_{12},\dots,x_{1n}),b(x_{21},x_{22},\dots,x_{2n})$间的夹角余弦为： $\cos \theta=\frac{a \cdot b}{|a||b|}$ 即： $\cos \theta = \frac{\sum_{k=1}^n x_{1k} x_{2k}} {\sqrt{\sum_{k=1}^n {x_{1k}}^2} \sqrt{\sum_{k=1}^n {x_{2k}}^2}}$。 夹角余弦取值范围为$[-1,1]$。余弦越大表示两个向量的夹角越小，余弦越小，夹角越大。当两个向量的方向重合时余弦取最大值1，当两个向量的方向完全相反取余弦最小值-1。 二维空间夹角余弦python实现： 1234def cos2(a, b): cos = (a[0]*b[0] + a[1]*b[1]) / (np.sqrt(a[0]**2 + a[1]**2) * np.sqrt(b[0]**2+b[1]**2)) return cosprint ('a,b 二维夹角余弦距离：',cos2((1,1),(2,2))) 多维空间向量的夹角余弦python实现： 123456789101112131415161718192021222324def cosn(a, b): """ n维夹角余弦 """ sum1 = sum2 = sum3 = 0 for i in range(len(a)): sum1 += a[i] * b[i] sum2 += a[i] ** 2 sum3 += b[i] ** 2 cos = sum1 / (np.sqrt(sum2) * np.sqrt(sum3)) return cosprint ('a,b 多维夹角余弦距离：',cosn((1,1,1,1),(2,2,2,2))) def cosn2(a, b): """ n维夹角余弦, 不使用循环 """ A, B = np.array(a), np.array(b) sum1 = sum(A * B) sum2 = np.sqrt(np.sum(A**2)) sum3 = np.sqrt(np.sum(B**2)) cos = sum1 / (sum2 * sum3) return cosprint ('a,b 多维夹角余弦距离：',cosn2((1,1,1,1),(2,2,2,2))) 汉明距离（hamming distance）​ 定义：两个等长的字符串s1与s2的汉明距离为：将其中一个变为另外一个所需要作的最小字符替换次数。 the hamming distance between “1011101” and “1001001” is 2 the hamming distance between “2143896” and “2233796” is 3 ​ 汉明重量：是字符串相对于同样长度的零字符串的汉明距离，即，它是字符串中非零的元素个数。对于二进制数字符串来说，就是1的个数。即11101的汉明重量为4。因此，向量空间中的元素a和b之间的汉明距离等于它们的汉明重量的差a-b。 ​ 应用：信息论、编码理论、密码学等领域都有应用。如在信息编码过程中，为了增强容错性，应使得编码间的最小汉明距离尽可能大。但是，如果比较两个不同长度的字符串，不仅要进行替换，而且要进行插入与删除的运算，这种情况下，通常使用更加复杂的编辑距离等算法。 python实现： 12345678910111213141516171819def hamming(a, b): """ 汉明距离 """ sumnum = 0 for i in range(len(a)): if a[i]!=b[i]: sumnum += 1 return sumnumprint ('a,b 汉明距离：',hamming((1,1,2,3),(2,2,1,3))) def hamming2(a, b): """ 汉明距离, 不使用循环 """ matV = np.array(a) - np.array(b) numsum = len(np.nonzero(matV)[0]) return numsumprint ('a,b 汉明距离：',hamming2((1,1,2,3),(2,2,1,3))) 杰卡德距离（Jaccard distance）​ 杰卡德相似系数（jaccard similarity coeffcient）:两个集合A和B的交集元素在A、B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。 ​ $J(A,B) = \frac{|A∩B|}{|A∪B|}$。 杰卡德相似系数python实现; 123456789def jaccard_coefficient(a, b): """ 杰卡德相似系数 """ set_a = set(a) set_b = set(b) distance = float(len(set_a &amp; set_b)) / len(set_a | set_b) return distanceprint ('a,b 杰卡德相似系数：', jaccard_coefficient((1,2,3),(2,3,4))) 杰卡德距离：与杰卡德相似系数相反，用两个集合中不同元素所占元素的比例来衡量两个集合的区分度： $J_\delta(A,B) = 1- J(A,B) = \frac{|A∪B|-|A∩B|}{|A∪B|} $。 杰卡德距离python实现： 123456789def jaccard_distance(a, b): """ 杰卡德距离 """ set_a = set(a) set_b = set(b) distance = float(len(set_a | set_b) - len(set_a &amp; set_b)) / len(set_a | set_b) return distanceprint ('a,b 杰卡德距离：', jaccard_coefficient((1,2,3),(2,3,4))) 杰卡德相似系数与杰卡德距离的应用： 杰卡德相似系数可以用在衡量样本的相似度上。 样本A和样本B是两个n维向量，而且所有维度的取值都是0或1。将样本看成一个集合，1表示集合包含该元素，0表示集合不包含该元素。 p: 样本A与B都是1的维度的个数 q: 样本A是1，样本B是0的维度的个数 r: 样本A是0，样本B是1的维度的个数 s: 样本A与B都是0的维度的个数 $p+q+r$可以理解为A与B的并集的元素的个数，$p$是A与B的交集的元素的个数。 则样本A与B的杰卡德相似系数可以表示为：$J(A,B) = \frac{p} {p+q+r}$ 样本A与B的杰卡德距离表示为：$1-J(A,B) = \frac{q+r} {p+q+r}$ 相关距离（correlation distance）​ 相关系数(correlation coefficient)：衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y的相关度越高。当X与Y线性相关时，相关系数为1（正线性相关）或-1（负线性相关）。 ​ $\large \rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)}{\sqrt{D(Y)}}} = \frac{E(X-EX)(Y-EY)}{\sqrt{D(X)}{\sqrt{D(Y)}}} $。 ​ 相关距离： ​ $D_{xy} = 1- \large \rho_{XY} $。 相关系数的python实现： 可以利用numpy库中的corrcoef函数来计算，例如：对于矩阵$\rm a,numpy.corrcoef(a)$可计算行与行之间的相关系数，$\rm numpy.corrcoef(a,rowvar =0)$用于计算各列之间的相关系数，输出为相关系数矩阵。 12345678def correlation_coefficient(): """ 相关系数 """ a = np.array([[1, 1, 2, 2, 3], [2, 2, 3, 3, 5], [1, 4, 2, 2, 3]]) print ('a的行之间相关系数为： ', np.corrcoef(a)) print ('a的列之间相关系数为： ', np.corrcoef(a,rowvar=0))correlation_coefficient() 相关距离的python实现（基于相关系数）： 同样对于矩阵a 12345678def correlation_distance(): """ 相关距离 """ a = np.array([[1, 1, 2, 2, 3], [2, 2, 3, 3, 5], [1, 4, 2, 2, 3]]) print ('a的行之间相关距离为： ', np.ones(np.shape(np.corrcoef(a)),int) - np.corrcoef(a)) print ('a的列之间相关距离为： ', np.ones(np.shape(np.corrcoef(a,rowvar = 0)),int) - np.corrcoef(a,rowvar = 0))correlation_distance() 信息熵（Information Entropy）​ 以上的距离度量方法度量的都是两个样本（向量）之间的距离，信息熵描述的时整个系统内部样本之间的一个距离，或者称之为系统内样本分布的集中程度（一致程度）、分散程度、混乱程度（不一致程度）。系统内样本分布越分散（或者说分布越平均），信息熵越大。分布越有序（或者说分布越集中），信息熵越小。 ​ 计算给定样本集X的信息熵公式： ​ $Entropy(X) = \sum_{i=1}^n \large-p_i log_{\small2} \large p_i$。 ​ 参数的含义： ​ $n$:样本集X的分类数 ​ $p_i$:X中的第i类元素出现的概率 ​ 信息熵越大表明样本集S的分布越分散（分布均衡），信息熵越小则表明样本集X的分布越集中（分布不均衡）。 ​ 当S中的n个分类出现的概率一样大时（都是$\frac1n$）,信息熵取最大值$log_2 \large n$。 ​ 当X只有一个分类时，信息熵取得最小值0。 python实现： 1234567891011121314151617181920212223242526272829303132333435def calc_entropy(x): """ 计算信息熵 """ x_value_list = set([x[i] for i in range(x.shape[0])]) ent = 0.0 for x_value in x_value_list: p = float(x[x == x_value].shape[0]) / x.shape[0] logp = np.log2(p) ent -= p * logp return ent def calc_condition_entropy(x, y): """ 计算条件信息熵 """ # calc ent(y|x) x_value_list = set([x[i] for i in range(x.shape[0])]) ent = 0.0 for x_value in x_value_list: sub_y = y[x == x_value] temp_ent = calc_entropy(sub_y) ent += (float(sub_y.shape[0]) / y.shape[0]) * temp_ent return ent def calc_entropy_grap(x,y): """ 计算信息增益 """ base_ent = calc_entropy(y) condition_ent = calc_condition_entropy(x, y) ent_grap = base_ent - condition_ent return ent_grap 具体涉及到数学知识参考此条笔记 聚类算法 参考周志华《机器学习》 概念聚类是一种无监督学习方法。 无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构和规律。 聚类直观上是将相似的样本聚在一起，从而形成一个类簇(cluster)，通过距离度量来评判相似性。对应到多维样本，每个样本可以对应于高维空间中的一个数据点，如果距离相近，则称相似。聚类结果的好坏，需要性能度量。 距离度量度量距离需要满足以下性质： 非负性：$\rm dist(x_i,x_j) \geq 0$ 同一性：$\rm dist(x_i,x_j) =0$当且仅当$x_i = x_j$ 对称性：$\rm dist(x_i,x_j) = dist(x_j,x_i)$ 直递性：$\rm dist(x_i,x_j) \leq dist(x_i,x_k) +dist(x_k,x_j)$，三角形不等式，两边之和大于第三边。 最常用的距离度量方法是闵科夫斯基距离： $\rm {dist_{mk}} (x_i,x_j) = \left( \sum\limits_{u=1}^n \left| x_{iu} -x_{ju}\right|^p\right)^{\frac{1}{p}}$ 当$p=1$时，闵科夫斯基距离即曼哈顿距离： $\rm {dist_{mk}} (x_i,x_j) =||x_i - x_j||_1 = \sum\limits_{u=1}^n |x_{iu} -x_{ju}|$ 当$p=2$时，闵科夫斯基距离即欧式距离： $\rm dist_{ed}$$(x_i,x_j) = ||x_i-x_j||_2 = \sqrt{\sum\limits_{u=1}^n |x_{iu} - x_{ju}|^2}$ 属性分为两种：连续属性和离散属性(取值有限个)。 对于连续属性：一般可以被学习器所用，有时会更具具体的情形作相应的预处理，例如：归一化等。 对于离散属性，需要做如下处理： 若属性值之间存在序关系，则可以将其转化为连续值。如：身高属性“高”“中”“矮”,可转化为{1，0.5，0}。 若属性之间不存在序关系，则可以将其转化为向量形式。如：性别属性“男”“女”，可以转化为{（1，0），（0，1）}。 在进行距离度量时，连续属性和存在序关系的离散属性都可以直接参与计算，都可以反应一种程度，称之为“有序属性”。 对于”无序属性”，一般采用VDM进行距离计算。如：对于离散属性的两个取值a和b，定义： $VDM_p(a,b) = \sum\limits_{i=1}^k \left| \frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}}\right|^p$，其中$i$表示类簇。 所以，在计算两个样本之间的距离时，可以将闵科夫斯基距离和VDM混合在一起计算 ，即： $MinkovDM_p(\pmb x_i,\pmb x_j) = \left( \sum\limits_{u=1}^{n_c}\left|x_{iu}-x_{ju}\right|^p - \sum\limits_{u=n_c+1}^n VDM_p(x_{iu},x_{ju}) \right)^{\frac{1}{p}}$ 其中式子括号内左边部分表示有序属性，右边部分表示无序属性。 通过距离度量相似性，距离越小，相似性越大，反之距离越大，相似性越小。 如果距离的度量方法不满足前面所说的四个基本属性，这样的方法称为：非距离度量（non-metric distance）。 性能度量外部指标将聚类结果与某个参考模型的结果进行比较，以参考模型的输出作为标准，来评价聚类好坏。假设聚类给出的结果为$\lambda$，参考模型输出的结果为$\lambda’$，将样本两两配对，定义： $a=|SS|,SS=\{(\pmb x_i,\pmb x_j) | \lambda_i = \lambda_j,\lambda’_i = \lambda’_j,i&lt;j\}$,参考结果同类簇，聚类结果同类簇。 $a=|SD|,SD=\{(\pmb x_i,\pmb x_j) | \lambda_i = \lambda_j,\lambda’_i \neq \lambda’_j,i&lt;j\}$,参考结果不同类簇，聚类结果同类簇。 $a=|DS|,DS=\{(\pmb x_i,\pmb x_j) | \lambda_i \neq \lambda_j,\lambda’_i = \lambda’_j,i&lt;j\}$,参考结果同类簇，聚类结果不同类簇。 $a=|DD|,DD=\{(\pmb x_i,\pmb x_j) | \lambda_i \neq \lambda_j,\lambda’_i \neq \lambda’_j,i&lt;j\}$,参考结果不同类簇，聚类结果不同类簇。 $a,b$代表着聚类结果好坏的正能量，$b,c$表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标： Jaccard系数(Jaccard Coefficient，JC)：$JC= \frac{a}{a+b+c}$ FM指数(Fowlkes and Malloes Index，FMI)：$FMI = \sqrt{\frac{a}{a+b}·\frac{a}{a+c}}$ Rand指数(Rand Index，RI)：$RI = \frac{2(a+d)}{m(m-1)}$ 这三个评价指标取值范围（0，1）,取值越大越好。 内部指标不依赖任何外部模型，直接对聚类结果进行评估，聚类的目的是将那些相似的样本尽可能地聚在一起，不相似地样本尽可能分开。定义为： $\rm avg(C)$$= \frac{2}{|C|(|C|-1)} \sum\limits_{1\le i&lt;j\le|C|}$$\rm dist$$(\pmb x_i,\pmb x_j)$，簇内平均聚类，越小越好。 $\rm diam$$(C)=\rm max$$_{1\le i&lt;j\le|C|}$$\rm dist$$(\pmb x_i,\pmb x_j)$，簇内最大距离，越小越好。 $d_{min}(C_i,C_j) = min_{\pmb x_i \in C_i,\pmb x_j \in C_j} \rm dist$$(\pmb x_i,\pmb x_j)$，簇间最小距离，越大越好。 $d_{cen}(C_i,C_j) = \rm dist$$(\pmb \mu_i ,\pmb \mu_j)$，簇中心距离，越大越好。 基于上面四个距离，可以导出下面常用的内部评价指标： DB指数(Davies-Bouldin Index，DBI)：$DBI=\frac{1}{k} \sum\limits_{i=1}^k \rm max_{\substack{j \ne i}} \left( \frac{avg(C_i)+avg(C_j)}{d_{cen}(\pmb \mu_i,\pmb \mu_j)}\right)$，越小越好。 Dunn指数(Dunn Index，DI)：$DI = \min\limits_{1\le i \le k} \left( \min\limits_{j \ne i}\left( \frac{d_{min}(C_i,C_j)}{\max\limits_{1 \le l \le k}diam (C_l)}\right) \right)$，越大越好。 原型聚类基于原型的聚类，即通过参考一个模板向量或者模板分布的方式完成聚类过程，常见的K-Means即是基于簇中心来实现聚类，混合高斯聚类是基于簇分布来实现聚类。 K-means首先随机指定类中心，根据样本与类中心的远近划分分类簇，接着重新计算类中心，迭代直至收敛。 将样本的类别看作隐变量(latent variable)，类中心看作为样本的分布参数，这一过程通过EM算法的两步走策略而计算出，目的是最小化平方误差函数E： $E=\sum\limits_{i=1}^k \sum\limits_{x\in C_i}||x-\mu_i||_2^2$ 算法流程如下： 学习向量量化(LVQ)LVQ也是基于原型的聚类算法，与K-Means不同，LVQ使用样本真实类标记辅助聚类。 LVQ根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个原型特征向量组，然后从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选择距离最小的原型向量所在的类簇作为其划分结果，再与真实类标比较。 若划分结果正确，则对应的原型向量向这个样本靠近一些；若划分结果不正确，在对应原型向量向这个样本远离一些。 高斯混合聚类K-Means和LVQ都是试图以类中心作为原型指导聚类，高斯混合聚类采用高斯分布来描述原型。 假设每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由$k$个多维高斯分布混合而成。 多维高斯分布的概率密度函数： $p(\pmb x) = \frac{1}{(2\pi)^{\frac{n}{2}} |\sum|^{\frac{1}{2}}} e^{ -\frac{1}{2}(\pmb x-\pmb \mu)^T \sum^{-1}(\pmb x- \pmb \mu) }$ 其中$\mu$表示均值向量，$\sum$表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。 定义高斯混合分布： $p_{\mathcal{M}}(\pmb x) = \sum \limits_{i=1}^k \alpha_i ·p(\pmb x| \pmb \mu_i, \Sigma_i)$ 其中$\alpha$称为混合系数，这样空间中样本的采集过程可以抽象为： （1）选择一个类簇（高斯分布） （2）再根据相应的高斯分布的密度函数进行采样 此时结合贝叶斯公式，有： $p_{\mathcal{M}}(z_j=i|\pmb x_j) = \frac{P(z_j =i)·p_{\mathcal{M}}(\pmb x_j| z_j =i)}{p_{\mathcal{M}}(\pmb x_j)} = \frac{\alpha_i ·p(\pmb x_j |\pmb \mu_i,\Sigma_i)}{\sum\limits_{l=1}^k \alpha_l·p(\pmb x_j| \pmb \mu_l, \Sigma_l)}$ 此时只需要选择$p_{\mathcal{M}}$最大时的类簇并将该样本划分到其中，与贝叶斯分类类似，都是通过贝叶斯公式展开，然后计算类先验概率$P(z_j=i)$和类条件概率$p_{\mathcal{M}}(\pmb x_j|z_j = i)$。 但是这里面没有真实类标信息，所以不能像贝叶斯分类那样通过最大似然法 计算出来，这里的样本可能属于所有的类簇，这里的似然函数变为： $LL(D)=\ln \left( \prod \limits_{j=1}^m p_{\mathcal{M}}(\pmb x_j)\right) = \sum\limits_{j=1}^m \ln \left( \sum\limits_{i=1}^k \alpha_i ·p(\pmb x_j|\pmb \mu_i,\Sigma_i)\right)$ 这里使用简单的最大似然法无法求出所有的参数，需要使用$EM$算法： 首先对高斯分布参数及混合系数进行随机初始化，计算出各个$p_{\mathcal{M}}$（即$\gamma_{ji}$,第$i$个样本属于$j$类），再最大化似然函数（即$LL(D)$分别对$\alpha,\mu,\Sigma$求偏导)，然后对参数进行迭代更新。 $\pmb \mu_i = \frac{\sum\limits_{j=1}^m \gamma_{ji}\pmb x_j}{\sum\limits_{j=1}^m \gamma_{ji}}$ ,$\Sigma_i = \frac{\sum\limits_{j=1}^m \gamma_{ji}(\pmb x_j-\pmb \mu_i)(\pmb x_j-\pmb \mu_i)^T}{\sum\limits_{j=1}^m \gamma_{ji}}$,$\alpha_i = \frac{1}{m}\sum\limits_{j=1}^m \gamma_{ji}$ 具体算法流程如下： 密度聚类基于密度的聚类，从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展类簇。其中最著名的是DBSCAN算法，首先定义以下几个概念： $\epsilon-$领域：对于$x_j \in D$，其$\epsilon-$领域包含样本集$D$中与$x_j$的距离不大于$\epsilon$的样本，即$N_{\epsilon}(x_j) = \{ x_i \in D| \rm dist(x_i,x_j) \le \epsilon\}$; 简单理解DBSCAN就是：找出一个核心对象所有密度可达的样本集合形成簇。 首先从数据集中任意选择一个核心对象$A$，找出所有$A$密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。 DBSCAN算法的流程如下： 层次聚类是一种基于树形结构的聚类方法，常用是自底向上的结合策略（AGNES算法）。 假设有$N$个待聚类的样本，基本步骤如下： 初始化：把每一个样本归为一类，计算每两个类之间的距离，即样本与样本之间的相似度 寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个） 重新计算新生成的这个类与各个旧类之间的相似度 重复2，3直到所有的样本点都归为一类，结束。 这里面最关键的一步就是计算两个类簇的相似度，这里有多种度量方法 ： 单链接(single-linkage)：取类间最小距离。即：$d_{min} (C_i,C_j) = \min \limits_{x\in C_i,z\in C_j} \rm dist(x,z)$ 全链接(complete-linkage): 取类间最大距离。即：$d_{max} (C_i,C_j) = \max \limits_{x\in C_i,z\in C_j} \rm dist(x,z)$ 均链接(average-linkage): 取类间两两的平均距离。即：$d_{avg} (C_i,C_j) = \frac{1}{|C_i||C_j|} \sum\limits_{x\in C_i} \sum\limits_{z\in C_j} \rm dist(x,z)$ 可以看出：单链接包容性强，稍微有点联系就划为一类，全链接要求最为严格，均链接从全局出发。 层次聚类算法流程如下： 贝叶斯决策理论 参见《模式识别》（边肇祺） 概念 bayes公式：$P(A|B)·P(B) =P(B|A)·P(A)$ 类别$w \in \{w_1,w_2\}$ 样本$X$ 状态先验概率$P(w_i),i=1,2$ 类条件概率密度$p(\pmb x|w_i),i=1,2$ 状态后验概率$p(w_i|\pmb x)$ 所以有：$p(X|w_i)·P(w_i) = p(w_i|X)·P(X),i=1,2$ $\implies p(w_i|\pmb x) = \frac{p(\pmb x|w_i)·P(w_i)}{\sum_{j=1}^2p( \pmb x|w_j)P(w_j)}$ Bayes本质 观察$\pmb x$（被识别的d维特征的测量），把状态的先验概率$P(w_i)$转化为状态的后验概率$p(w_i|\pmb x)$。 最小错误率bayes决策规则若$p(w_1|\pmb x)&gt;p(w_2|\pmb x)$，则把$\pmb x$归为状态$w_1$; 若$p(w_1|\pmb x)&lt;p(w_2|\pmb x)$，则把$\pmb x$归为状态$w_2 $。 几种等价形式： $若p(w_i|\pmb x)=\underset{j=1,2}{max} p(w_j|\pmb x),则\pmb x \in w_i$ $若p(\pmb x | w_i)P(w_i)=\underset{j=1,2}{max} p(\pmb x |w_j)P(w_j),则\pmb x \in w_i$ $若l(\pmb x) = \frac{p(\pmb x|w_1)}{p(\pmb x|w_2)} \underset{&lt;}{ &gt;} \frac{P(w_2)}{P(w_1)}，则 \pmb x \in \cases{w_1\\w_2}$ ,其中$l(\pmb x)$称为似然比。$\frac{P(w_2)}{P(w_1)}$表示似然比阈值 $若h(\pmb x) =-ln[l(x)] = -ln[ p(\pmb x|w_i)] + ln[p(\pmb x|w_2)] \underset{&gt;}{&lt;} ln \frac{P(w_1)}{P(w_2)},则 \pmb x \in \cases{w_1\\w_2}$ 【说明】： Bayes进行分类时要求： ​ 各类别的总体的概率分布$P(w)$是已知的； ​ 要决策的分类的类别是一定的 e.g. 某局部地区的细胞识别中正常为$w_1$和异常$w_2$两类，先验概率分别是$\cases{正常状态：P(w_1)=0.9\\异常状态：P(w_2) =0.1}$。 有一个待测的细胞：观察值为$\pmb x$，从类条件概率密度分布曲线上查得 $p(\pmb x|w_1)=0.2,p(\pmb x| w_2) =0.4$ 解： 由$ p(w_1|\pmb x) = \frac{p(\pmb x|w_1)·P(w_1)}{\sum_{j=1}^2p( \pmb x|w_j)P(w_j)} =\frac{0.2\times0.9}{0.2\times0.9+0.4\times0.1} =0.818$ $p(w_2|\pmb x) = 1- p(w_1| \pmb x) =0.182$ $p(w_1|\pmb x)&gt; p(w_2|\pmb x)$，所以合理的决策是把$\pmb x$归类于正常状态$w_1$。 平均错误率以$P(e)$表示，定义为： $P(e) = \int_{-\infty}^{\infty} P(e,\pmb x)d \pmb x =\int_{-\infty}^{\infty} P(e|\pmb x) p(\pmb x) d \pmb x$ 其中： $\int_{-\infty}^{\infty} () d \pmb x$表示在整个d维特征空间上的积分。 对于两类别问题，根据最小概率bayes决策规则可知： $若P(w_2|\pmb x)&gt;P(w_1| \pmb x),决策应该为w_1,此时做出决策错误概率为P(w_1|\pmb x)。反之应为P(w_2|\pmb x)$。 可表示为： $P(e| \pmb x) = \cases{P(w_1|\pmb x),当P(w_2|\pmb x)&gt;P(w_1| \pmb x) \ P(w_2|\pmb x),当P(w_1|\pmb x)&gt;P(w_2| \pmb x)}$ 如果令$t$表示两类的分界面，将特征空间$\mathbb{R}$划分为两类$\mathbb{R_1} （ -\infty,t）,\mathbb{R_2} （ t,\infty）$，这样就有：$$\begin{equation}\begin{aligned}P(e) &amp;=\int_{-\infty}^{t} P(w_2|\pmb x) p(\pmb x) d \pmb x +\int_{t}^{\infty} P(w_1|\pmb x) p(\pmb x) d \pmb x \\&amp;= \int_{-\infty}^{t} p(\pmb x|w_2) P(w_2) d \pmb x +\int_{t}^{\infty} p(\pmb x| w_1) P(w_1) d \pmb x\\&amp;=P(\pmb x \in \mathbb{R_1},w_2)+P(\pmb x \in \mathbb{R_2},w_1)\\&amp;=P(\pmb x \in \mathbb{R_1}|w_2)P(w_2)+P(\pmb x \in \mathbb{R_2}|w_1)P(w_1)\\&amp;=P(w_2) \int_{\mathbb{R_1}}p(\pmb x|w_2)d\pmb x +P(w_1) \int_{\mathbb{R_2}}p(\pmb x|w_1)d\pmb x \\&amp;=P(w_2)P_2(e) +P(w_1)P_1(e)\end{aligned}\end{equation}$$可见： 最小错误率的决策规则实际上是对每个$\pmb x$都使得$P(e|\pmb x)$取得最小者，这就使得 $P(e) = \int_{-\infty}^{\infty} P(e,\pmb x)d \pmb x =\int_{-\infty}^{\infty} P(e|\pmb x) p(\pmb x) d \pmb x$取得最小，即 使得平均错误率$P(e)$达到最小。 多类决策过程假设有c类别，即最小错误率贝叶斯决策规则： $若p(w_i|\pmb x)=\underset{j=1,2,\dots,c}{max} p(w_j|\pmb x),则\pmb x \in w_i$ $若p(\pmb x | w_i)P(w_i)=\underset{j=1,2,\dots,c}{max}p(\pmb x |w_j)P(w_j),则\pmb x \in w_i$ 多类别决策过程中，把特诊空间分割为$\mathbb{R_1},\mathbb{R_2},\dots,\mathbb{R_c}$个区域，可能错分的情况很多，平均错误概率$P(e)$将由$c(c-1)$项组成。即：$$\begin{equation} \begin{aligned}P(e)&amp;=[P(\pmb x \in \mathbb{R_2}|w_1)+P(\pmb x \in \mathbb{R_3}|w_1) + \dots +P(\pmb x \in \mathbb{R_c}|w_1) ]P(w_1)\\&amp;+[P(\pmb x \in \mathbb{R_1}|w_2)+P(\pmb x \in \mathbb{R_3}|w_2) + \dots +P(\pmb x \in \mathbb{R_c}|w_2) ]P(w_2)\\ \vdots\\&amp;+[P(\pmb x \in \mathbb{R_1}|w_c)+P(\pmb x \in \mathbb{R_2}|w_c) + \dots +P(\pmb x \in \mathbb{R_{c-1}}|w_c) ]P(w_c)\\&amp;= \sum_{\substack{i=1}}^c \sum_{\substack{i=1\\j \neq i}}^c [P(\pmb x \in \mathbb{R_j}|w_i)]P(w_i)\end{aligned} \end{equation}$$直接求$P(e)$计算量很大，代之以计算平均正确分类概率$P(c)$表示，即： $P(e) =1-P(c) = 1- \sum_{j=1}^c P(\pmb x \in \mathbb{R_j}|w_j)P(w_j) =1-\sum_{j=1}^c \int_{\mathbb{R_i}} p(\pmb x|w_j)P(w_j)d \pmb x$ 最小风险的贝叶斯决策是考虑到各种错误造成的损失不同而提出的决策规则。 用决策论的观点进行讨论： 在决策论中采取的行动称为决策或行动，所有可能的各种决策组成的集合称为决策空间或行动空间,记为$\Lambda$ 观察$\pmb x是d维随机向量： \pmb x =[x_1,x_2,\dots,x_d]^T$,其中$x_1,x_2,\dots,x_d为一维随机变量。$ 状态空间$\Omega ，由c个自然状态（c类）组成。即，\Omega =\{w_1,w_2,\dots,w_c\}$ 决策空间由$\alpha 个决策\alpha_i，i=1,2,\dots,a$组成，即$\Lambda = \{\alpha_1.\alpha_2,\dots,\alpha_a\}$【注:这里的$a不同于c，因为除了c个类别有c种不同的决策外$还有可能采取其他决策，比如“拒绝”的决策，此时$a=c+1$】 损失函数$\lambda(\alpha_i,w_j),i=1,2,\dots,a;j=1,2,\dots,c$。表示当真实状态为$w_j$时而采取的决策$\alpha_i$时所带来的损失。 条件期望损失$R(\alpha_i| \pmb x)$表示，定义为： 给定的$\pmb x$，如果采取决策$\alpha_i$，损失$\lambda 可以在c个\lambda(\alpha_i,w_j)，j=1,2,\dots,c$中任取一个，其相应的概率为$P(w_j|\pmb x)$。 在采取决策$\alpha_i$情况下的条件期望损失为： $R(\alpha_i|\pmb x)=E[\lambda(\alpha_i,w_j)] =\sum_{j=1}^c \lambda(\alpha_i,w_j)P(w_j| \pmb x)，i=1,2,\dots,a$ 也称为条件风险。 期望风险由于$\pmb x$是随机向量的观察值，所以，对于不同的$\pmb x$采取的决策$\alpha_i$时，其条件风险不同。把决策$\alpha$看成随机向量$\pmb x$的函数，记为$\alpha(\pmb x)$，定义期望风险$R$。 $R=\int R(\alpha(\pmb x)|\pmb x)p(\pmb x)$，其中$d\pmb x$是$d$维度特征空间上的体积元，积分是在整个特征空间上进行。 【说明】 $\cases{期望风险R反应的是整个特征空间上所有 \pmb x的取值采取相应的决策\alpha( \pmb x)所带来的平均风险。\\条件风险R(\alpha_i |x)只反应某一个随机变量x的取值，采取相应的决策\alpha_i所带来的风险。}$ 决策规则如果$R(\alpha_k|\pmb x) = \underset{i=1,\dots,a}{min} R(\alpha_i| \pmb x),则\alpha= \alpha_k$ 步骤如下： (1)已知$P(w_j),p(\pmb x| w_j),j=1,2,\dots,c及待识别的\pmb x的情况下$，根据贝叶斯公式计算出后验概率： $p(w_j|\pmb x) = \large{\frac{p(\pmb x|w_j)·P(w_j)}{\sum_{i=1}^cp( \pmb x|w_i)P(w_i)}}，j=1,2,\dots,c$ (2)利用后验概率和决策表，计算出采取$\alpha_i,i=1,2,\dots,a$决策的条件风险$R(\alpha_i | \pmb x) ,i=1,2,\dots,a$ $R(\alpha_i|\pmb x)=E[\lambda(\alpha_i,w_j)] =\sum_{j=1}^c \lambda(\alpha_i,w_j)P(w_j| \pmb x)，i=1,2,\dots,a$ (3)对(2)中得到的$a$个条件风险值$R(\alpha_i | \pmb x) ,i=1,2,\dots,a$进行比较，找出使条件风险最小的决策$\alpha_k$，即： $R(\alpha_k| \pmb x) = \underset{i=1,2,\dots,a}{min} R(\alpha_i| \pmb x)$ 则$\alpha_k$就是最小风险饿贝叶斯决策。 e.g.给定决策表，按最小风险贝叶斯决策进行分类。 决策损失状态 $w_1$ $w_2$ $\alpha_1$ 0 6 $\alpha_2$ 1 0 已知条件： $P(w_1)=0.9,P(w_2)=0.1;p(\pmb x| w_1) = 0.2,p(\pmb x| w_2) = 0.4;$ $\lambda_{11} =0,\lambda_{12} =6,\lambda_{21} =1,\lambda_{22} =0$ 根据贝叶斯公式得到： $P(w_1|\pmb x) =0.818,P(w_2| \pmb x) = 0.182$ 根据$R(\alpha_i|\pmb x)=E[\lambda(\alpha_i,w_j)] =\sum_{j=1}^c \lambda(\alpha_i,w_j)P(w_j| \pmb x)，i=1,2,\dots,a$得到： $R(\alpha_1|\pmb x)=\sum_{j=1}^2 \lambda_{1j}P(w_j| \pmb x)=\lambda_{12}P(w_2| \pmb x) = 1.092$ $R(\alpha_2|\pmb x)=\lambda_{21}P(w_1| \pmb x) = 0.818$ ∵$R(\alpha_1| \pmb x) &gt; R(\alpha_2 | \pmb x)$ ∴决策为$w_2 $的风险小于决策为$w_1$的风险，因此采取决策行$\alpha_2 $，即判断$\pmb x \in w_2$。 【说明】: 最小风险贝叶斯决策要求： $ \cases{ 先验概率P(w_j)已知\\类条件概率密度p(\pmb x| w_j),j=1,2,\dots,c已知\\合适的损失函数\lambda(\alpha_i,w_j),i=1,2,\dots,a;j=1,2,\dots,c}$ 合适的决策表需要根据所研究的具体问题来决定。 最小错误率和最小风险Bayes决策关系设损失函数为：$$\begin{equation}\lambda(\alpha_i,w_j)=\begin{cases}1: i = j\\0: {i \neq j}\end{cases} \qquad i,j =1,2,\dots,c\end{equation}$$称为0-1损失函数。不考虑”拒绝”情况 则有： $R(\alpha_i|\pmb x)=E[\lambda(\alpha_i,w_j)] =\sum_{j=1}^c \lambda(\alpha_i,w_j)P(w_j| \pmb x)=\sum_{j=1\\j \neq i}^c P(w_j| \pmb x)$ 其中： $\sum_{j=1\\j \neq i}^c P(w_j| \pmb x)$表示对$\pmb x 采取决策 w_i 的条件错误概率$。在0-1损失函数时，使得： 最小风险贝叶斯决策$R(\alpha_k| \pmb x) = \underset{i=1,2,\dots,a}{min} R(\alpha_i| \pmb x)$ $\iff \sum_{j=1\\j \neq i}^c P(w_j| \pmb x) = \underset{i=1,2,\dots,a}{min} R(\alpha_i| \pmb x)$ 等价于最小错误率贝叶斯决策。 即： 最小错误率贝叶斯决策就是在0-1损失函数条件下的最小风险贝叶斯决策，前者是后者的一个特例。 Neyman-Pearson准则又称为 ： 在限定一类错误率$\varepsilon_2$为常数而使得另一类错误率$\varepsilon_1$最小的决策规则。 可看成： 在$P_2(e) = \varepsilon_0$条件下，求$P_1(e) $的极小值的条件极值问题。可以用Lagrande乘子法，建立数学模型为： $\gamma = P_1(e) + \lambda(P_2(e)- \varepsilon_0)，其中\lambda 是拉格朗日乘子，目的是求\gamma的极小值。$ 已知： $P_1(e)= \int_{\mathbb{R_2}}p(\pmb x|w_1)d\pmb x ;P_2(e)= \int_{\mathbb{R_1}}p(\pmb x|w_2)d\pmb x$ $\mathbb{R_1} +\mathbb{R_2} = \mathbb{R},划分整个特征空间$，分界点（面）$t$，若被识别的样本$\pmb x落入 \mathbb{R_1}，属于w_1类，否则属于w_2类。$ 根据类条件概率密度的性质有： $ \int_{\mathbb{R_2}}p(\pmb x|w_1)d\pmb x =1- \int_{\mathbb{R_1}}p(\pmb x|w_1)d\pmb x$ 所以有： 所以Neyman-Pearson决策规则为： $ \frac{p(\pmb x|w_1)}{p(\pmb x|w_2)} \underset{&lt;}{ &gt;} \lambda ,则 \pmb x \in \cases{w_1\\w_2}$ 高维情况分析在高维情况下，求解边界面不容易，此时可以利用似然比密度函数来确定$\lambda$的值。即： 似然比$l(\pmb x) = \frac{p(\pmb x|w_1)}{p(\pmb x|w_2)} $ 似然密度函数$p(l|w_2) $: ​ $P_2(e) = 1- \int_0^{\lambda}p(l|w_2) dl = \varepsilon_0 $ 可见： $ p(l|w_2) \ge 0,P_2(e) 是 \lambda 的单调函数，即，当\lambda =0时，P_2(e) =1;当\lambda → \infty 时，P_2(e) =0$ 采用试探法，对几个$\lambda $的值计算出$P_2(e)$后，可以找到一个合适的$\lambda$的值，刚好能满足$P_2(e) = \varepsilon_0$的条件，又使得$P_1(e) $尽可能的小。 Neyman-Pearson决策规则与最小错误率Bayes决策规则对比两者都是以似然比为基础的 不同的是： 最小错误率贝叶斯决策规则： $若l(\pmb x) = \frac{p(\pmb x|w_1)}{p(\pmb x|w_2)} \underset{&lt;}{ &gt;} \frac{P(w_2)}{P(w_1)}，则 \pmb x \in \cases{w_1\\w_2}$ ,其中$l(\pmb x)$称为似然比。$\frac{P(w_2)}{P(w_1)}$表示似然比阈值 Neyman-Pearson决策规则： $ \frac{p(\pmb x|w_1)}{p(\pmb x|w_2)} \underset{&lt;}{ &gt;} \lambda ,则 \pmb x \in \cases{w_1\\w_2}$，其中用的阈值是Lagrange乘子$\lambda$。 最小风险贝叶斯决策规则也可以写成似然比形式： $ \frac{p(\pmb x|w_1)}{p(\pmb x|w_2)} \underset{&lt;}{ &gt;} \frac{(\lambda_{12}-\lambda_{22})P(w_2)}{(\lambda_{21}-\lambda_{11})P(w_1)}$ 其中: $\lambda_{11} $——$当\pmb x \in w_1时，决策为\pmb x \in w_1的损失$ $\lambda_{21} $——$当\pmb x \in w_1时，决策为\pmb x \in w_2的损失$ $\lambda_{22} $——$当\pmb x \in w_2时，决策为\pmb x \in w_2的损失$ $\lambda_{12} $——$当\pmb x \in w_2时，决策为\pmb x \in w_1的损失$ 最小最大决策考虑$P(w_i)$变化的情况下，如何使得最大可能风险为最小，即在最差的条件下争取最好的结果。 对于两类问题，假定损失函数： $\lambda_{11} $——$当\pmb x \in w_1时，决策为\pmb x \in w_1的损失$ $\lambda_{21} $——$当\pmb x \in w_1时，决策为\pmb x \in w_2的损失$ $\lambda_{22} $——$当\pmb x \in w_2时，决策为\pmb x \in w_2的损失$ $\lambda_{12} $——$当\pmb x \in w_2时，决策为\pmb x \in w_1的损失$ 通常做出错误决策总比做出正确决策所带来的损失要大，即 $\lambda_{21} &gt;\lambda_{11}; \lambda_{12} &gt;\lambda_{22}$ 假定决策域$\mathbb{R_1},\mathbb{R_2}$已确定，则风险为： $R= \int R(\alpha(\pmb x) |\pmb x)p(\pmb x) d\pmb x =\int_{\mathbb{R_1}} R(\alpha_1(\pmb x) |\pmb x)p(\pmb x) d\pmb x+\int_{\mathbb{R_2}} R(\alpha_2(\pmb x) |\pmb x)p(\pmb x) d\pmb x$ 我们可以在（0，1）区间内，对先验概率$P(w_1)$取若干个不同的值，分别按最小风险概率贝叶斯决策规则确定对应的决策域，从而计算出相应的最小风险R，这样可以得到关于最小贝叶斯风险R与先验概率$P(w_1)$的关系曲线。 找到决策域使得系数$b=0$，即： $b=(\lambda_{11}-\lambda_{22}) + (\lambda_{21}-\lambda_{11}) \int_{\mathbb{R_2}}p(\pmb x|w_1)d\pmb x - (\lambda_{12}-\lambda_{22}) \int_{\mathbb{R_1}}p(\pmb x|w_2)d\pmb x=0$ 这样风险变为： $R=\lambda_{22} +(\lambda_{12}-\lambda_{22}) \int_{\mathbb{R_1}}p(\pmb x|w_2)d\pmb x=a$ 该式子表示的直线平行于$P(w_1)$的坐标轴，这样无论$P(w_1)$怎么变化，风险$R$都不会发生变化，其最大风险为$a$,这时最大风险最小。 综上所述： 在作最小风险贝叶斯决策时，若考虑先验概率$P(w_1)$可能改变或者对先验概率毫无所知的情况下，应选择最小贝叶斯风险$R’$为最大值时的$P’(w_1)$来设计分类器。 因为此时，风险最大，能保证在不管$P(w_1)$怎么变化时，使得最大风险降为最小。——最大最小决策。 即： 最小最大决策的任务是找到使得贝叶斯风险为最大时的决策域$\mathbb{R_1},\mathbb{R_2}$,这也对应方程： $b=(\lambda_{11}-\lambda_{22}) + (\lambda_{21}-\lambda_{11}) \int_{\mathbb{R_2}}p(\pmb x|w_1)d\pmb x - (\lambda_{12}-\lambda_{22}) \int_{\mathbb{R_1}}p(\pmb x|w_2)d\pmb x=0$ 的解。 这是一种偏于保守的方法。 序贯分类方法考虑到，获取了$k$个特征（$k&lt;d$）就做判决分类更为合理，因为其余$d-k$个特征加入会使得分类错误的降低而造成的代价减小补偿不了这些特征花费的代价。 解决办法是: 先用一部分的特征来分类，逐步加入特征以减少分类损失，然后每步都要衡量加入新特征所花费的代价与所降低分类损失的大小，以便于决定是否继续再加新特征还是停止。 这种方法的计算量和存储容量要求大，因此发展了一系列次优的序贯方法，主要是： 假定在第$k$步做出决策时只考虑到$k+v$步，即决策一定停止在第$k$步和$k+v$之间。此外还有进行特诊的排序 。 分类器的设计处理的主要问题是:应用这些决策规则对观察向量$\pmb x$进行分类。 几个名词概念： 决策面：对于$c类问题$，按照决策规则可以把$d$维特征空间划分成$c$个决策域，将划分决策域的边界面称为决策面。 判别函数：用于表达决策规则的某些函数称为判断函数。 多类情况定义一组判别函数$g_i(\pmb x) ,i=1,2,\dots,c$用于表示多类决策规则。 （1）判别函数 如果对于$g_i(\pmb x) &gt; g_j(\pmb x),在所有j \neq i都成立，则将\pmb x 归于w_i类$。因此可以定义为： $g_i(\pmb x) =P(w_i| \pmb x)$ $g_i(\pmb x) =p( \pmb x | w_i)P(w_i)$ $g_i(\pmb x) =ln p(\pmb x| w_i) +ln P(w_i)$ 更一般的可以定义为：$f(p(w_i| \pmb x)) +h(\pmb x)，其中f()为任一单调增函数$。 （2）决策面方程 各区域$\mathbb{R_i}$被决策面分割，这些决策面是特征空间中的超曲面，相邻两个决策域在决策面上的判断函数值是相等的，即： $若\mathbb{R_i} 和\mathbb{R_j}$相邻的，则分割它们的决策面方程满足$g_i(\pmb x) = g_j(\pmb x)$。 （3）分类器设计 功能是先计算出$c个判别函数g_i，$再从中选择对应于判别函数为最大值的类作为决策结果。 两类情况（1）判别函数 定义为：$g(\pmb x) = g_1(\pmb x) -g_2(\pmb x)$ 决策规则可以表示为：$\cases{g(\pmb x)&gt;0,则决策w_1\\g(\pmb x)&lt;0,则决策w_2}$ 可定义出如下的判别函数： $g(\pmb x) = P(w_1|\pmb x)-P(w_2|\pmb x)$ $g(\pmb x) = p(\pmb x| w_1)P(w_1)- p(\pmb x|w_2)P(w_2)$ $g(\pmb x) = ln \frac{p(\pmb x| w_1)}{p(\pmb x| w_2)}-ln \frac{P(w_1)}{P(w_2)}$ （2）决策面方程 ​ $g(\pmb x) =0$ 一般来说，$\pmb x$为一维时，决策面为一分界点；二维时，决策面为一曲面；三维时，是一曲面，$\pmb x是d维$时，决策面时一超曲面。 （3）分类器设计 计算判别函数$g(\pmb x)$，根据计算结果划分$\pmb x$的所属类别。 e.g.给定决策表，按最小风险贝叶斯决策进行分类。 决策损失状态 $w_1$ $w_2$ $\alpha_1$ 0 6 $\alpha_2$ 1 0 已知条件： $P(w_1)=0.9,P(w_2)=0.1;p(\pmb x| w_1) = 0.2,p(\pmb x| w_2) = 0.4;$ $\lambda_{11} =0,\lambda_{12} =6,\lambda_{21} =1,\lambda_{22} =0$ 根据贝叶斯公式得到： $P(w_1|\pmb x) =0.818,P(w_2| \pmb x) = 0.182$ 判别函数可以定义为： $g(\pmb x) =R(\alpha_2|\pmb x) - R(\alpha_1| \pmb x) \ \qquad = \lambda_{21}P(w_1|\pmb x) - \lambda_{12}P(w_2| \pmb x) \ \qquad =\lambda_{21}p(\pmb x|w_1)P(w_1) - \lambda_{12}p(\pmb x |w_2)P(w_2) \ \qquad = 0.9p(\pmb x|w_1) -0.6p(\pmb x| w_2)=0$ 即：$9p(\pmb x|w_1) -6p(\pmb x|w_2) =0$ 正态分布的统计决策（参数估计决策）概率密度函数的定义和性质（1）单变量情况 单变量的正态分布概率密度函数定义： $p(x) = N(\mu,\sigma^2)= \frac{1}{\sqrt{2\pi} \sigma} exp \{ -\frac{1}{2} (\frac{x-\mu}{\sigma})^2\}$ $方差\sigma^2 = \int_{-\infty}^{\infty}(x-\mu)^2p(x)dx$ 离散情况$\sigma = E[(X-E(X))^2] = E(X^2)-(E(X))^2$ $期望\mu = E(x) = \int_{-\infty}^{\infty} xp(x)dx$ 概率密度函数满足： $\cases{p(\pmb x)\geq 0,(-\infty&lt;x&lt;\infty)\ \int_{-\infty}^{\infty}p(\pmb x)d\pmb x =1} $ 【说明】： 正态分布的样本主要集中在均值附近，其分散程度可以用标准差$\sigma$表示，越大则分散程度也越大。 正态分布的总体样本抽取样本，约有95%的样本落在区间$(\mu-2\sigma,\mu+2\sigma)$之间；也有$3\sigma$原理，99.7%的样本。 （2）多元正态分布 多元正态分布的概率密度函数定义： $p(\pmb x) = \frac{1}{(2\pi)^{d/2} |\sum|^{1/2}} exp \{ -\frac{1}{2} (\pmb x-\pmb \mu)^T \sum^{-1}(\pmb x-\pmb \mu)\}$ 其中$\pmb x=[x_1,x_2,\dots,x_d]^T$ ​ $\pmb \mu = [\mu_1,\mu_2,\dots,\mu_d]^T$ ​ $ \sum 是d*d维协方差矩阵，\sum^{-1}是其逆矩阵，|\sum|是其行列式$。 ​ $\pmb \mu = E(\pmb x) \ \sum= E\{(\pmb x-\pmb \mu)^T(\pmb x-\pmb \mu)\}$ 即有： $\mu_i = E(x_i) =\int_{E^d} x_ip(\pmb x) d\pmb x = \int_{-\infty}^{\infty}x_ip(x_i)dx_i$ 其中$p(x_i)$是边缘分布，即： $p(x_i) = \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}p(\pmb x)dx_1dx_2\dots dx_{i-1}dx_{i} \dots dx_d$ $协方差 \sigma_{ij}^2 = E[(x_i-\mu_i)(x_j-\mu_j)] \ \quad = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x_i-\mu_i)(x_j-\mu_j)p(x_i,x_j)dx_idx_j$ 协方差矩阵是对称非负定阵，且可以表示为：$$\sum= \begin{bmatrix} \sigma_{11}^2 &amp; \sigma_{12}^2&amp;\dots &amp; \sigma_{1d}^2 \\ \sigma_{12}^2&amp;\sigma_{22}^2 &amp;\dots &amp; \sigma_{2d}^2 \\ \vdots &amp; \vdots&amp; \ddots &amp; \vdots \\ \sigma_{1d}^2 &amp;\sigma_{2d}^2&amp; \dots&amp; \sigma_{dd}^2 \end{bmatrix}_{d\times d}$$其中对角元素是$x_i$的方差$\sigma_{ii}^2$，非对角元素$\sigma_{ij}^2是x_i和x_j的协方差。$只考虑正定情况下，即$|\sum|&gt;0$。 多元正态 概率模型下的最小错误率贝叶斯判别函数和决策面正态概率型$p(\pmb x|w_i) = N(\mu_i,\sum_i),i=1,2,\dots,c$ 判别函数为： $g_i(\pmb x) =- \frac{1}{2}(\pmb x-\mu_i) \sum_i^{-1}(\pmb x-\mu_i)^T - \frac{d}{2}ln2\pi -\frac{1}{2}ln|\sum_i| +lnP(w_i)$ 【因为】： $g_i(\pmb x) =ln p(\pmb x| w_i) +ln P(w_i)$ $p(\pmb x) = \frac{1}{(2\pi)^{d/2} |\sum|^{1/2}} exp \{ -\frac{1}{2} (\pmb x-\pmb \mu)^T \sum^{-1}(\pmb x-\pmb \mu)\}$ 决策面方程: $g_i(\pmb x) = g_j(\pmb x)$ 即得到： $- \frac{1}{2}[(\pmb x-\mu_i) \sum_i^{-1}(\pmb x-\mu_i)^T -(\pmb x-\mu_j) \sum_j^{-1}(\pmb x-\mu_j)^T]- \frac{d}{2}ln2\pi -\frac{1}{2} ln \frac{|\sum_i|}{|\sum_j|} +ln \frac{P(w_i)}{P(w_j)} =0$ 下面对一些特殊情况进行分析： （1）第一种情况：$\sum_i = \sigma^2I,i =1,2,\dots,c;其中I为单位阵$ 这种情况中的每类协方差矩阵都相等，而且类内各个特征空间相互独立，具有相等的方差$\sigma^2$。 若先验概率不等，即$P(w_i) \neq P(w_j)$ 此时协方差矩阵为：$$\sideset{}{_i} \sum = \begin{bmatrix} \sigma^2 &amp;\dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots&amp; \sigma^2 \end{bmatrix}$$即： $|\sum_i| = \sigma^{2d} \\\sum_i^{-1} = \frac{1}{\sigma^2}I \\因为\sum_i*\sum_i^{-1} =I$ 则判别函数可以写为： $g_i(\pmb x) =- \frac{1}{2}(\pmb x-\mu_i) \sum_i^{-1}(\pmb x-\mu_i)^T - \frac{d}{2}ln2\pi -\frac{1}{2}ln|\sum_i| +lnP(w_i)$ $ \qquad = - \frac{(\pmb x-\mu_i) (\pmb x-\mu_i)^T}{2\sigma^2}- \frac{d}{2}ln2\pi -\frac{1}{2}ln\sigma^{2d} +lnP(w_i) $ $ \qquad =- \frac{(\pmb x-\mu_i) (\pmb x-\mu_i)^T}{2\sigma^2} +lnP(w_i) 【因为第二、三项和类别i无关，可忽略】$ $式中：\ (\pmb x-\mu_i) (\pmb x-\mu_i)^T = ||\pmb x -\pmb \mu_i||^2 = \sum_{j=1}^{d} (x_j -\mu_{ij})^2,i=1,2,\dots,c $ $表示\pmb x到类w_i的均值向量\mu_i的欧式距离平方。$ 若先验概率不等，即$P(w_i) \neq P(w_j)$ 计算$||\pmb x -\mu_i||^2$,然后把$\pmb x归于具有\underset{1=1,2,\dots,c}{min}||\pmb x-\mu_i||^2$的类。这种分类器也称为最小距离分类器。 （2）第二种情况：$\sum_i = \sum$ 表示只要求各类的协方差矩阵都相等。 判别函数式可以简化为： $g_i(\pmb x) =- \frac{1}{2}(\pmb x-\mu_i) \sum^{-1}(\pmb x-\mu_i)^T +lnP(w_i)$ 若$c$类先验概率都相等，判别函数可以简化为： $g_i(\pmb x) = \pmb\gamma^2=(\pmb x-\mu_i) \sum^{-1}(\pmb x-\mu_i)^T $,也称为$\pmb x 到\mu 的Mahalanobis距离（马氏距离）的平方$ 此时决策规则为： 为了观察$\pmb x$进行分类，只要计算出$\pmb x$到每类的均值$\pmb \mu_i$的马氏距离的平方$\pmb \gamma^2$，最后把$\pmb x归类于\pmb \gamma^2最小的类别$。 （3）第三种情况：$\sum_i \neq \sum_j ,i,j=1,2,\dots,c$ 即各类的协方差矩阵不相等。 此时判别函数可以表示为： $g_i(\pmb x) =- \frac{1}{2}(\pmb x-\mu_i) \sum_i^{-1}(\pmb x-\mu_i)^T - \frac{d}{2}ln2\pi -\frac{1}{2}ln|\sum_i| +lnP(w_i) \ \qquad $ $g_i(\pmb x)=- \frac{1}{2}(\pmb x-\mu_i) \sum_i^{-1}(\pmb x-\mu_i)^T -\frac{1}{2}ln|\sum_i| +lnP(w_i) 【忽略掉与i无关的项】 $ $ \qquad = \pmb x^TW_i\pmb x + \pmb w_i^T\pmb x +w_{i0} \ 其中：\ W_i = - \frac{1}{2}\sum_i^{-1} 【d*d矩阵】$ $ \pmb w_i = \sum_i^{-1} \mu_i【d维列向量】$ $ w_{i0}=- \frac{1}{2}(\mu_i) \sum_i^{-1}(\mu_i)^T -\frac{1}{2}ln|\sum_i| +lnP(w_i) $ $ 若决策域\mathbb{R_1}与\mathbb{R_2}相邻，则决策面应满足：\ g_i(\pmb x) =g_j(\pmb x) $ $即：\ \pmb x^T(W_i-W_j)\pmb x + (\pmb w_i-\pmb w_j)^T\pmb x +w_{i0} -w_{j0} =0 $ 概率密度函数的估计监督参数估计——样本所属的类别及类条件概率密度函数的形式已知，而表征概率密度函数的某些参数未知。 非监督参数估计——已知总体样本概率密度函数形式但未知样本的所属类别，要求推断出概率密度函数的某些参数。 非参数估计——已知样本所属类别，但未知总体概率密度函数的形似，要求我们直接推断概率密度函数本身。 参数估计的两种典型方法对比: 最大似然估计——把参数看作是确定而未知的，最好的估计值是在获得实际观察样本的概率为最大的条件下得到的。 贝叶斯估计——把未知的参数当作是具有某种分布的随机变量，样本的观察结果使得先验分布转化为后验分布，再根据后验分布修正原先对参数的估计。 参数估计（1）统计量：样本包含着总体的信息，通过样本集把有关信息抽取出来，根据不同的要求构造出样本的某种函数。 （2）参数空间：将总体分布未知参数$\theta$的全部可容许值组成的集合，记为$\Theta$。 （3）点估计、估计量和估计值：点估计问题就是构造一个统计量$d(\pmb x_1,\dots,\pmb x_N)$作为参数$\theta$的估计$\hat{\theta} $，在统计学中称$\hat{\theta}为\theta的统计量$。 ​ 如果$\pmb x_1^{i},\dots,\pmb x_N^{i}是属于类别w_i$的几个样本的估计值，代入统计量得到对于$i类的\hat{\theta}$的具体数值，这个数值称为$\theta$的估计值。 （4）区间估计：用区间（$d_1,d_2$）作为$\theta$的可能取值范围的一种估计。这个区间称为置信区间。 最大似然估计（maximum likehood estimation,MLE）是一种给定观察数据来评估模型参数的方法，即：模型已知，参数未知 如：统计全国人口的身高，首先假设身高服从正态分布，但是该分布的均值和方差未知，无法统计全国每个人的身高，但是可以通过采样，获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布的均值和方差。 采样需要满足一个假设：所有的采样都是独立同分布的。 假设$x_1,x_2,\dots,x_n$为独立同分布的采样，$\theta 为模型参数，f为所使用的模型$，则$参数\theta 的模型f产生的采样可以表示为：$ $p(x_1,x_2,\dots,x_n| \theta) = p(x_1|\theta) \cdot p(x_1|\theta)\dots p(x_n|\theta)$ 其中$x_1,x_2,\dots,x_n$已知，$\theta$未知。 似然函数定义为： $L(\theta|x_1,x_2,\dots,x_n) = p(x_1,x_2,\dots,x_n|\theta) = \prod_{i=1}^{n}p(x_i|\theta)$ 对数似然（两边取对数）： $lnL(\theta|x_1,x_2,\dots,x_n) = \sum_{i=1}^{n}ln p(x_i|\theta)$ 平均对数似然： $\hat{l} = \frac{1}{n}ln L$ 最大似然函数：（最大的平均似然） $\hat{\theta}_{mle} = \underset{\theta \in \Theta}{arg max} \hat{l}(\theta|x_1,x_2,\dots,x_n)$ 【说明】： $\underset{x}{arg max} f(x)= \{x|\forall y:f(y) \leq f(x) \}$，表示$f(x)具有最大值M的x的值的集合。$ 同理，$\underset{x}{arg min} f(x)$表示$f(x)具有最小值m的x的值的集合。$ 最大似然估计量： 令$l(\theta)$为样本集$\{x_1,x_2,\dots,x_n\}$的似然函数，如果$\hat{\theta}$是参数空间$\Theta$中能使得似然函数$l(\theta)极大化的\theta的值，称\hat{\theta}为参数\theta的最大似然估计量。$ 最大似然估计量的求解： 在似然函数满足连续、可微的正则条件下，最大似然估计量的解为： $\frac{dl(\theta)}{d\theta} = 0，一般转化为对数求解更为容易，即：$ $令H(\theta)= ln l(\theta) $ $ 此时最大似然估计量为:\ \frac{dH(\theta)}{d\theta} =0的解。$ 如果未知参数不止一个，有s个，则： $\pmb \theta = [\theta_1,\theta_2,\dots,\theta_s]^T $ $H(\pmb \theta) = ln[L(\theta)] = ln p(x_1,x_2,\dots,x_n|\theta_1,\theta_2,\dots,\theta_s) $ $在样本独立抽出的条件下，可写为：$ $H(\pmb \theta) = ln \prod_{k=1}^n p(x_k| \pmb \theta) = \sum_{k=1}^n ln p(x_k|\pmb \theta) $ $令： \nabla_{\theta} = [\frac{\partial}{\partial \theta_1},\dots,\frac{\partial}{\partial \theta_s}]^T $ $则有：\ \nabla_{\theta}H(\pmb \theta) = \sum_{k=1}^n \nabla_{\theta}ln p(x_k| \pmb\theta) =0 $ $由这s个方程可以获得\pmb \theta 的最大似然估计量 \hat{\pmb \theta}。$ $存在多解情况，但不是都使得似然函数最大，只有\hat{\pmb \theta}才使得似然函数最大。$ e.g.1 假设一组连续采样值$(x_1,x_2,\dots,x_n)$，服从正态分布，标准差已知，问这个正态分布的期望值为多少时，产生这个已知数据的概率最大？ 解： $L(\theta|x_1,x_2,\dots,x_n) = p(x_1,x_2,\dots,x_n|\theta) = \prod_{i=1}^{n}p(x_i|\theta)$ 可得： $L(\theta|x_1,x_2,\dots,x_n) = (\frac{1}{\sigma \sqrt{2\pi}})^nexp(-\frac{1}{2\sigma^2} \sum_{i=1}^n(x_i - \mu)^2)$ 对$\mu$求导可得： $(\frac{1}{\sigma \sqrt{2\pi}})^nexp(-\frac{1}{2\sigma^2} \sum_{i=1}^n(x_i - \mu)^2) \frac{(\sum_{i=1}^nx_i-n \mu)}{\sigma^2}$ 令上式子等于零可得，最大似然估计的结果为： $\hat{\mu} = \frac{x_1+x_2+\dots+x_n}{n}$ 【结论】： 均值向量$\mu$的最大似然估计是样本均值，对于多元正态分布情况下即为： $\hat{\mu} = \frac{1}{N}\sum_{k=1}^N \pmb x_k$。 e.g.2 假设随机变量$x服从均匀分布$。但参数$\theta_1,\theta_2$未知 $p(x|\theta) = \cases{\frac{1}{\theta_2-\theta_1},\theta_1&lt;x&lt;\theta_2 \ 0, 其它}$ 从总体中独立地抽取n个样本$x_1,x_2,\dots,x_n$,则似然函数为： $l(\pmb \theta) = \cases{ p(x_1,x_2,\dots,x_n| \pmb \theta) =\frac{1}{(\theta_2-\theta_1)^n} \ 0 }$ 对数似然函数为： $H(\pmb \theta) = -N ln(\theta_2-\theta_1) \\\frac{\partial H}{\partial \theta_1}=N·\frac{1}{\theta_2-\theta_1}=0\\\frac{\partial H}{\partial \theta_2}=-N·\frac{1}{\theta_2-\theta_1}=0$ 上述两式成立必然解出的参数$\theta_1,\theta_2$有一个是无穷大，结果无意义。 采取方法： 当$\theta_2-\theta_1越小，似然函数越大，对于观察样本x_1,x_2,\dots,x_n，假设其中最大的一个为x^{‘’}，最小的一个为x^{‘}，则：$ 显然$\theta_2不能小于 x^{‘’}，\theta_1不能大于x^{‘}，因此\theta_2-\theta_1最小的可能值为x^{‘’}-x^{‘},此时\theta的最大似然估计量为：\ \hat{\theta_1}= x^{‘} \\\hat{\theta_2} = x^{‘’}$ 贝叶斯估计回顾贝叶斯决策：设状态空间为:$\Omega = \{w_1,w_2,\dots,w_c\}$ 识别对象: $\pmb x = [x_1,x_2,\dots,x_d]^T$ 设真实状态$w_j$，而采取的决策$\alpha_i$所造成的损失为$\lambda(\alpha_i,w_j)$。 条件期望损失或条件风险为： $R(\alpha_i|\pmb x)=E[\lambda(\alpha_i,w_j)] =\sum_{j=1}^c \lambda(\alpha_i,w_j)P(w_j| \pmb x)，i=1,2,\dots,a$ 由于$\pmb x$是随机向量的观察值，所以，对于不同的$\pmb x$采取的决策$\alpha_i$时，其条件风险不同。把决策$\alpha$看成随机向量$\pmb x$的函数，记为$\alpha(\pmb x)$，定义期望风险$R$为： $R=\int_{E^d} R(\alpha_i(\pmb x)|\pmb x)p(\pmb x) d\pmb x\ \quad = \int_{E^d} \lambda(\alpha_i,w_j)P(w_j| \pmb x)p(\pmb x)d\pmb x \ \quad =\int_{E^d} \sum_{j=1}^c \lambda(\alpha_i,w_j)P(\pmb x,w_j)d\pmb x $ $R称为贝叶斯风险，使得R最小的决策\alpha_k称为贝叶斯决策$ 若决策空间为$A=(\alpha_1,\alpha_2,\dots,\alpha_a)$，当： $\int_{E^d} R(\alpha_k(\pmb x)|\pmb x)p(\pmb x) d\pmb x = \underset{i=1,2,\dots,a}{min}\int_{E^d} R(\alpha_i(\pmb x)|\pmb x)p(\pmb x) d\pmb x$ 则采取决策$\alpha_k$。 贝叶斯估计概念设有一个样本集$\mathcal{X}而不是一个x$，要求找出估计量$\hat{\theta}(而不是最佳决策\alpha_k)$，用来估计样本集所属总体分布的某个真实参数$\theta$（而不是真实状态$w_j $）使得带来的贝叶斯风险最小。 决策问题 估计问题 样本$x$ 样本集 $\mathcal{X}$ 决策 $\alpha_i$ 估计量 $\hat{ \theta }$ 真实状态$w_j$ 真实参数$ \theta $ 状态空间$A$是离散空间 参数空间$\Theta $是连续空间 先验概率$P(w_j)$ 参数的先验分布$p(\theta)$ $R=\int_{E^d} \int_{\Theta}\lambda(\hat{\theta},\theta)p(\pmb x,\theta) d\theta d\pmb x $ $\quad =\int_{E^d} \int_{\Theta}\lambda(\hat{\theta},\theta)p(\theta | \pmb x) p(\pmb x)d\theta d\pmb x $ $ \quad =\int_{E^d} p(\pmb x)\int_{\Theta}\lambda(\hat{\theta},\theta)p(\theta | \pmb x) d\theta d\pmb x $ $ \quad =\int_{E^d}R(\hat{\theta}| \pmb x)p(\pmb x)d\pmb x $ $\ 因为：\ 根据贝叶斯公式：\ p(\theta| \pmb x) = \frac{p(\pmb x| \theta)p(\theta)}{ \int p(\pmb x| \theta)p(\theta)d\theta} = \frac{p(\pmb x| \theta)p(\theta)}{p(\pmb x)} \ 整理即得：\ p(\theta,\pmb x) = p(\theta| \pmb x)p(\pmb x) = p(\pmb x| \theta)p(\theta)$ 所以： $R(\hat{\theta}| \pmb x) = \int_{\Theta}\lambda(\hat{\theta},\theta)p(\theta | \pmb x) d\theta$ 其中$E^d为\pmb x 得取值的d维欧氏空间，\Theta 为\theta 的可能取值的参数空间。$ $R(\hat{\theta}| \pmb x)$为给定$\pmb x $条件下的估计量$\hat{\theta} $的期望损失。定义： 如果$\theta$的估计量$\hat{\theta}能使得$条件风险$R(\hat{\theta}| \pmb x)$最小，称$\hat{\theta}$是关于$\theta$的贝叶斯估计量。 求解贝叶斯估计量必须定义适当损失函数，一般来说，不同的损失函数，得到不同的贝叶斯估计量$\hat{\theta}$。 规定损失函数为： $\lambda(\hat{\theta},\theta) = (\theta - \hat{\theta})^2$，即平方误差损失函数。 【定理】： 如果损失函数为二次函数，即：$\lambda(\hat{\theta},\theta) = (\theta - \hat{\theta})^2$，则$\theta$的贝叶斯估计量$\hat{\theta}$是给定$\pmb x 时\theta的条件期望$，即： $\hat{\theta} = E[\theta| \pmb x] = \int_{\Theta} \theta p(\theta | \pmb x)d\theta$ proof: 贝叶斯估计使得贝叶斯风险R最小，即使得： $R=\int_{E^d}R(\hat{\theta}| \pmb x)p(\pmb x)d\pmb x $达到最小，即使得： $R(\hat{\theta}| \pmb x) = \int_{\Theta} \lambda(\hat{\theta},\theta) p(\theta | \pmb x)d\theta = \int_{\Theta} (\theta - \hat{\theta})^2 p(\theta | \pmb x)d\theta$达到最小。 因为： $R(\hat{\theta}| \pmb x) = \int_{\Theta} (\theta - \hat{\theta})^2 p(\theta | \pmb x)d\theta $ $ \qquad = \int_{\Theta}[ (\theta - E(\theta| \pmb x) +E(\theta| \pmb x)-\hat{\theta})^2 p(\theta | \pmb x)d\theta $ $ \qquad = \int_{\Theta}[ (\theta - E(\theta| \pmb x) ]^2p(\theta | \pmb x)d\theta+ \int_{\Theta}[ (E(\theta| \pmb x) - \hat{\theta} ]^2p(\theta | \pmb x)d\theta $ $+2\int_{\Theta}[ (\theta - E(\theta| \pmb x) ][E(\theta| \pmb x)-\hat{\theta}] p(\theta | \pmb x)d\theta $ $ 交叉项：$ $ \int_{\Theta}[ (\theta - E(\theta| \pmb x) ][E(\theta| \pmb x)-\hat{\theta}] p(\theta | \pmb x)d\theta $ $ =[E(\theta| \pmb x)-\hat{\theta}] \times \int_{\Theta}[ (\theta - E(\theta| \pmb x) ] p(\theta | \pmb x)d\theta $ $ = [E(\theta| \pmb x)-\hat{\theta}] \times\{\int_{\Theta}\theta p(\theta | \pmb x)d\theta -E(\theta| \pmb x)\int_{\Theta} p(\theta | \pmb x)d\theta \} $ $ =[E(\theta| \pmb x)-\hat{\theta}] \times [E(\theta| \pmb x)-E(\theta| \pmb x)] =0 $ 所以： $R(\hat{\theta}| \pmb x) =\int_{\Theta}[ (\theta - E(\theta| \pmb x) ]^2p(\theta | \pmb x)d\theta+ \int_{\Theta}[ (E(\theta| \pmb x) - \hat{\theta} ]^2p(\theta | \pmb x)d\theta $ 第一项非负，且与$\hat{\theta}$无关，第二项非负，当且仅当： $\hat{\theta} = E[\theta| \pmb x] = \int_{\Theta} \theta p(\theta | \pmb x)d\theta$ 时条件风险$R(\hat{\theta}| \pmb x)$最小。 综上所述，求解平方误差损失函数情况下的贝叶斯估计量$\hat{\theta}$，步骤如下： （1）确定$\theta的$先验分布$p(\theta)$ （2）有样本集$\mathcal{X}=\{x_1,x_2,\dots,x_n\}，求出样本联合分布p(\mathcal{X}| \theta)$ （3）利用贝叶斯公式，求出$\theta$的后验分布，即： ​ $\ p(\theta| \mathcal{X}) = \frac{p(\mathcal{X}| \theta)p(\theta)}{ \int p(\mathcal{X}| \theta)p(\theta)d\theta} $ （4）利用定理：$\hat{\theta} = E[\theta| \pmb x] = \int_{\Theta} \theta p(\theta | \pmb x)d\theta$求出贝叶斯估计量。 贝叶斯学习相对于贝叶斯估计，前三步一致，当求出后验分布后，不再估计$\hat{\theta}$，而是直接通过联合密度求类条件概率密度，即： $p(\pmb x| \mathcal{X}) = \int p(\pmb x,\theta| \mathcal{X})d\theta = \int p(\pmb x| \theta)p(\theta| \mathcal{X})d\theta$ 其中参数$\theta 的后验概率密度p(\theta| \mathcal{X})$，根据贝叶斯公式： $\ p(\theta| \mathcal{X}) = \frac{p(\mathcal{X}| \theta)p(\theta)}{ \int p(\mathcal{X}| \theta)p(\theta)d\theta} $ 如果$p(\mathcal{X}|\theta)在\theta =\hat{\theta}处有尖锐凸峰$，根据最大似然法有： $p(\pmb x| \mathcal{X}) \approx p(\pmb x| \hat{\theta})$，其中$\hat{\theta} 是\theta 的最大似然估计值$。 最大似然估计&amp;贝叶斯估计&amp;贝叶斯学习关系（1）最大似然估计把参数$\theta$看成确定的未知参数，似然函数定义为： $L(\theta|x_1,x_2,\dots,x_n) = p(x_1,x_2,\dots,x_n|\theta) = \prod_{i=1}^{n}p(x_i|\theta)$，其中$\mathcal{X} = \{x_1,x_2,\dots,x_n\}$ 最大似然估计就是求使得似然函数$l(\theta)$为最大时的$\hat{\theta}$作为最大似然估计量。 （2）贝叶斯估计把参数$\theta$看成随机的未知参数，一般$\theta$具有先验分布$p(\theta)$。样本通过似然函数$p(\mathcal{X}| \theta)$，并利用贝叶斯公式将$\theta$的先验分布转为后验分布： $\ p(\theta| \mathcal{X}) = \frac{p(\mathcal{X}| \theta)p(\theta)}{ \int p(\mathcal{X}| \theta)p(\theta)d\theta} $ $p(\theta| \mathcal{X}) $包含了关于$\theta$的先验信息及样本提供的后验信息，然后利用定理： 【如果损失函数为二次函数，即：$\lambda(\hat{\theta},\theta) = (\theta - \hat{\theta})^2$，则$\theta$的贝叶斯估计量$\hat{\theta}$是给定$\pmb x 时\theta的条件期望$，即： $\hat{\theta} = E[\theta| \pmb x] = \int_{\Theta} \theta p(\theta | \pmb x)d\theta$】 求出贝叶斯估计量$\hat{\theta}$，使得平方误差损失函数的贝叶斯风险极小化。 （3）贝叶斯学习利用$\theta$的先验分布及样本提供的信息求出$\theta$的后验分布$p(\theta| \mathcal{X})$，然后直接求总体分布： $p(\pmb x| \mathcal{X}) = \int p(\pmb x| \theta)p(\theta| \mathcal{X})d\theta$ 非参数估计概念参数估计是在已知总体分布的形式，很多实际问题不知道总体分布，或者总体分布不是一些通常遇到的典型分布，不能写成某些参数的函数。 于是有了直接样本估计总体分布的方法，称之为估计分布的非参数方法。 【基本思想】： 每个样本都对估计$p(x)$有贡献，将这些贡献叠加（差分拟合），得到$p(x)$的一个估计。 估计方法随机变量$\pmb x$落入区域$\mathcal{R}$的概率P为： $p= \int_{\mathcal{R}} p(\pmb x)d\pmb x$ 其中$p(\pmb x)$为总体概率密度函数。若从总体中独立抽取$n$个样本$x_1,x_2,\dots,x_n$,分别以$p(x_1),p(x_2),\dots,p(x_n)$出现，其中有$k$个样本落入区域$\mathcal{R}$的概率为$P_k$，服从二项分布。即： $P_k = C_n^k p^k(1-p)^{n-k} = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}$ 其中$p$是样本$\pmb x $落入区域$\mathcal{R}$的概率，$x_k$是随机抽取落入$\mathcal{R}$的样本，数目$k$也是随机的，。 则： ​ $k$的期望值为：$E(k) =np$ ​ $p的估计为： \hat{p} \approx \frac{k}{n}$ 如果区域$\mathcal{R}$足够小，以至于$p(\pmb x)$在这个很小的区域内没有什么变化，可以认为是恒定不变的，则： $p= \int_{\mathcal{R}} p(\pmb x)d\pmb x = p(\pmb x)*V$ 其中$V$是区域$\mathcal{R}$的体积。即： $\hat{p} = p= \hat{p}(\pmb x) V \approx \frac{k}{n}$ $\hat{p}(\pmb x) = \frac{k/n}{V}$ （1）如果体积$V$固定 $\frac{\hat{p}}{V}= \frac{\int_{\mathcal{R}}\hat{p}(\pmb x)d\pmb x}{\int_{\mathcal{R}}d\pmb x}$ （2）若训练样本$n$确定的，令$\mathcal{R}$不断缩小，此时会出现两种情况$\cases{\hat{p}(\pmb x)=0\\\hat{p}(\pmb x) \rightarrow \infty}$。 假设有无限多的样本，构造一串包括$\pmb x$的区域序列$\mathcal{R_1},\dots,\mathcal{R_n},\dots$，对$\mathcal{R_1}$采用一个样本估计，对$\mathcal{R_2}$采用两个样本估计，依此。 设$V_n是\mathcal{R_n}的体积，k_n是落入区域\mathcal{R_n}的样本数，\hat{p}_n(\pmb x) 是p(\pmb x)的第n次估计，$则： ​ $\hat{p}_n(\pmb x) = \frac{k_n/n}{V_n}$ 添加限制条件： $\cases{(1) \underset{n \rightarrow \infty}{\lim} V_n =0\ (2)\underset{n \rightarrow \infty}{\lim} k_n = \infty \(3)\underset{n \rightarrow \infty}{\lim} \frac{k_n}{n} =0 } $ 使得$\hat{p}_n(\pmb x)$收敛于$p(\pmb x)$。 为了满足这三个条件，区域$\mathcal{R}$序列的选择方法有两个基本途径： $(1)、parzen窗法：把包含\pmb x 点的区域序列V_n选为训练样本数目n的某个函数（如：V_n = \frac{1}{\sqrt{n}}）\ \qquad 并且不断缩小，此时对k_n和\frac{k_n}{n}都要加限制条件。$ $(2)、k_n近邻法：让k_n为n的某个函数（如：k_n= \sqrt{n}），而V_n选取正好是使相应的\mathcal{R_n}包含\ \qquad \pmb x的k_n个近邻。$ parzen窗法概念假定围绕$\pmb x的区域\mathcal{R_n}$是一个超立方体，其棱长为$h_n$，$d$为特征空间的维数，则超立方体的体积为： $V_n = h_n^d$ 为考查训练样本是否落入超立方体的内部，则检查$x-x_k$的每个分量是否小于$h_n/2$，其中$x$窗函数的中心。 定义窗函数： ​ $\phi(\mu)=\cases{1,当|\mu_j| \le \frac{1}{2},j=1,2,\dots,d \ 0,其它}$ 其中$\phi(\mu)$是以原点为中心的一个超立方体。 所以： $当 \pmb x_i 落入以\pmb x 为中心的，体积为V_n的超立方体内时，\phi(\mu)= \phi[\frac{\pmb x-\pmb x_i}{h_n}]=1，否则为0。$ 即： 若令:$\mu = \frac{\pmb x-\pmb x_i}{h_n}$，则： ​ $\phi( \frac{\pmb x-\pmb x_i}{h_n})=\cases{1,当|\pmb x-\pmb x_k|_j \le \frac{h_n}{2},j=1,2,\dots,d \ 0,其它}$ 因此落入该超立方体的样本数为： $k_n = \sum_{i=1}^n\phi[\frac{\pmb x-\pmb x_i}{h_n}] $ 代入$\hat{p}_n(\pmb x) = \frac{k_n/n}{V_n}$可得到$parzen$窗函数的基本公式： ​ $\hat{p}_n(\pmb x) = \frac{1}{n} \sum_{j=1}^n \frac{1}{V_n} \phi(\frac{\pmb x-\pmb x_i}{h_n})$ 实质上： 窗函数的作用是内插，每一个样本对估计起到的作用依赖于它到窗函数中心$\pmb x $的距离，所以$h_n$对$\hat{p}(\pmb x)$的估计有重要影响。 估计量$\hat{p}_n(\pmb x)$为密度函数的条件必须满足： （1）$\phi(\mu ) \ge 0$ （2）$\int \phi(\mu) d\mu =1$ 此时窗函数本身具有密度函数的形式，则$\hat{p}_n(\pmb x)$一定为密度函数。 窗函数选择（1）矩形窗 ​ $\phi(\mu)=\cases{1,当|\mu| \le \frac{1}{2} \ 0,其它}$ （2）正态窗（高斯窗） ​ $\phi(\mu)= \frac{1}{\sqrt{2\pi}}exp\{-\frac{1}{2}\mu^2\}$ （3）指数窗(曼哈顿函数) ​ $\phi(\mu)=exp\{-|\mu|\}$ 窗宽$h_n$对估计量$\hat{p}_n(\pmb x)$的影响在样本数有限的条件下，窗宽$h_n$对估计量的影响很大。 定义函数： $\delta_n(\pmb x) = \frac{1}{V_n}\phi(\frac{\pmb x}{h_n})$ 可以把$\hat{p}_n(\pmb x)$看成一个平均值，即： $\hat{p}_n(\pmb x) = \frac{1}{n} \sum_{j=1}^n \delta_n(\pmb x-\pmb x_i)$ 因为$V_n = h_n^d$，所以，$h_n影响\delta_n(\pmb x)$的幅度，即： $(1)、若h_n很大，则\delta_n(\pmb x)的幅度很小，只有\pmb x_i离\pmb x较远时候才能使得\delta_n(\pmb x)和\delta(0)相差得多一些。\ 此时，\hat{p}_n(\pmb x)变成N个宽度较大且函数值变化缓慢得函数的叠加，从而使得它是p(\pmb x)的一个平均估计。$ $(2)、若h_n很小，则\delta_n(\pmb x -\pmb x_i)的幅度很大，此时\hat{p}_n(\pmb x)就成了n个以样本为中心的尖峰函数的叠加，\\估计的统计变动很大。此时，在h_n \rightarrow 0的极端条件下，\delta_n(\pmb x-\pmb x_i)趋于一个以\pmb x_i 为中心的\delta函数，\\从而使得\hat{p}_n(\pmb x)趋于以样本为中心的\delta函数的叠加。 $ 【所以】: $h_n$对$\hat{p}(\pmb x)$的估计有重要影响,可以让$V_n$随着$n$的不断增加而缓慢趋于零，这样使得$\hat{p}_n(\pmb x)收敛于p(\pmb x)$。 实际问题中样本有限，需要做适当折中考虑。 K-近邻估计提出背景$parzen$窗估计存在一个具体问题是体积序列$V_1,\dots,V_n,\dots$的选择问题。对任何有限的样本数$n$，得到的结果对初值$V_1$很敏感，如果： $（1）、V_1选太小，大部分体积为空，从而使得\hat{p}_n(\pmb x)估计不稳定 $ $（2）、V_1选太大，则\hat{p}_n(\pmb x)估计比较平坦，反映不出真实总体分布的变化。$ 基本思想使用体积为数据的函数，而不是样本数$n$函数。 即： 如为了从$n$个样本中估计$p(\pmb x)$，可以先确定$n$的某个函数$k_n$，然后在$\pmb x $的周围选择一个体积，并让它不断增长至捕获$k_n$个样本为止，这样样本为$\pmb x$的$k_n$个近邻。 如果附近的密度比较高，则包含$k_n$个样本的体积自然相对比较小，从而提高分辨力；如果附近密度较低，则体积较大，但一进入高密度区就会停止增大。 最邻近规则假定有$c$个模式类别$w_1,w_2,\dots,w_c$，每类有标明类别的训练样本集有$N_i$个样本$i=1,2,\dots,c$, 规定$w_i$判别函数为： $g_i(\pmb x)= \underset{k}{min} \Arrowvert \pmb x- \pmb x_i^k \Arrowvert ，k= 1,2,\dots,N_i$ 其中： $\pmb x_i^k中i表示w_i类，k表示w_i类N_i个样本中的第k个$。 若： $g_j(\pmb x)= \underset{i}{min} g_i(\pmb x)，i= 1,2,\dots,c$ 则$\pmb x \in w_j$ 最邻近法的错误率分析设$N$个样本下的平均错误率为$P_N(e)，且样本\pmb x的最近邻为\pmb x’则：$ $P_N(e) = \iint P_N(e|\pmb x,\pmb x’)p(\pmb x’| \pmb x)d\pmb x’p(\pmb x)d\pmb x$ 定义最近邻法渐近平均错误率为$P$，则： $P= \underset{N \rightarrow \infty}{\lim} P_N(e)$ 存在以下关系： $P’ \le P \le P’\times(2- \frac{c}{c-1}P’)$ 其中$P’$表示贝叶斯错误率，$c$为分类数。 由于$P’$一般较小，所以上式可粗略表示为： $P’ \le P \le 2P’$ 即近邻法错误率在贝叶斯错误和两倍贝叶斯错误率之间。 $k$-近邻规则​（KNN）最近邻规则的一个推广即是”k-近邻规则”。取未知样本的$\pmb x $的$k$个近邻，看这个$k$个近邻多数属于哪一类，就把$\pmb x$归为哪一类。即： 假定有$c$个模式类别$w_1,w_2,\dots,w_c$，训练样本集有$N$个样本，每个样本记为$x_j,j=1,2,\dots,N$,确定一个常数$k$，记未知测试样本为$\pmb x $。 从未知测试样本点$\pmb x$开始生长，不断扩大区域，直至包含进$k$个训练样本点为止。把这个$k$个样本构成集合$\Omega(k)$，如果$\Omega(k)$中出现类别最多的类别是$w_i$，则$\pmb x \in w_i $。 近邻法存在缺点（1）需要将所有样本存入计算机，每次决策都要计算待识别的样本$\pmb x$和全部训练样本$\pmb x_i^k$，$i=1,2,\dots,c,k=1,2,\dots,N_i$之间的距离进行比较。存储量和计算量很大。 （2）虽然在所有情况下，对未知样本$\pmb x$都可以进行决策，但当错误代价很大时，会有较大风险。 （3）要求样本$N \rightarrow \infty$，这在实际场合中无法实现。 近邻法改进算法（1）快速算法 基本考虑是将样本分级分成一些不相交的子集，并在子集的基础上进行搜索，对最近邻和k-近邻都适用。 （2）剪辑近邻法 基本思想： ​ 将样本集分为两个独立的结合——设计集和考试集，并利用设计集设计分类器，用考试集估计错误率，在两个集合独立的条件下，对错误率的估计较为准确。 主要过程： ​ 设$N$个样本分成$c$类，并用集合$\mathcal{X}^N = \{\mathcal{X_1}^{N_1},\mathcal{X_2}^{N_2},\dots,\mathcal{X_c}^{N_c} \}$表示， 其中一类表示为$\mathcal{X_i}^{N_i}= \{\pmb x_i^k\} (i=1,2,\dots,N_i)$。 第一步：利用已知的样本集$\mathcal{X}^N$中的样本进行预分类，并剪辑掉错误的分类的样本，留下样本构成的剪辑样本集$\mathcal{X}^{NE}$，显然$\mathcal{X}^{NE}$要比$\mathcal{X}^N$中的样本数要少（即剪辑）。 第二步：利用剪辑的样本集$\mathcal{X}^{NE}$和近邻规则对未知样本$\pmb x$进行分类。 （2）压缩近邻法 剪辑近邻法只能去掉两类边界附近的样本，而靠近两类中心的样本几乎没有去掉，而根据近邻规则，这些样本的大多数对分类决策没有什么用处，因此在剪辑的基础上，再去掉一部分这样的样本有助于进一步缩短计算时间和降低存储要求。 CONDENSING算法： 有两个存储器，STORE和GRABBAG，把第一个样本放入STORE中，其它所有样本放入GRABBAG中， 第一步：用当前STORE的中的样本以1近邻规则对GRABBAG的第$i$类样本进行分类。如果分类正确，则该样本仍然送回中GRABBAG，否则放入STORE中，对GARBBAG中的所有样本重复此过程。 第二步：若GRABBAG中所有样本在进行上述检验过程中没有一个样本从GRABBAG转到STORE或者当GRABBAG为空时，算法终止，否则转入第一步 。 线性判别函数背景实际问题中，由于样本特征空间的类条件概率密度的形式常常很难确定，利用parzen窗函数等非参数方法估计分布需要大量样本，而且随着样本特征空间的维数增加所需要样本数急剧增加。 在实际问题中，往往不需要去恢复类条件概率密度，而是利用样本集直接设计分类器。即： 首先给定一个判别函数类，然后利用样本集确定判别函数中的未知参数。 线性判别函数是统计模式识别中基本方法之一，首先假定判别函数$g(\pmb x)$是$\pmb x$的线性函数，即$g(\pmb x) = \pmb w^T\pmb x+w_0$,对于$c$类问题，可以定义$c$个判别函数，$g_i(\pmb x) = \pmb w_i^T\pmb x +w_{i0},i=1,2,\dots,c$。利用样本去估计各$\pmb w_i$和$w_{i0}$，并把未知样本$\pmb x$归类到具有最大判别函数值的类别中去。 基本概念一般形式: $g(\pmb x) = \pmb w^T\pmb x + w_0$ 其中$\pmb x $ 称为$d$维样本向量，$\pmb w$称为权向量，$w$是个常数，称为阈值权。 $\pmb x = [x_1,x_2,\dots,x_d]^T$ $\pmb w=[w_1,w_2,\dots,w_d]^T$ 对于两类问题的线性分类器采用以下决策规则： 令$g(\pmb x) = g_1(\pmb x) - g_2(\pmb x)$，若： $\cases{g(\pmb x) &gt; 0 ,则 \pmb x \in w_1 \ g(\pmb x)&lt;0 ,则\pmb x \in w_2 \ g(\pmb x) =0,则\pmb x任意分到某一类，或拒绝}$ 方程$g(\pmb x) = 0$定义了一个决策面，把归类于$w_1$和$w_2$的点分隔开，当$g(\pmb x)$为线性函数时，这个决策面是超平面。 假设$\pmb x_1$和$\pmb x_2$都在决策面$H$上，则有： $\pmb w^T\pmb x_1 + w_0 = \pmb w^T\pmb x_2 + w_0$ 即：$\pmb w^T(\pmb x_1-\pmb x_2) =0$ 上式表明：$\pmb w$与超平面$H$上任一向量正交，即是超平面的法向量。 超平面$H$把特征空间分成两个半空间，即对$w_1$的决策域$\mathcal{R_1} $和对$w_2$类的决策域$\mathcal{R_2} $。 广义线性判别函数线性判别函数的局限性较大，不适用于非凸决策区域和多连通区域的划分问题。 二次判别函数的一般形式： $g(x) = c_0 + c_1x+c_2x^2$ 如果适当选择$\pmb x \rightarrow \pmb y$的映射，可以把二次判别函数化为$\pmb y$的线性函数，即： $g(x) = \pmb a^T\pmb y = \sum \limits_{i=1}^3a_iy_i$ 其中：$$\pmb y =\begin {bmatrix}y_1\\y_2\\y_3\end{bmatrix} =\begin{bmatrix}1\\x\\x^2\end{bmatrix} ,\pmb a =\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix} =\begin{bmatrix}c_0\\c_1\\c_2\end{bmatrix}$$$g(x) = \pmb a^T \pmb y$称为广义线性判别函数，$\pmb a$称为广义权向量。 一般来说： 对于任意高次判别函数$g(x)$都可以进行适当的变换，化为广义判别函数来处理。【此时$g(x)$可以看作对任意判别函数作级数展开，取其结尾部分的逼近】，但是这样的变换会使得维数大大增加，带来“维数灾难”。 $\pmb a^T\pmb y =0$在$\pmb Y$空间确定了一个通过原点的超平面。 广义判别函数的一个特例： $g(\pmb x) = \pmb w^T\pmb x + w_0 = w_0 + \sum\limits_{i=1}^dw_ix_i = \sum \limits_{i=1}^d a_iy_i = \pmb a^T\pmb y$ 其中：$$\pmb y =\begin {bmatrix}1\\x_1\\x_2\ \vdots \ x_d\end{bmatrix} =\begin{bmatrix}1\ \pmb x\end{bmatrix} ,\pmb a =\begin{bmatrix}w_0\\w_1\\w_2\ \vdots \ w_d\end{bmatrix} =\begin{bmatrix}w_0\ \pmb w\end{bmatrix}$$称为线性判别函数的齐次简化，$\pmb y=[1,\pmb x]^T$称为增广样本向量，$\pmb a$称为增广权向量。都是$\hat{d} =d+1$维向量。 虽然增加一维，但是样本间的欧式距离不变，仍然全部位于$d$维空间，即： 原$\pmb X$空间中，$\pmb a^T \pmb y =0$ 在$\pmb Y$空间确定了一个通过原点的超平面$\hat{H}$,它对$d$维子空间的划分与原决策面$\pmb w^T \pmb x +w_0 =0 $对原空间的划分完全相同。 $\pmb Y$空间内任意一点$\pmb y$ 到$\hat{H}$的距离为： $\hat{r} = \frac{g(\pmb x)}{\Arrowvert{\pmb a} \Arrowvert} = \frac{\pmb a^T \pmb y}{\Arrowvert{\pmb a} \Arrowvert}$ 设计线性分类器的主要步骤实际是寻找最好的$\pmb w$和$w_0$的过程，步骤： （1）要有一组具有类别标志的样本集$\mathcal{X} = \{\pmb x_1,\pmb x_2,\dots,\pmb x_N\}$。 （2）根据实际情况确定一个准则函数$J$，满足： $J$是样本集$\mathcal{X}和\pmb w、w_0或\pmb a 的函数$ $J$的值反映分类器的性能，它的极值解则对应于“最好”的决策 （3）用最优化技术求出准则函数的极值解$\pmb w^和w_0^或\pmb a^*$。 可以得到线性判别函数$g(\pmb x) = \pmb w^+ w_0^$或$g(\pmb x) = \pmb a^{*^T} \pmb y$。 对于未知样本$\pmb x_k$，只要计算$g(\pmb x)$，然后根据决策规则，判断$\pmb x_k$所属类别。 Fisher​线性判别应用统计方法解决模式识别问题时，降低维数是一个关键。 Fisher的基本思想和问题（1）把$d$维空间的样本投影到一条直线上，形成一维空间，即把维数压缩为一维。 （2）投影到一条直线上，会存在样本的混叠无法识别情况 （3）总可以找到某个方向，使得在这个方向上的直线上的样本投影能最好的分开。 基本参量（1）在$d$维$\pmb X $ 空间（原空间） 各类样本的均值向量$\pmb m_i$ $\pmb m_i = \frac{1}{N_i} \sum\limits_{\pmb x \in \mathcal{X}} \pmb x,i=1,2$ 样本内离散度矩阵$S_i$和总类内离散度矩阵$S_w$ $S_i = \sum \limits_{\pmb x \in \mathcal{X}}(\pmb x- \pmb m_i)(\pmb x-\pmb m_i)^T，i=1，2$ $S_w = S_1+S_2$ 样本类间离散度矩阵$S_b$ $S_b = (\pmb m_1-\pmb m_2)(\pmb m_1-\pmb m_2)^T$ 其中$S_w$是对称半正定矩阵，而且当$N&gt;d$时通常时非奇异的。$S_b$也是对称半正定矩阵，在只有两类的条件下，秩最大等于1。 （2）在一维$\pmb Y$空间（投影） 各类样本的均值向量$\widetilde{ m_i} $ $\widetilde{ m_i} = \frac{1}{N_i} \sum\limits_{ y \in \mathcal{Y}} y,i=1,2$ 样本内离散度矩阵$\widetilde{S_i^2}$和总类内离散度矩阵$\widetilde{S_w}$ $\widetilde{S_i} = \sum \limits_{ y \in \mathcal{Y}}( y- \widetilde{m_i})^2，i=1，2$ $\widetilde{S_w} = \widetilde{S_1^2}+\widetilde{S_2^2}$ Fisher准则函数我们希望在投影后，在一维$\pmb Y$空间里各类别样本尽可能分得开些，即希望两类均值之差$(\widetilde{m_1} - \widetilde{m_2})$越大越好；同时希望各类样本内部尽量密集，即希望类内离散度越小越好。 定义准则函数为： $ J_F(\pmb w) = \frac{(\widetilde{m_1} - \widetilde{m_2})^2}{\widetilde{S_1^2}+\widetilde{S_2^2}}$ 选择使得$J_F(\pmb w)$尽可能大的$\pmb w$作为投影方向，式子不含参数$\pmb w$，需要将其转变为$\pmb w$显函数。 过程如下： 假设一集合$\mathcal{X}$包含$N$个$d$维样本$x_1,x_2,\dots,x_N$，其中$N_1$个属于$w_1$类别的样本子集记为$\mathcal{X_1}$,$N_2$个属于$w_2$类别的样本子集记为$\mathcal{X_2}$若对$x_n$分量作线性组合可得标量： $y_n = \pmb w^Tx_n ,n=1,2,\dots,N_i$，其中权向量$\pmb w = [w_1,w_2,\dots,w_d]^T$ 这样得到$N$个一维样本$y_n$组成的集合，如果$||\pmb w|| =1$，则每个$y_n$就是相对应的$x_n$到方向$\pmb w$的直线上的投影。 $\widetilde{ m_i} = \frac{1}{N_i} \sum\limits_{ y \in \mathcal{Y}} y= \frac{1}{N_i} \sum\limits_{\pmb x \in \mathcal{X}} \pmb w^T \pmb x = \pmb w^T \pmb m_i$ $(\widetilde{m_1} - \widetilde{m_2})^2 = (\pmb w^T \pmb m_1- \pmb w^T \pmb m_2)^2 \ = \pmb w^T (\pmb m_1 - \pmb m_2)(\pmb m_1 - \pmb m_2)^T \pmb w \ = \pmb w^TS_b \pmb w$ $\widetilde{S_i} = \sum \limits_{ y \in \mathcal{Y}}( y- \widetilde{m_i})^2 = \sum \limits_{\pmb x \in \mathcal{X_i}}(\pmb w^T \pmb x -\pmb w^T\pmb m_i)^2 \\=\pmb w^T \left [ \sum \limits_{\pmb x \in \mathcal{X_i}} (\pmb x -\pmb m_i)(\pmb x -\pmb m_i)^T\right] \pmb w = \pmb w^T S_i \pmb w \ 因为上面:\ \widetilde{ m_i} = \frac{1}{N_i} \sum\limits_{ y \in \mathcal{Y}} y= \frac{1}{N_i} \sum\limits_{\pmb x \in \mathcal{X}} \pmb w^T \pmb x = \pmb w^T \pmb m_i$ 所以： $\widetilde{S_1^2}+\widetilde{S_2^2} = \pmb w^T(S_1+S_2)\pmb w = \pmb w^TS_w \pmb w$ 代入准则函数有： $J_F(\pmb w)= \frac{\pmb w^T S_b \pmb w}{\pmb w^T S_w \pmb w}$ Lagrange乘子法求极值，得最优解： 由于$\pmb w$得幅值不会影响$\pmb w$得大小，不会影响$J_F$函数得值，因此可以蒋分母常数化而优化分子最大，把问题转化为如下约束条件： $\max\{ \pmb w^TS_b \pmb w\} \ s.t. \quad \pmb w^T S_w \pmb w = c \neq 0$ 定义拉格朗日函数，引入拉格朗日乘子$\lambda$: $L(\pmb w,\lambda) = \pmb w^TS_b \pmb w - \lambda(\pmb w^TS_w\pmb w -c) $ $ \frac{\partial L(\pmb w,\lambda)}{\partial\pmb w} = S_b\pmb w - \lambda S_w \pmb w =0$ $S_b\pmb w’ = \lambda S_w \pmb w’ $ $ 左乘S_w^{-1}得：$ $ S_w^{-1} S_b \pmb w’ = \lambda \pmb w’ $ $ 其中：\ S_b\pmb w’ = (\pmb m_1-\pmb m_2)(\pmb m_1-\pmb m_2)^T \pmb w’ $ $ 其中：\ (\pmb m_1-\pmb m_2)^T \pmb w’ = R(标量，矩阵乘法) $ $ 所以： \ \lambda \pmb w^*= S_w^{-1}(\pmb m_1-\pmb m_2)R $ $ \pmb w’ = \frac{R}{\lambda} S_w^{-1}(\pmb m_1 -\pmb m_2) 忽略比例因子可得：$ $ 准则函数取极大值时得解：$ $ \pmb w’ = S_w^{-1}(\pmb m_1-\pmb m_2)$ 当维数$d$和样本数$N$都很大时，可采用贝叶斯决策规则。 最小平方误差（LMSE）准则函数和H-K算法主要针对$w_i /w_j$两类问题 MSE准则函数把来自$w_j$类得训练样本的各分量均乘以（-1）,则所有模式样本$\pmb x$均满足： $b= [b_1,b_2,\dots,b_n]^T$ 定义误差函数：$e= X*W-b$ 定义平方误差准则函数： $J_s(W,\pmb x,b )= ||e||^2 = ||X*W-b||^2 = \sum\limits_{i=1}^n (W^T\pmb x_i - b_i)^2$ 为使得到的解$\pmb w$确实是解向量$\pmb w^*$，应保证： $ W^T \pmb x_i - b_i \ge 0 ,i=1,2,\dots,n$ 即：$e$的各分量应不小于0$(b_i &gt; 0 )$ 【一个重要条件】 现求$J_s$对$\pmb w$的偏导数（求梯度） $ \nabla J_s = \frac{\partial J_s}{\partial W} = 2( W^T \pmb x_i- b)\pmb x_i = 2X^T(X*W -b) \ 令\nabla J_s =0得到： \ X^TXW = X^T b$ 其中$X^TX$是$(d+1)*(d+1)$维方阵，是非奇异阵（存在逆矩阵），有唯一解，即： $W^* = X^{+}b $，其中$X^+ = (X^TX)^{-1}X^T$，称为伪逆解。 由于$W^*$依赖于$b$，现求$b$，即： 为使得$J_s$最小，用梯度下降法建立迭代式，有： $b(k+1) = b(k) +(- c·\frac{\partial J_s}{\partial b})_{b=b(k)}$ 负号表示梯度反向，$c&gt;0$为常量，用于修正(此处是采用固定增量)。 其中：$\frac{\partial J_s}{\partial b} = -2(X*W-b)$ 要使得$J_s$最小，则需满足：$X*W-b =0$。同时： 考虑到$b(k)$的各个分量应取正值的约束条件，$b(k)$的增量$\delta b(k) =(- c·\frac{\partial J_s}{\partial b})$的各个分量取非负值，要求： （1）若$X*W(k) -b(k) \le 0$,则$\delta b(k) =0$ （2）若$X*W(k) -b(k) &gt; 0 $，则： ​ $\delta b(k) = (- c·\frac{\partial J_s}{\partial b})_{b=b(k)} = 2c[X*W(k)-b(k)]$ ​ 误差向量$\pmb e_k = XW(k) -b(k)$,则$\delta b(k)$可统一表示为： ​ $\delta b(k) = c(\pmb e_k + |\pmb e_k|)$ $b(k)$的迭代式变为： $b(k+1) = b(k) + \delta b(k) \ 代入：W’ = X^+b得：$ $ W’(k+1) = X^+b(k+1) = X^+ b(k) + X^+\delta b(k)$ $ =W’(k) +X^+\delta b(k)$ 此时可建立一种最小平方差算法(LMSE)，也称为H-K算法。 H-K算法步骤（1）由训练集样本构成增广矩阵$X$,求伪逆$X^+ = (X^TX)^{-1}X^T$。 （2）赋给初值$b(1)$，使得各分量均为正值，选择常数$c(c&gt;0)$，置$k=1$。 （3）计算：$W(k) = X^+b(k) \ \pmb e_k= XW(k) -b(k)$ （4）判断：若$\pmb e_k$各分量停止变为正值，或不全为0，则线性不可分，停止迭代；若$\pmb e_k$各分量接近于0，即$\pmb e_k \rightarrow 0$，则迭代完成，算法结束。否则继续。 （5）计算：$W(k+1) =W(k) +c·X^+[\pmb e_k +|\pmb e_k|] =W(k) +c·X^+|\pmb e_k| \ b(k+1) = b(k) +c·[\pmb e_k + |\pmb e_k|]$ （6）$k=k+1$；返回（3）。 H-K算法可监视迭代过程。 e.g.1 两类训练样本 $w_1 : (0,0)^T,(0,1)^T$ $w_2:(1,0)^T,(1,1)^T$ 用H-K算法求解向量。 令$c=1,b(1) = (1,1,1,1)^T$（要求各分量大于零） $W(1) =X^+b = (-2,0,1)^T$ 由于$\pmb e_k$各个分量为零，即$W(1)$为解向量，即 $W^* =(-2,0,1)^T$ 可知决策面方程为：$-2x_1+1=0$ e.g.2(线性不可分情况) $w_1 : (0,0)^T,(1,1)^T$ $w_2:(1,0)^T,(0,1)^T$ 12345678&gt; \begin&#123;equation&#125;&gt; X=\left.\left[&gt; \begin&#123;array&#125;&gt; 00&amp;0&amp;1\\1&amp;1&amp;1\\-1&amp;0&amp;-1\\0&amp;-1&amp;-1&gt; \end&#123;array&#125;&gt; \right]\right\&#125;&gt; \end&#123;equation&#125;_&#123;w_2类（×（-1））&#125;^&#123;w_1类&#125;&gt; 令$c=1,b(1) = (1,1,1,1)^T$（要求各分量大于零） $W(1) =X^+b = (0,0,0)^T$ 误差向量： $\pmb e_1 = XW(1) -b(1) = (-1,-1,-1,-1)^T$，各分量均为负值，终止迭代。 特征选择与提取基本概念模式特征： 分为物理特征、结构特征（如汉字，几何形式等）、数字特征（均值，方差等） 物理和结构特征：易于为人的直觉感知，但是有时难以定量描述，因此不利于机器判别 数学特征：易于用机器判别和分析，如统计特征。 原始特征： 在模式采集过程中形成的样本测量值，如一幅$512\times 512$的图像，原始特征维数：$512\times 512$ 原始特征是我们直接测量获得的，但是往往不用于模式识别中，主要有以下几个原因： 原始特征不能反映对象的本质特征； 高维的原始特征不利于分类器的设计。 计算量大，如对于一幅1024*768的灰度图像，灰度级为256级，直接表示需要786432 bytes，进行训练识别所需的空间、时间和计算量都无法接受 冗余，原始特征空间中，大量的特征都是相关性强的冗余特征 样本分布十分稀疏，对于有限训练样本而言，在高维的原始特征空间中分布十分稀疏 如果将数目过多的测量值不做分析，直接用于分类特征，不但耗时，而且会影响分类效果，产生“维数灾难”等问题。 针对原始特征以上的特性和不足，为了设计出更好的分类器，通常需要对原始特征的测量值集合进行分析，经过选择和变换处理，组成有效的识别特征。 处理方式主要有以下思路： 在保证一定分类精度的前提下，减少特征维数，进行“降维”处理，使分类器实现快速、准确、高效的分类； 去掉模棱两可、不利于分类的特征，使得提供的特征具有更好的可分性，使分类器容易判别； 提供的特征不应重复，即去掉相关性强但是没有增加更多分类信息的特征。 特征提取(feature extraction)： 通过映射或变换的方法，把模式空间的高维特征变成特征空间的低维特征。即：$T: E_{D} \rightarrow E_d,d \ll D$。 特征选择： 从一个特征集中挑选出最有利于分类的特征子集的过程。 【注】：对于特征提取，目标要求，方法差异非常大，很难有统一的方法，对任何目标都有效。 类别可分离性判据把一个高维空间变换为低维空间的映射很多，哪种映射对分类更为有利，需要一个标准。 从$D$个原始特征中选择 $d$个特征的各种可能组合很多，哪种组合分类的效果最好，也要有个标准。 要求： 与错误率（或错误概率的上界或者下界）有单调关系，这样使得判据取最大值的效果一般说来其错误概率也最小。 当特征独立时有可加性。即：$J_{ij}(x_1,x_2,\dots,x_d) = \sum\limits_{k=1}^d J_{ij}(x_k)$。其中$J_{ij}$是第$i$类和$j$类的可分性准则，其值越大，两类的分离程度越大。$x_1,x_2,\dots,x_d$是一定类别的相应特征的随机变量。 度量特性。即：$\cases{J_{ij} &gt; 0;当i \neq j时 \ J_{ij} = 0;当i =j时 \ J_{ij} = J_{ji}}$ 单调性。即：新的特征加入时，判据不断减小，$J_{ij} (x_1,x_2,\dots,x_d) \leq J_{ij}(x_1,x_2,\dots,x_d,x_{d+1})$。 基于距离的可分性判据从类别的可分性看，同一类的样本相似度越大，不同类的样本的相似度越小对分类越有利，因此可以用样本之间的距离作为度量特征的可分性判据。 将$c$个类别的样本集分别表示为$D_1,D_2,\dots,D_c$，其中$D_i=\{x_1^{(i)},x_2^{(2)},\dots,x_{n_i}^{(i)}\}$，样本的上标表示所属类别$i$，$n_i$表示第$i$个类别的样本数，特征以集合的形式表示：$X = \{x_1,\dots,x_d\}$。 类内距离类内距离度量的是在特定特征集合$X$上的同类别样本之间的相似程度。第$i$类样本集合中任意两个样本之间的均方距离为： $d_i^2 = \frac{1}{2n_i^2} \sum \limits_{k=1}^{n_i} \sum\limits_{l=1}^{n_i} d^2(\pmb x_k^{(i)},\pmb x_l^{(i)})$ 其中$\pmb x_k^{(i)},\pmb x_l^{(i)}$表示第$i$类的$D$维特征向量；$d^2(\pmb x_k^{(i)},\pmb x_l^{(i)})$表示两个向量之间的距离。 所有类别的样本的总的均方距离为： $J_{msd} (X) =\sum\limits_{i=1}^c P_i d_i^2$ 其中$P_i$表示第$i$类的先验概率，可以用第$i$个类别样本在全部样本中所占的比例来估计，即：$P_i \approx \frac{n_1}{n}$,$n = \sum\limits_{j=1}^c n_j$ 当采用欧式距离度量时。总均方距离可表示为： $J_{msd} (X)= \sum \limits_{i=1}^c \frac{P_i}{2n_i^2} \sum\limits_{k=1}^{n_i}\sum\limits_{l=1}^{n_i}(\pmb x_k^{(i)} - \pmb x_{l}^{(i)})^T(\pmb x_k^{(i)} - \pmb x_{l}^{(i)})$ 类内距离判据的简化计算方式为： $J_{msd}(X) = \frac{1}{n} \sum \limits_{i=1}^c \sum\limits_{k=1}^{n_i} (\pmb x_k^{(i)} - \pmb \mu^{(i)})^T(\pmb x_k^{(i)} - \pmb \mu^{(i)})$，$\pmb \mu^{(i)} = \frac{1}{n_i} \sum\limits_{k=1}^{n_i}\pmb x_k^{(i)}$ 其中$\pmb \mu^{(i)}$表示第$i$类样本集的均值向量。 用$\pmb \mu$表示所有各类的样本集总平均向量为：$\pmb \mu = \sum\limits_{i=1}^c P_i \pmb \mu^{(i)}$ 类内距离判据度量的是在特征集合$X$上的类内样本的距离程度。 类间距离类间距离度量的是不同类别样本之间的差异程度。第$i$个类别和第$j$个类别之间的任意两个样本之间的均方距离为： $d_{ij}^2 =\frac{1}{n_in_j} \sum \limits_{k=1}^{n_i}\sum\limits_{l=1}^{n_j} d^2(\pmb x_k^{(i)} - \pmb x_l^{(j)})$ 所有不同类别样本之间的均方距离为： $J_{bsd} = \frac{1}{2} \sum\limits_{i=1}^c P_i \sum\limits_{j=1,j \neq i}^c P_j ·\frac{1}{n_in_j} \sum \limits_{k=1}^{n_i}\sum\limits_{l=1}^{n_j} d^2(\pmb x_k^{(i)} - \pmb x_l^{(j)})$ 在欧式距离下，总的类间均方距离有如下两种简化计算方式： $J_{bsd} = \frac{1}{2} \sum\limits_{i=1}^c P_i \sum\limits_{j=1,j \neq i}^c P_j (\pmb \mu^{(i)} -\pmb \mu^{(j)})^T(\pmb \mu^{(i)} - \pmb \mu^{(j)})$ 其中$\pmb \mu^{(i)} = \frac{1}{n_i} \sum\limits_{k=1}^{n_i}\pmb x_k^{(i)}$ $J_{bsd} = \sum\limits_{i=1}^c P_i (\pmb \mu^{(i)} -\pmb \mu)^T(\pmb \mu^{(i)} - \pmb \mu)$ 其中$\pmb \mu = \frac{1}{n} \sum\limits_{i=1}^c \sum\limits_{k=1}^{n_i}\pmb x_k^{(i)}$ 基于散布矩阵的可分性判据第$i$个类内散布矩阵为：$S_i = J_{msd}(X) = \frac{1}{n_i} \sum\limits_{k=1}^{n_i} (\pmb x_k^{(i)} - \pmb \mu^{(i)})^T(\pmb x_k^{(i)} - \pmb \mu^{(i)})$ 总的类内散布矩阵为：$S_w = \sum \limits_{i=1}^c P_i S_i$ 类内散布矩阵描述的是同类样本在特征空间的分布情况。 类间散布矩阵描述的是不同类别样本在特征空间的分布情况，即： $S_b = \sum\limits_{i=1}^c P_i(\pmb \mu^{(i)} -\pmb \mu)^T(\pmb \mu^{(i)} -\pmb \mu)$ 其中$S_b,S_w$都是$d \times d$的对称矩阵， $S_w$的主对角线元之和为欧式距离测度下的类内均方距离 $S_b$的主对角线元之和是欧式距离度量下的类间均方距离 $S_w,S_b$的非主对角线元分别描述同类样本和不同类样本对应的特征对之间的相关程度。 所有样本的总体散布矩阵为： $S_t = \frac{1}{n} \sum\limits_{i=1}^c \sum\limits_{k=1}^{n_i} (\pmb x_k^{(i)} - \pmb \mu)^T(\pmb x_k^{(i)} -\pmb \mu)$ 可以证明：$S_t = S_w +S_b$，总体的散布矩阵$S_t$即是训练样本集的协方差矩阵。 由三个散布矩阵可以定义很多可分性判据，常用有： $J_1(X )= tr(S_w ^{-1}S_b)$，其中矩阵的迹是方阵的所有主对角线元之和。 $J_2(X) = \frac{tr(S_b)}{tr(S_w)}$ $J_3(X) = \frac{|S_b|}{|S_w|} = |S_w^{-1}S_b|$ $J_4(X) = \frac{|S_t|}{|S_w|}$ 这里的类别可分性判据与聚类准则非常相似，两者都是评价样本集在一组特征上的区分程度。 区别是： 聚类分析中样本集是无监督的，每个样本没有所属类别的信息，建立聚类准则的目的是要评价将样本集划分为不同子集时，不同子集之间的区分度； 特征选择和提取的样本集是有监督的，可分性判据评价的是这个样本集在不同的特征子集上的区分程度。 基于概率分布的可分性判据考虑各类的概率分布，以此确切表明各类交叠的情况。 对于两类情况$\omega_1$和$\omega_2$, 完全可分情况下：$p(\pmb x| \omega_1)$和$p(\pmb x| \omega_2)$没有交叠 完全不可分情况下：$p(\pmb x| \omega_1) =p(\pmb x| \omega_2)$ 分布密度的交叠程度可用$p(\pmb x| \omega_1)$和$p(\pmb x| \omega_2)$这两个分布密度函数之间的距离$J_p$表示。则有如下条件： $J_p$非负 当两个完全不交叠事，$J_p$取最大值，即若对所有的$\pmb x$有$p(\pmb x| \omega_2)\neq 0$时$p(\pmb x| \omega_1) =0$，则$J_p = \max$ 当两类分布密度相同时，即$p(\pmb x| \omega_1) =p(\pmb x| \omega_2)$时，$J_p =0$ 常用的概率距离度量Bhattacharyya距离和Chernoff界限B-距离定义为： $J_B = - \ln \int [p(\pmb x| \omega_1) p(\pmb x| \omega_2)]^{1/2} d \pmb x$ 与错误概率上界有直接关系，因为： $P_e = P(\omega_1) \int_{\mathcal{R}}p(\pmb x| \omega_1)d \pmb x + P(\omega_2) \int_{\mathcal{\overline{R}}}p(\pmb x| \omega_2) d \pmb x $ $ \quad = \int_{-\infty}^{\infty} \min \{ P(\omega_1)p(\pmb x| \omega_1),P(\omega_2)p(\pmb x| \omega_2)\}d \pmb x $ $ \quad \leq \int_{-\infty}^{\infty}\{P(\omega_1)P(\omega_2)p(\pmb x| \omega_1)p(\pmb x| \omega_2\}^{1/2} d \pmb x $ $ \quad =[P(\omega_1)P(\omega_2)]^{1/2} \int_{-\infty}^{\infty}\{p(\pmb x| \omega_1)p(\pmb x| \omega_2\}^{1/2} d \pmb x $ $ \quad = [P(\omega_1)P(\omega_2)]^{1/2} \exp \{-J_B\}$ 其中$\mathcal{R}$表示的是$p(\pmb x| \omega_2) &gt; p(\pmb x| \omega_1)$的区域，$\mathcal{\overline{R}}$表示的是$p(\pmb x| \omega_2) &lt;p(\pmb x| \omega_1)$的区域。 与之相似的另一个判据是Chernoff界限$J_c$： $J_c = - \ln \int p^s(\pmb x| \omega_1) p^{1-s}(\pmb x| \omega_2) d\pmb x$ 其中$s \in [0,1]$，当$s=0.5$时，$J_c = J_B$ 散度设有两类$\omega_i$和$\omega_j$，其对数似然比为： $l_{ij}(\pmb x) = \ln \frac{p(\pmb x| \omega_i)}{p(\pmb x| \omega_j)}$ 可以提供$\omega_i$对$\omega_j$可分性信息，对$\omega_i$类的平均可分性信息为： $I_{ij}(\pmb x) = E[l_{ij}(\pmb x)] = \int_{X}p(\pmb x| \omega_i) \ln \frac{p(\pmb x| \omega_i)}{p(\pmb x| \omega_j)} d\pmb x$ 同样对$\omega_j$类的平均可分性信息为： $I_{ji}(\pmb x) = E[l_{ji}(\pmb x)] = \int_{X}p(\pmb x| \omega_j) \ln \frac{p(\pmb x| \omega_j)}{p(\pmb x| \omega_i)} d\pmb x$ 定义 散度$J_D$区分$\omega_i$和 $\omega_j$类的总平均信息，等于两类平均可分性信息之和： $J_D = I_{ij} +I_{ji} = I_{ji}(\pmb x) = E[l_{ji}(\pmb x)] = \int_{X}[p(\pmb x| \omega_i)-p(\pmb x| \omega_j)] \ln \frac{p(\pmb x| \omega_i)}{p(\pmb x| \omega_j)} d\pmb x$ 正太分布时类别的可分性判据表达式当概率分布密度属于某种参数形式时（如指数分布），上面的复杂形式的$J_B,J_D$可进一步简化，特别是当满足是正太分布时： 假定二分类都是$d$维正太分布，$\omega_i$类为$N(\mu_i,\Sigma_i)$，$\omega_j$类为$N(\mu_j,\Sigma_j)$ $p(\pmb x| \omega_i) = \frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} \exp \left[ -\frac{1}{2}(\pmb x -\mu_i)^T\Sigma_i^{-1}(\pmb x -\mu_i)\right]$ $p(\pmb x| \omega_j) = \frac{1}{(2\pi)^{d/2} |\Sigma_j|^{1/2}} \exp \left[ -\frac{1}{2}(\pmb x -\mu_j)^T\Sigma_j^{-1}(\pmb x -\mu_j)\right]$ 求出对数似然比$l_{ij}$，得到两类的平均可分性信息$I_{ij}$，两类间的散度$J_D = I_{ij} +I_{ji}$。 当两类的协方差矩阵相等时$\Sigma_i = \Sigma_j = \Sigma$时，可进一步简化。 基于熵函数的可分性判据最佳分类器由后验概率确定，可由后验概率分布来衡量它对分类的有效性。 如果对于某些特征，各类后验概率是相等的，即$P(\omega_i| \pmb x) = \frac{1}{c}$,其中$c$为类别数。 这样无法确定样本的所属类别，这只能任意确定$\pmb x$属于某一类（假定先验概率相等或不知），此时错误概率为：$P_e = 1- \frac{1}{c}$ 另一个极端例子是，如果 有一组特征使得： $P(\omega_i | \pmb x) = 1$，且$P(\omega_j | \pmb x) = 0 ,\forall j \neq i$ 则此时$\pmb x$可以肯定划为$\omega_i$类，而错误概率为0。 可见：后验概率分布愈集中，错误概率越小。后验概率分布越平缓（接近均匀分布）则分类错误的概率越大。 为了衡量后验概率分布的集中程度，需要规定一个定量指标，引出熵的概念。 设$\omega$可能取值为$\omega_i ,i=1,2,\dots,c$的一个随机变量，它取值依赖于分布密度为$p(\pmb x)$的随机向量$\pmb x$(特征向量)，即给定$\pmb x$后的$\omega_i$的概率为$P(\omega_i| \pmb x)$。 从特征提取的角度看，用具有最小不确定性的那些特征进行分类是有利的，“熵”用作不确定性的度量，它是$P(\omega_i| \pmb x) ,P(\omega_2| \pmb x), \dots, P(\omega_c | \pmb x)$的函数，即： $H = J_c[P(\omega_i| \pmb x) ,P(\omega_2| \pmb x), \dots, P(\omega_c | \pmb x)]$ 熵函数具有如下性质： 熵为正且对称：$H_c (P_1,P_2,\dots,P_c)=H_c(P_2,P_1,\dots, P_c) = \dots = H_c(P_c,\dots,P_1) \ge 0$ 若$P_{i_0} =1$且$P_i =0(1 \le i\le c, i \neq i_0)$,则：$H_c(P_1,P_2,\dots,P_c) =0$ $H_c(P_1,P_2,\dots,P_c) = H_{c+1}(P_1,P_2,\dots,P_c,0)$ 对于任意的概率分布，$P_i \ge 0,(i=1,\dots,c),\sum\limits_{i=1}^c P_i =1$，有：$H_c(P_1,P_2,\dots,P_c) \le H_c(\frac{1}{c} ,\frac{1}{c},\dots, \frac{1}{c})$ 对所有的事件，熵函数是连续函数。 满足上述性质的一族信息度量是如下形似的广义熵： $J_c^{\alpha} [P(\omega_1| \pmb x),P(\omega_2| \pmb x),\dots, P(\omega_c| \pmb x)] = (2^{1-\alpha} -1)^{-1}\left[\sum\limits_{i=1}^c P^{\alpha}(\omega_i| \pmb x)-1\right]$ 其中$\alpha$是一个实的正参数，且$\alpha \neq 1$。 不同的$\alpha$可以得到不同的熵分离度量，例如： 当$\alpha $趋近于1时，据L’Hospital准则有： $J_c^{1} [P(\omega_1| \pmb x),P(\omega_2| \pmb x),\dots, P(\omega_c| \pmb x)] \\= \lim \limits_{\alpha \rightarrow 1}(2^{1-\alpha} -1)^{-1}\left[\sum\limits_{i=1}^c P^{\alpha}(\omega_i| \pmb x)-1\right] \ = - \sum\limits_{i=1}^c P(\omega_i | \pmb x) \log _2 P(\omega_i | \pmb x)$ 称为Shannon熵（香农熵）。 当$\alpha =2$时，得到平方熵： $J_c^{2} [P(\omega_1| \pmb x),P(\omega_2| \pmb x),\dots, P(\omega_c| \pmb x)] = 2\left[1-\sum\limits_{i=1}^c P^{2}(\omega_i| \pmb x)\right]$ 为了对所提取的特征进行评价，需要计算空间每一点的熵函数，在熵函数取值较大的那一部分空间，不同的类的样本必然在较大的程度上互相交叠。 因此熵函数的期望值：$E\{J_c^{\alpha} [P(\omega_1| \pmb x),P(\omega_2| \pmb x),\dots, P(\omega_c| \pmb x)] \}$ 可以表征类别的分离程度，用作所提取的特征的分类性能的评价指标。 特征提取把高维空间变换为低维空间，并且能够在低维空间中更好的进行分类。 主成分分析(PCA)主成分分析（principal components analysis，PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。 一般来说数据降维后样本中的信息会有所丢失，新的特征对样本的描述也存在一定的误差，PCA就是从尽量减少信息损失的角度来实现特征降维。主成分分析是能保证最优性。 PCA思想 找出数据里最主要的方面，用数据的最主要的方面来代替原始数据。具体来说，假设数据集是$n$维，共有$m$个数据$(x^{(1)},x^{(2)},\dots,x^{(m)})$。目的是将这$m$个数据的维度从$n$维降为$n’$，希望这$m$个$n’$维数据集尽可能地代表原始数据集。 最简单地情况是将数据从二维降到一维，即$n=2,n’=1$， 希望找到一个维度方向，它可以代表这两个维度地数据，图中列出了两个向量方向$u_1,u_2$，直观上来说，$u_1$比$u_2$好，有两种解释： 第一种解释是样本点到这个直线地距离足够近 第二种解释是样本点在这个直线上地投影能尽可能地分开 加入把$n’$从1维推广到任意维，则希望降维的标准是：样本点到这个超平面的距离足够近，或者说样本点在这个超平面上的投影能尽可能地分开。 PCA推导：基于最小投影距离 假设m个n维数据$(x^{(1)},x^{(2)},\dots,x^{(m)})$都已经进行了中心化，即$\sum\limits_{i=1}^m x^{(i)} =0$。经过投影变换后得到新的坐标系为$\{w_1,w_2,\dots,w_n\}$,其中$w$是标准正交基，即$||w||_2 =1,w_i^Tw_j =0$。 将数据从$n$维降到$n’$维，丢弃新坐标系中的部分坐标，则新的坐标系维$\{w_1,w_2,\dots,w_{n’}\}$，样本点$x^{(i)}$在$n’$维坐标系中的投影为：$z^{(i)} = (z_1^{(i)},z_2^{(i)},\dots,z_{n’}^{(i)})^T$，其中，$z_j^{(i)} = w_j^Tx^{(i)}$是$x^{(i)}$是在低维空间坐标系里第$j$维的坐标。 如果用$z^{(i)}$来恢复原始数据$x^{(i)}$，则得到的恢复数据为：$\overline{x}^{(i)} = \sum \limits_{j=1}^{n’} z_j^{(i)}w_j = Wz^{(i)}$，其中，$W$标准正交基组成的矩阵。 考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化：$\sum\limits_{i=1}^m ||\overline{x}^{(i)} - x^{(i)}||_2^2$ 将这个式子整理，得到： $\sum\limits_{i=1}^m ||\overline{x}^{(i)} - x^{(i)}||_2^2 = \sum \limits_{i=1}^m ||Wz^{(i)} -x^{(i)}||^2_2 【其中：\overline{x}^{(i)} = Wz^{(i)}】$ $ = \sum\limits_{i=1}^m (Wz^{(i)})^T(Wz^{(i)}) - 2\sum\limits_{i=1}^m (Wz^{(i)})^Tx^{(i)} + \sum\limits_{i=1}^m x^{(i)T}x^{(i)} 【平方和展开】$ $ = \sum\limits_{i=1}^m z^{(i)T}z^{(i)} - 2\sum\limits_{i=1}^m z^{(i)T}W^Tx^{(i)} + \sum\limits_{i=1}^m x^{(i)T}x^{(i)} 【矩阵转置：(AB)^T = B^TA^T,W^TW=I】$ $ = \sum\limits_{i=1}^m z^{(i)T}z^{(i)} - 2\sum \limits_{i=1}^m z^{(i)T}z^{(i)} + \sum \limits_{i=1}^m x^{(i)T}x^{(i)} 【其中：z^{(i)} = W^Tx^{(i)}】$ $ =- \sum\limits_{i=1}^m z^{(i)T}z^{(i)} + \sum \limits_{i=1}^m x^{(i)T}x^{(i)} 【合并同类项】$ $ =-tr\left(W^T (\sum\limits_{i=1}^m x^{(i)} x^{(i)T})W\right) + \sum\limits_{i=1}^m x^{(i)T}x^{(i)} 【其中：z^{(i)} = W^T x^{(i)} ，tr(BA^T) = A^T B】$ $ =-tr(W^T XX^TW)+ \sum\limits_{i=1}^m x^{(i)T}x^{(i)}【代数和转换为矩阵形式】$ 注意到$\sum \limits_{i=1}^m x^{(i)}x^{(i)T}$是数据集的协方差矩阵，$W$的每一个向量$w_j$是标准正交基。$\sum\limits_{i=1}^m x^{(i)T}x^{(i)}$是一个常量【因为$x^{(i)}$是一个列向量】。最小化上式等价于： $\underbrace{arg \min}_{W} -tr(W^T XX^T W)$，$s.t. W^TW = I$ arg 是变元（即自变量argument）的英文缩写。arg min 就是使后面这个式子达到最小值时的变量的取值arg max 就是使后面这个式子达到最大值时的变量的取值 例如 函数F(x,y): arg min F(x,y)就是指当F(x,y)取得最小值时，变量x,y的取值 直接观察可以发现最小值对应的$W$由协方差矩阵$XX^T$最大的$n’$个特征值对应的特征向量组成。数学推导，可以有以利用拉格朗日函数得到： $J(W) = -tr \left(W^TXX^TW + \lambda(W^TW-I)\right)$ 对$W$求导有$-2XX^T W +2\lambda W =0$，整理得到：$XX^T W = \lambda W$ 【注】：这里面运用到矩阵迹求导 可以看到，$W$为$XX^T$的$n’$歌特征向量组成的矩阵，而$\lambda$为$XX^T$的若干特征值组成的矩阵，特征值在主对角线上，其余位置为0。 所以： 将数据集从$n$维降到$n’$维是，需要找到最大的$n’$歌特征值对应的特征向量。这$n’$个特征向量组成的矩阵$W$即为我们需要的矩阵。 对于原始的数据集，只需要用$z^{(i)} = W^Tx^{(i)}$，就可以把原始数据集降维到最小投影距离的$n’$维数据集。 PCA推导：基于最大投影方差 假设m个n维数据$(x^{(1)},x^{(2)},\dots,x^{(m)})$都已经进行了中心化，即$\sum\limits_{i=1}^m x^{(i)} =0$。经过投影变换后得到新的坐标系为$\{w_1,w_2,\dots,w_n\}$,其中$w$是标准正交基，即$||w||_2 =1,w_i^Tw_j =0$。 将数据从$n$维降到$n’$维，丢弃新坐标系中的部分坐标，则新的坐标系维$\{w_1,w_2,\dots,w_{n’}\}$，样本点$x^{(i)}$在$n’$维坐标系中的投影为：$z^{(i)} = (z_1^{(i)},z_2^{(i)},\dots,z_{n’}^{(i)})^T$，其中，$z_j^{(i)} = w_j^Tx^{(i)}$是$x^{(i)}$是在低维空间坐标系里第$j$维的坐标。 对于任意一个样本$x^{(i)}$，在新的坐标系下的投影为$W^Tx^{(i)}$，在新坐标系中的投影方差为$W^Tx^{(i)}x^{(i)T}W$，要使得所有的样本的投影方差和最大【即最能分开】，即最大化$\sum\limits_{i=1}^m W^Tx^{(i)}x^{(i)T}W$的迹，即： $\underbrace{arg \max}_{W} tr(W^T XX^T W)$，$s.t. W^TW = I$ 利用拉格朗日函数可以得到： $J(W) = tr \left(W^TXX^TW + \lambda(W^TW-I)\right)$，对$W$求导有： $2XX^T W +2\lambda W =0$，整理可得： $XX^TW = (-\lambda) W$ 可以看到，$W$为$XX^T$的$n’$歌特征向量组成的矩阵，而$-\lambda$为$XX^T$的若干特征值组成的矩阵，特征值在主对角线上，其余位置为0。 PCA算法流程 求样本$x^{(i)}$的$n’$维的主成分其实就是求样本集的协方差矩阵$XX^T$的前$n’$个特征值对应的特征向量矩阵$W$，然后对于每个样本$x^{(i)}$，做如下的变换$z^{(i)} = W^T x^{(i)}$，以达到降维的PCA目的。 具体算法^1流程： 输入：$n$维样本集$D= \left( x^{(1)} ,x^{(2)},\dots,x^{(m)}\right)$，要降维到的维数$n’$。 输出：降维后的样本集$D’$ 对所有的样本进行中心化：$x^{(i)} = x^{(i)} - \frac{1}{m} \sum\limits_{j=1}^m x^{(j)}$ 计算样本的协方差矩阵$XX^T$ 对矩阵$XX^T$进行特征值分解 取出最大的$n’$个特征值对应的特征向量$(w_1,w_2,\dots,w_{n’})$，将所有的特征向量标准化后，组成特征向量矩阵$W$。 对样本集中的每一个样本$x^{(i)}$，转化为新的样本$z^{(i)} = W^T x^{(i)}$ 得到输出样本集$D’= \left( z^{(1)},z^{(2)},\dots,z^{(m)}\right)$ 有时候，不指定降维后的$n’$的值，而是指定一个降维到的主成分比重阈值$t$。这个阈值$t \in (0,1]$之间。假如$n$个特征值为$\lambda_1 \ge \lambda_2\ge \dots \ge \lambda_n$，则$n’$可以通过下式得到： ​ $\frac{\sum\limits_{i=1}^{n’}\lambda_i}{\sum\limits_{i=1}^{n}\lambda_i} \ge t$ PCA实例 假设我们的数据集有10个二维数据(2.5,2.4), (0.5,0.7), (2.2,2.9), (1.9,2.2), (3.1,3.0), (2.3, 2.7), (2, 1.6), (1, 1.1), (1.5, 1.6), (1.1, 0.9)，需要用PCA降到1维特征。 首先我们对样本中心化，这里样本的均值为(1.81, 1.91),所有的样本减去这个均值后，即中心化后的数据集为(0.69, 0.49), (-1.31, -1.21), (0.39, 0.99), (0.09, 0.29), (1.29, 1.09), (0.49, 0.79), (0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)。 现在我们开始求样本的协方差矩阵，由于我们是二维的，则协方差矩阵为： $XX^T=\left(\begin{matrix}cov(x_1,x_1)&amp; cov(x_2,x_1)\\\\cov(x_1,x_2)&amp;cov(x_2,x_2)\end{matrix}\right)$ 对于我们的数据，求出协方差矩阵为： $XX^T=\left(\begin{matrix}0.616555556 &amp;0.615444444 \\\\0.615444444&amp;0.716555556\end{matrix}\right)$ 求出特征值为（0.0490833989， 1.28402771） 对应的特征向量分别为：$(0.735178656,0.677873399)^T$和$(−0.677873399,−0.735178656)^T$ 由于最大的k=1个特征值为1.28402771，对于的k=1个特征向量为$(−0.677873399,−0.735178656)^T$ 则我们的$W=(−0.677873399,−0.735178656)^T$ 我们对所有的数据集进行投影$z^{(i)}=W^Tx^{(i)}$，得到PCA降维后的10个一维数据集为：(-0.827970186， 1.77758033， -0.992197494， -0.274210416， -1.67580142， -0.912949103， 0.0991094375， 1.14457216, 0.438046137， 1.22382056) 核主成分分析KPCA介绍以上的PCA算法中，假设存在一个线性超平面，可以对数据进行投影，但是有时候数据不是线性的，不能直接进行PCA降维。 这里需要用到核支持向量机一样的核函数思想，先把数据集从$n$维映射到线性可分的高维$N$，然后再从$N$维降维到一个低维度$n’$，这里的维度之间满足：$n’ &lt; n &lt; N$。 使用核函数的PCA称为核主成分分析(Kernellized PCA,简称KPCA)。假设高维空间的数据是由$n$维空间的数据通过映射$\phi$产生。 则对于$n$为空间的特征分解： $\sum\limits_{i=1}^m x^{(i)}x^{(i)T}W = \lambda W$ 映射为： $\sum\limits_{i=1}^m \phi\left(x^{(i)}\right)\phi\left(x^{(i)}\right)^TW =\lambda W$ 通过在高维空间进行协方差矩阵的特征值分解，然后用核PCA一样的方法进行降维。需要进行核函数运算，计算量要比PCA大很多。 PCA算法总结作为一个非监督学习得降维方法，只需要特征值分解，就可以对数据进行压缩，去噪。应用广泛。 为克服PCA的一些缺点，出现了很多PCA的变种，如解决非线性降维的KPCA，解决内存限制的增量PCA方法（Incremental PCA），以及解决稀疏数据降维的PCA方法Sparse PCA。 PCA算法的主要优点： 仅仅需要以方差衡量信息量，不受数据集以外的因素影响 各主成分之间正交，可消除原始数据成分间的相互影响的因素 计算方法简单，主要运算是特征值分解，易于实现。 PCA算法的主要缺点： 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。 K-L变换K-L变换与PCA不同之处在于，PCA是一种无监督的特征变换，K-L变换能够考虑到不同的分类信息，实现有监督的特征提取。 随机过程中K-L展开理论，将随机过程描述为无数个正交函数的线性组合，在模式识别中，可以将一个样本看成是随机向量的某一次实现结果。假设有一$d$维随机向量$X$，可以写成一组正交基$\sigma_i, i=1,\dots,\infty$的线性组合，且模为1： $ x= \sum\limits_{i=1}^{\infty} y_i \sigma_i$ 变形得到：$y_i = \sigma_i^T x$ 假设有用信息集中在其中的$q$维上，用$q$维去近似$x$： $ x’=\sum \limits_{i=1} ^q y_i \sigma_i$ 近似前后样本向量的差向量为：$x - x’$ 上述差向量的均方误差（MSE）为：$$\begin{equation}\begin{aligned} e&amp;= E\{ (x-x’)(x-x’)^T\} \\\\ &amp;= E\left\{ \left(\sum\limits_{i=q+1}^{\infty}y_i\sigma_i \right)\left(\sum\limits_{i=q+1}^{\infty}y_i\sigma_i \right) ^T \right\} \\\\ &amp;= E\left\{ \sum\limits_{i=q+1}^{\infty} y_i y_i^T \right\} \\\\ &amp;= \sum\limits_{i=q+1}^{\infty} \sigma_i^T E\{ xx^T\}\sigma_i \\\\ &amp;= \sum\limits_{i=q+1} ^{\infty} \sigma_i^T \Sigma_X \sigma_i\end{aligned}\end{equation}$$其中$\sigma_i^T \sigma_i =1$，正交基。变换矩阵$\Sigma_X$是原样本向量$X$的二阶矩阵（注意：这里还可以是其他矩阵，如协方差矩阵），与PCA相比较，当变换矩阵$\Sigma_X$是协方差矩阵时，K-L就是PCA。 最小化上述MSE，同PCA的求解方法一致，得到下面拉格朗日目标函数： $L(\sigma) = \sum\limits_{i=q+1}^{\infty} \sigma_i^T \Sigma_X \sigma_i - \sum\limits_{i=q+1}^{\infty} \lambda_i(\sigma_i^T\sigma_i -1)$ 对$\sigma$求导并令其等于零，有： $2(\Sigma_X -\lambda_i I)\sigma_i =0, i= q+1,\dots,\infty$ 其中$\lambda_i$就是$\Sigma_X$的特征值，均方误差为： $ e= \sum\limits_{i=q+1}^{\infty} \lambda_i$ 所以： 想用$q$维来表示样本向量化并使得MSE最小化，合理的做法是：把变化矩阵$\Sigma_X$的特征值从大到小排列，然后选择前$q$个特征值对应的特征向量，此时的截断误差能够保证最小，其中$\sigma_i , i=1,\dots,\infty$中的前$q$个正交向量就组成了新的特征空间，而原样本向量$X$在这个新的特征空间上的展开系数$y_i$就组成了心得特征向量，这种变换称为K-L变换，对于其他不同形式，主要基于变换矩阵$\Sigma_X$的具体形式。 K-L变换的几个重要性质 变换后得到的新特征满足零均值： proof： 设有如下K-L变换：$Y= A(X -m_x)$，其中矩阵$A$是变换核矩阵，$m_x$是$X$的均值；对$X$的变换结果$Y$求均值： $m_y = E[Y] = E[A(X-m_x)] = AE[X-m_x] = AE[X] - Am_x = 0$ K-L变换是一种正交变换 K-L变换的新特征彼此之间不相关 K-L变换信号的最佳压缩表示，用q维新特征表示原样本特征带来的误差在所有q维正交坐标变换中最小 K-L坐标系来表示数据，意味着熵最小，即样本的方差信息最大程度的集中在较少的维数上 K-L变换的新特征向量的二阶矩阵是对角阵，且对角线元素就是原特征的二阶矩阵的特征值，即$(\Sigma_X )^T = \Sigma_X $ proof:$$\begin{equation}\begin{aligned}\Sigma_{\xi} &amp;= E[\xi \xi^T] = E[A^T (x-u)(A^T(x-u))^T] \\\\&amp;= E[A(x-u)(x-u)^TA^T]= AE[(x-u)(x-u)^T]A^T \\\\&amp;= A\Sigma A^T \\\\&amp;= \left[\begin{matrix} \gamma_1&amp; 0 &amp; \dots&amp; 0 \ 0 &amp; \gamma_2 &amp; \dots &amp; 0 \ \vdots &amp; \vdots &amp; \ddots &amp;\vdots \ 0 &amp; 0&amp;\dots &amp; \gamma_p \end{matrix}\right]\end{aligned}\end{equation}$$ K-L变换与PCA联系与区别联系： 两者都属于正交变换，当对原特征$X$进行中心化时（即变换矩阵为协方差矩阵），K-L变换等价于PCA PCA是离散的K-L变换，都可以实现降维变换 区别： K-L变换可以实现有监督的特征提取，但是PCA的变换是一种无监督的 K-L变换可以处理连续和离散的情况（较为广义），而PCA只针对离散情况（较为侠义） K-L变换的变换矩阵可以是很多种，如二阶矩阵、协方差矩阵（总体散布矩阵）等，即变换矩阵是自相关矩阵。而PCA的变换矩阵只是协方差矩阵 协方差矩阵： $C_x = E\{(x-m_x)(x-m_x)^T\}$ 其中$m_x$是均值。 其中$\pmb x^H$是共轭转置矩阵，当为实矩阵时，等价于转置矩阵。 协方差矩阵和自相关矩阵的关系：$$\begin{equation}\begin{aligned}C_x &amp;= E[ (x-m_x)(x-m_x)^H ] \\\\&amp;=E[ xx^H - xm_x^H-m_x x^H + m_xm_x^H]\\\\&amp;= E[xx^H] - E[x]E[m_x^H] -E[m_x]E[x^H] - E[m_xm_x^H] \\\\&amp;= E[xx^H] -m_x m_x^H - m_x m_x^H + m_xm_x^H \\\\&amp;= R_x - m_x m_x^H\end{aligned}\end{equation}$$ 特征选择特征选择的目的是从原始的特征集$X$中中挑选出一组最有利于分类的特征$X’$，由于类别可分性判据可以评价挑选出的一组特征对于分类问题的有效性，因此特征选择实际上是一个对某种选定的可分性判据的优化： $X’ =arg \max \limits_{\widetilde{X} \subset X} J(\widetilde{X})$ 其中原始特征集合$X$中包含$d$个特征，$X’$中包含$d’ &lt; d$个特征，$\widetilde{X}$是任意包含$d’$个元素的$X$的子集。 求解优化问题的一个简单的思路：用可分性判据$J$分别评价每一个特征，然后根据判据值得大小对特征重新排序，使得$J(x_1) \ge J(x_2) \ge \dots \ge J(x_{d’}) \ge \dots \ge J(x_d)$ 其中有一个问题：单独使用使得$J$较大得前$d’$特征作为特征选择结果，不能保证是一个最优解。因为这个过程中没有考虑各个特征之间得相关性，只有当特征之间相互独立时才能保证是最优解。 另一个简单思路是对所有得$\widetilde{X} \subset X$的组合进行穷举，计算每一种组合判据值，选出最优组合。从$d$个特征中选择出$d’$个特征，所有可能得组合数为： $q= C_d^{d’} = \frac{d!}{(d-d’)! \times{d’}!}$ 穷举法可以保证最优性，但是需要巨大的计算量作为代价。 分支定界法分支定界法是一种能减小穷举法计算复杂度的最优特征组合搜索算法，但是依赖于类别可分性判据的一个重要性质——单调性，即对于两个特征子集$X_1 \subset X_2 \implies J(X_1) \le J(X_2)$。 单调性不是所有类别可分判据都有的性质，只有当可分判据满足单调性时，分支定界法才能保证搜索到最优的特征组合。 算法： 初始化：根据原始特征维度$d$和选择特征维度$d’$构建搜索树，使用非对称树保证生成的叶节点不会出现重复的特征，树高$d-d’$，设置界值$B=0$ 从右向左分支定界搜索： 如果当前节点没有分支，则向下搜索，直到叶节点为止。计算叶节点代表的特征集合的可分性判据，如果大于界值$B$，则替换$B$，并记录这个特征集，作为当前的最优选择；向上回溯，直到有节点存在未搜索的分支，按照从右往左的顺序搜索其子节点 如果当前节点有分支，则计算当前节点代表特征集合的可分性判据，小于界值$B$，则终止该节点往下搜索，否则按照从右往左的顺序搜索其子节点。 输出：最优特征集合 存在问题： 算法能否搜索到最优的特征组合依赖于所采用的类别可分性判据是否具有单调性 分支定界法的计算复杂度是不确定的，与最优解分支的所在位置有关，如果最优解分支在最右端并且根节点的子节点判据值均小于最优解，则搜索效率最高；如果每个分支的可分性判据都大于其左端分支的可分性判据，那么需要计算搜索树上所有节点的判据值，实际的计算复杂度会超过穷举法。 以求解整数规划问题解释分支定界法 $max z = 5x_1 +8x_2$ $ LIP0\cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z} $ (1)首先不考虑整数限制条件，应用线性规划，得到$LIP0$的松弛问题： 得到最优解为：$x_1 = 2.25,x_2 =3.75，z_0 =41.25$ 由此可得到最优解$0 \le z^ \le z_0 =41.25 $，且必须为整数，所以$0 \le z^ \le 41$。这一过程称为定界，即给出 $ILP0$问题目标函数最优解的$z^*$的上下界。 （2）但由于$x_1,x_2$都必须是整数，必须得去掉小数。对于$x_2$，最终的最优解不会在3和4之间取值，所以有： $\max z=5x_1+ 8x_2$ $ ILP1 = \cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_2 \le 3 \\\\x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z}$ 或者： $ ILP2= \cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_2 \ge 4 \\\\x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z}$ 这样将可行域分成了$x_2 \le 3$或$x_2 \ge 4$，把原来线性规划的解点$(2.25,3.75)$排除出去，但没有排除任何整数可行解。这一过程称为分支，即用两个矛盾的约束条件代入原问题形成两个子问题$ILP1,ILP2$。 解$ILP1$的松弛问题得到： $(x_1,x_2) = (3,3),z_{max} = 39$ 解$IL2P$的松弛问题得到： $(x_1,x_2) = (1.8,4),z_{max} = 41$ （3）修改上下界：从$ILP1,ILP2$的解得到$39 \le z^{*} \le 41$ （4）再分支： 针对于$x_1$，去掉其中的小数部分，增加约束$x_1 \le 1 ,x_1 \ge 2$对$ILP2$进一步分支，【注：因为在$ILP2$问题中得到的$z_{max}$取值最大，所以再对其进行分支】，即： $\max z=5x_1+ 8x_2$ $ ILP3 = \cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_2 \ge 4 \\\ x_1 \le 1 \\\\x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z}$ $ ILP4 = \cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_2 \ge 4 \\\ x_1 \ge 2 \\\\x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z}$ 解$ILP3$得到： $(x_1,x_2) =(1,4),z_{max} = 40$ $ILP4$无可行解。 （5）再修改界，有：$39 \le z^{*} \le 40$ （6）再分支，继续对$ILP3$进行分支【注：因为$ILP4$问题无解，不再分支，在$x_2\ge 4$的基础上再往前分支，计算$z_{max}$值】，得到： $\max z=5x_1+ 8x_2$ $ ILP5 = \cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_2 \ge 4 \\\ x_1 \le 1 \\\ x_2 \le 4 \\\\x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z}$ $ ILP6 = \cases{x_1 +x_2 \le 6 \\\ 5x_1 + 9x_2 \le 45 \\\ x_2 \ge 4 \\\ x_1 \le 1 \\\ x_2 \ge 5 \\\\x_1 \ge 0,x2 \ge 0 ;x_1,x_2 \in Z}$ 解$ILP5$得到： $(x_1,x_2) =(1,4),z_{max} =37$ 解$IL6P$得到： $(x_1,x_2) =(0,5),z_{max} =40$ 至此，所有的子问题都已探明，$z$的最大值不再改变，求解结束。 寻找的 上下界为：$39 \le z^{*} \le 40$ 最终找到的$ILP0$(原问题)的最优解：$z_{max} = 40$ 次优搜索算法很多的实际问题不再追求找到最优的特征组合，转而采用某种次优搜索算法，选出一组较好的特征。 顺序前进法Sequential Forward Selection（SFS），也称为自下而上的搜索方法。从一个空集开始每次向选择的特征集合中加入一个特征，直到特征集合中的特征数满足要求为止，每次选择加入特征的原则是将其加入特征集合后能够使得可分性判据最大。 算法： 初始化：原始特征集合$X$，设置特征集合$X’ = \Phi$ 循环直至$X’$中包含$d’$个特征为止： 计算将任意未被选择的特征加入$X’$后的可分性判据值：$J\left( X’ \cup \{x_i\}\right), \forall x_i \in X-X’$ 寻找最优特征：$x’= arg \max \limits_{x’ \in X-X’} J\left( X’ \cup \{x_i\}\right)$ 将最优特征加入选择特征集合：$X’ =X’\cup \{x’\}$ 输出：特征集合$X’$ 循序前进法需要计算判据值的次数为: $\sum\limits_{i=0}^{d’-1} (d-i) = \frac{d’(2d-d’ +1)}{2}$ 顺序后退法Sequential Backward Selection(SBS)，也称自上而下的搜索方法。从全集开始，每一轮从特征值中删除一个最差的特征，选择特征的原则是将其删除后使得特征集合的判据值下降得最少。 算法： 初始化：原始特征集合$X$，设置选择特征集合$X’ =X$ 循环直到$X’$中包含$d’$个特征为止： 计算将任何一个$X’$中元素删除后的可分性判据值：$J\left( X’ - \{x_i\}\right),\forall x_i \in X’$ 寻找最优的删除特征：$x’ = arg \max\limits_{x’ \in X’} J\left( X’ - \{x_i\}\right)$ 将选择的特征移出集合：$X’ = X’ -\{x’\}$ 输出：特征集合$X’$ 顺序后退法需要计算判据值的次数为$\sum\limits_{i=0}^{d-d’-1}(d-i) = \frac{(d-d’)(d+d’+1)}{2}$ 广义顺序前进（后退）法Generalized Sequential Forward(Backward) Selection，GSFS，GSBS。顺序前进和顺序后退法都是每次增删1个特征，而广义的顺序前进和顺序后退则是每次增删$r$个特征。 如果进行了$k$轮迭代，判据值的计算次数为$\sum\limits_{i=0}^{k-1} C_{d-i\times r}^r = \frac{1}{r!} \times \sum\limits_{i=0}^{k-1} \frac{(d-i \times r)!}{(d- i\times r -r)!}$ 一般来说，广义的顺序前进（后退）法的计算量都要大于顺序前进（后退）法，但是由于考虑了特征之间的统计相关性，优化的结果一般要好于每次选择一个特征的顺序前进或后退法。 增l-删r法（l-r法）顺序前进（后退）法中一旦某个特征被增（删）后，既不能被删（增）了，这对搜索最优二点特征组合是不利的，因为选择这些特征时只考虑了和当前$X’$中的特征的相关性和增（删）后的判据值大小，而没有考虑之后增（删） 的某些特征的情况。 $l-r$法允许对特征选择过程进行回溯，先用顺序前进法向$X’$加入$l$个特征，再用顺序后退法从$X’$中删除$r$个特征$(l&gt; r)$，循环这个过程直至满足特征数要求。 算法： 初始化：设置选择特征集合$X’ = \Phi$ 循环直到$X’$中包含$d’$个特征为止： 调用顺序前进法$l$次，向$X’$中添加$l$个特征 调用顺序后退法$r$次，向$X’$中删除$r$个特征 输出：特征集合$X’$ 回溯过程也可以反方向进行，此时$l&lt;r$，从全集开始，先调用顺序后退法$r$次，再用顺序前进法$l$次，直到特征数满足要求为止。 神经网络神经网络，一个广泛的定义是： 神经网络是由具有适应性的简单单元组成的广泛的并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。 神经网络是一种监督学习算法，也是一种线性分类模型。 神经元模型神经网中的基本单元是神经元$（neuron）$。每个神经元有多个树突$（dendrite）$,一个轴突$(axon)$和一个细胞体$(cell body)$，树突短而多支，轴突长只有一个（1m长左右）。 结构： 功能： 树突用于传入其它神经元传递的神经冲动，轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。 形式化描述： 树突对应输入部分，每个神经元收到$n$个其它神经元传递过来的输入信号，通过带权重的连接（有损失）传递给细胞体，权重称为连接权(connection weight)。 细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或称为累计电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数(activation function)的处理，产生输出从轴突传送给其它神经元。 激活函数： 与线性分类类似，神经元最理想的激活函数是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，分别对应于兴奋和抑制。 阶跃函数不连续，不光滑，也采用sigmoid函数来近似，sigmoid函数将较大范围内变化的输入值挤压到（0，1）输出范围内，也称为挤压函数(squashing function)。 将多个神经元按一定的层次结构连接起来得到神经网络。是一种包含多个参数的模型，若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。 感知器感知器(perceptron,1957)是由两层神经元组成的简单模型。 感知器与线性模型区别： 两者都是对属性加权与另一个常数求和，再使用sigmoid函数将输出压缩在0-1之间，解决分类问题。 不同在于感知器的输出层可以有很多个神经元，从而实现多分类，两个模型所使用的参数估计方法不同。 感知器结构： 输出层神经元进行激活函数处理，称为功能神经元(functional neuron)；输出层只接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。 感知器的学习规则： 给定训练集，感知器的$n+1$个参数($n$个参数和1个阈值)都可以通过学习得到，阈值可以看成输入$x_{n+1} = -1$，对应权重为$w_{n+1}$，使得权重和阈值统一为权重的学习。 学习规则如下： 对于训练样本$(x,y)$，每一个样本进入感知器产生一个输出值。若该输出值与样本的真实标记不一致，则感知器会对权重进行调整，若激活函数为为阶跃函数，则调整的方法（基于梯度下降法）为： 对于样本$(x,y)$,其预测值为： $\hat{y} = f\left(\sum \limits_{i=1}^{n} \omega_ix_i - \theta \right) = f\left(\sum \limits_{i=1}^{n+1} \omega_ix_i \right)$ 其中$x_{i+1} =-1$为固定值。 均方误差为：$E= \frac{1}{2}(y-\hat{y})^2$ 使用梯度下降法寻找最小的均方差误差$\min E $，负的梯度方向为最速下降方向。 $\frac{\partial E}{\partial \omega_i} = -(y-\hat{y})\frac{\partial \hat{y}}{\partial \omega_i} = -(y-\hat{y}) \frac{\partial f\left(\sum \limits_{i=1}^{n+1} \omega_ix_i \right)}{\partial \omega_i}$ 因为函数$f$为阶跃函数，即有： $\frac{\partial f\left(\sum \limits_{i=1}^{n+1} \omega_ix_i \right)}{\partial \omega_i} = x_i$ 令下降步长为$\eta，\eta \in (0,1)$，则： $\Delta \omega_i = - \frac{\partial E}{\partial \omega _i} \times \eta = \eta(y -\hat{y}) x_i$ 其中$\eta \in (0,1)$表示学习速率。 可见： 感知器是通过逐个样本输入来更新权重，首先设置好初始权重（一般随机设置），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本的数据的输出值都与真实标记相同。 感知器模型总能够将训练数据的每一个样本都预测正确，感知器模型很容易产生过拟合问题。 过拟合（overfitting） 为得到一致假设而使得假设变得过度严格。为拟合一个模型，使用过多参数。 多层网络感知器只有一层功能神经元，功能有限，只能处理线性可分问题，这个过程感知器一定收敛(converge)。 解决非线性问题，需要使用多层功能神经元，多层神经网络的拓扑结构如下： 结构满足以下特点： 每层神经元与下一层神经元之间完全互连 神经元之间不存在同层连接 神经元之间不存在跨层连接 常用的神经网络为“多层前馈神经网络”(multi-layer feedforward neural network)。 其中，“前馈”指的是在网络拓扑结构中不存在环或者回路。 神经网络的学习过程是根据训练数据来调整神经元之间的连接权以及每一个神经元的阈值，即：神经网络学习到的东西都蕴含在网络的连接权和阈值中。 BP神经网络算法BP神经网络是基于前馈神经网络增加了反馈调节机制。 多层网络使用简单感知器的权重调整规则显然不够用，BP神经网络算法（即误差反向传播算法（error backpropagation））是针对多层前馈神经网络设计的。也适用于其它类型的神经网络，如训练递归神经网络。 基本思想给定训练集$D= \{ (x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\},x_i \in \mathbb{R}^d,y_i \in \mathbb{R}^l$，输入示例由$d$个输入神经元，输出$l$维实值向量。 $d$个输入神经元、$l$个输出神经元、$q$个隐含层神经元的多层前馈网络结构，其中输出层第$j$个神经元阈值为$\theta_j$，隐含层第$h$个神经元阈值为$\gamma_h$。 输入层第$i$个神经元与隐含层第$h$个神经元的连接权重为$v_{ih}$，隐含层第$h$个神经元与输出层第$j$个神经元之间的连接权重为$w_{hj}$。 记：隐含层第$h$个神经元接收到的输入为$\alpha_h = \sum \limits_{i=1}^dv_{ih}x_i$ 输出层第$j$个神经元接收到的输入为$\beta_j = \sum\limits_{h=1}^q w_{hj}b_h$，其中$b_h$为隐含层第$h$个神经元的输出。 假设隐含层和输出层都使用sigmoid函数，则对训练例$(x_k,y_k)$，假定网络输出为$\hat{y_k} = (\hat{y_1}^k,\hat{y_2}^k,\dots,\hat{y_l}^k)$，即： $\hat{y_j}^k =f(\beta_j - \theta_j)$ 隐含层输出：$b_h = f(\alpha_h- \gamma_h)$ 在$(x_k.y_k)$上的均方误差为： $E_k = \frac{1}{2} \sum \limits_{j=1}^l (\hat{y_j}^k - y_j^k)^2$ 其中有：输入层到隐含层$d \times q$个权值、隐层到输出层$q \times l$个权值、$q$个隐含层神经元的阈值、$l$个输出层神经元的阈值 参数需要确定。 任意参数$v$的更新估计为：$v \leftarrow v+ \Delta v$。 BP算法基于梯度下降（gradient descent），以目标负梯度方向对参数进行调整。 $E_k = \frac{1}{2} \sum \limits_{j=1}^l (\hat{y_j}^k - y_j^k)^2$ $\hat{y_j} = f\left( \sum\limits_{h=1}^q w_{hj}b_h-\theta_j\right)$ 令：$b_{q+1} = -1$，则：$\hat{y_j} = f\left( \sum\limits_{h=1}^{q+1} w_{hj}b_h\right)$ 令：$\beta_j = \sum\limits_{h=1}^q w_{hj}b_h$ 根据链式法则：$E_k$首先受到$\hat{y_j}$的影响，接着$\hat{y_j}$受到$\beta_j$的影响，$\beta_j$直接受到$w_{hj}$的影响。即： $\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y_j}} · \frac{\partial \hat{y_j}}{\partial \beta_{j}} · \frac{\partial \beta_j}{\partial w_{hj}} $ $ \qquad= (\hat{y_j} -y_j)·f’(\beta_j)·b_h $ $ \qquad =(\hat{y_j} -y_j)·\frac{1}{ 1+ e^{-\beta_j}}·(1- \frac{1}{1+e^{-\beta_j}})·b_h $ $\qquad = (\hat{y_j} -y_j)·\hat{y_j}·(1-\hat{y_j})·b_h$ sigmoid函数一个性质：$f’(x) = f(x)[1-f(x)]$ 对于误差$E_k$，给定学习率$\eta$，有： $\Delta w_{hj} = - \eta \frac{\partial E_k}{\partial w_{hj}}$,其中：$\eta \in (0,1)$ 令：$g_j =-\frac{\partial E_k}{\partial \hat{y_j}}·\frac{\partial \hat{y_j}}{\partial \beta_j} \ \quad= \hat{y_j}·(1-\hat{y_j})·(y_j - \hat{y_j})$ 其中$g_j$表示梯度项。 所以有： $\Delta w_{hj} = \eta g_j b_h$ $\Delta \theta_j = -\eta g_j$，即：$b_h =-1$ 类似可得： $\Delta v_{ih} = \eta e_h x_i$ $\Delta \gamma_h = - \eta e_h$，其中：$e_h = b_h(1-b_h)\sum\limits_{j=1}^l w_{hj}g_j$ $e_h = - \frac{\partial E_k}{\partial b_h}·\frac{\partial b_h}{\partial \alpha_h} \ \quad= -\sum\limits_{j=1}^l \frac{\partial E_k}{\partial \beta_j}·\frac{\partial \beta_j}{\partial b_h} ·\frac{\partial b_h}{\partial \alpha_h}$ 因为$\frac{\partial b_h}{\partial \alpha_h} = f’(\alpha_h-\gamma_h)$，利用sigmoid函数性质可得： $\frac{\partial b_h}{\partial \alpha_h} = f’(\alpha_h - \gamma_h)=f(\alpha_h - \gamma_h)·[1-f(\alpha_h -\gamma_h)] \ \qquad = b_h·(1-b_h)$ $\frac{\partial E_k}{\partial \beta_j} = -g_j$ $\frac{\partial \beta_j}{\partial b_h} =w_{hj}$ 所以：$e_h = b_h(1-b_h)\sum\limits_{j=1}^l w_{hj}g_j$ 学习率$\eta \in (0,1)$控制着沿反向梯度方向下降的步长，若： 步长太大则下降太快容易发生振荡；步长太小则收敛速度太慢。一般令$\eta = 0.1$，有时更新权重时会将输出层和隐含层设置不同的学习率。 基本流程 输入：训练集$D=\{(x_k,y_k)\}_{k=1}^m$ 过程： 1：在$(0,1)$范围内随机初始化网络中所有连接权值和阈值 2：repeat{ ​ 3： for all $(x_k,y_k) \in D$ do ​ 4：根据当前参数计算当前样本输出$\hat{y_k}$ ​ 5：计算输出层神经元梯度项$g_j$ ​ 6：计算隐含层神经元梯度项$e_h$ ​ 7：更新连接权重$w_{hj},v_{ih}$与阈值$\theta_j,\gamma_h$ ​ 8：end for ​ } 9：until 达到停止条件 输出：连接权重和阈值确定的多层前馈神经网络 相关说明标准BP算法： 更新规则是基于每一个样本的预测值与真实输出的均方误差来进行调节，每次更新只针对单个样例，所实现的最终目标是：最小化整个训练集$D$上的累积误差，即： $E= \frac{1}{m} \sum \limits_{k=1}^m E_k$ 累积BP算法： 基于累计误差最小化的更新规则，得到累积误差逆传播算法(accumulated error backpropagation)，即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数的更新频率相比标准BP算法低很多。 当数据量很大时，往往标准BP算法会获得较好的结果。 BP神经网络的强大的学习能力容易造成过拟合问题，有以下两种策略缓解BP网络的过拟合问题： 早停：将数据分为训练集和测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，测试集的累积误差增加，则停止训练。 引入正则化(regularization)：在累积误差函数中增加一个用于描述网络复杂度的部分，如所有权值与阈值的平方和，其中$\lambda \in (0,1)$，用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。其中误差目标函数变为： $E= \lambda \frac{1}{m}\sum\limits_{k=1}^m E_k + (1+\lambda)\sum\limits_i w_i^2$，增加连接权与阈值平方和这一项，训练过程将会偏好较小的连接权重和阈值，使得网络输出更加“光滑”，缓解过拟合。 全局最小与局部最小模型学习的过程实质是：寻找最优参数的过程。 在BP算法试图通过最速下降来寻找使得累积误差最小的权值和阈值时，谈到最优时，一般会提到局部最小(local minimum)和全局最小(global minimum)。 局部极小解：参数空间中的某个点，其领域点的误差函数值均不小于该点的误差函数值。 全局最小解：参数空间的某个点，所有其他点的误差函数值均不小于该点的误差函数值。 局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，但全局最小只有一个。 梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处梯度为0，则表示达到一个局部最小，参数更新停止。 如果误差函数仅有一个局部极小，即为全局最小。但是如果具有多个局部极小，则不能保证找到的解是全局最小，陷入局部极小情况。 现实任务中，常采用以下策略尽可能的去接近全局最小： 以多组不同参数初始化多个神经网络，按标准方法，迭代停止后，取其中误差最小的解作为最终参数。可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果。 使用“模拟退火”(simulated annealing)算法，在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。 使用随机梯度下降法，即在计算梯度时加入随机因素，使得在局部最小时，计算的梯度可能不为0，从而有机会跳出局部极小，迭代可以继续进行。 其他常见神经网络RBF网络径向基函数(radial basis function)网络是一种单隐含层前馈神经网络，使用径向基函数作为隐含层神经元激活函数，而输出层则是对隐含层神经元输出的线性组合。 假定输入为$d$维向量$\pmb x$，输出为实数值，则RBF网络可以表示为： $\varphi(\pmb x) = \sum\limits_{i=1}^q w_i \rho(\pmb x,c_i)$ 其中$q$是隐含层神经元个数，$c_i$和$w_i$分别是第$i$个隐含层神经元所对应的中心和权重，$\rho(\pmb x,c_i)$是径向基函数，是某种沿径向对称的标量函数，通常定义为样本$\pmb x$到数据中心$c_i$之间的欧式距离的单调函数。常用的高斯径向基函数形如： $\rho(\pmb x,c_i) =e^{-\beta_i ||\pmb x-c_i||^2}$ 【park and sandberg,1991】已证明。具有足够多的隐含层神经元的RBF网络能以任意精度逼近任意连续函数。 训练RBF网络步骤： 1、确定神经元中心$c_i$，常用的方式包括随机采样、聚类等。 2、利用BP算法等来确定参数$w_i,\beta_i$。 RBF函数网络补充径向基函数： 径向基函数(Radical Basis Function,RBF)，由Powell在1985年提出，径向基函数是某种沿径向对称的标量函数。通常定义为空间中任一点$x$到某一个中心$c$之间的欧氏距离的单调函数，记作$\phi_j(x-c_j)$，其作用往往是局部的，即当$x$远离$c$时函数取值很小，例如高斯径向基函数（也称为高斯核函数（Kernel Function）)： $\phi_j(x-c_j) = \exp \left[ - \frac{(x-c_j)^T(x-c_j)}{2\sigma_j^2}\right]$，其中$\sigma_j$是第$j$个RBF神经元的参数（标准差，高斯函数窗大小）。 径向基网络： 区别于BP网络，BP网络使用的是sigmoid函数作为激活函数，RBF使用的高斯核函数是钟型。 输出：$y_k(x) = \sum\limits_{j=1}^J w_{kj} \phi_j(x-c_j)，j=1,2,\dots,J$ 其中$J$是RBF中间层神经元的数目，由聚类得到；$c_j$是RBF的第$j$个中心向量。 核函数有很多形式，通常用到的就是Gaussian核。即： $\phi_j(x-c_j) = \exp \left[ - \frac{(x-c_j)^T(x-c_j)}{2\sigma_j^2}\right]$ RBF两阶段学习算法： 聚类中心向量初始化。确定$J$的初始聚类中心向量$c_{10},c_{20},\dots,c_{J0}$，数据集中通常把最初的$J$个数据作为初期聚类中心向量。 数据输入及对应的聚类选择。输入$x_i$，而且选择最近的聚类$j^$，其中：$c_j^ = \arg \max |x_i - c_j| \implies $表示当$|x_i - c_j|$取最小值时的$x_i$的取值。这一步对全部数据执行。 聚类中心向量更新。$c_j^{new} = \frac{1}{N_j} \sum X，X \in cluster(j)$。这一步对全部数据执行。 判断。如果全部聚类中心向量没有变化，即聚类中心确定，进入第Ⅱ阶段，否则，进入第2步。 第Ⅰ阶段 第Ⅱ阶段 中间层与输出层数值初始化。$w_{kj}$用较小的随机数设定。 网络输出计算。输入样本（数据），由$y_k(x)，\phi_j(x-c_j)$计算出网络输出。其中：$\sigma_j^2 = \frac{1}{N_j} \sum\limits_{X\in cluster(j)}(X-c_j)^T(X-c_j)$ 权值更新。首先求出各个输出神经元的误差，即：$e_k = d_k - y_k$。其中$d_k$表示期望输出，$y_k$表示实际输出。 接下来更新： $w_{kj}^{new} = w_{kj}^{old} + \eta \times e_k \times \phi_j(X-c_j)$，$\eta$是一可调常数。 判定。如果满足事先确定的条件，训练结束，否则，返回步骤2。 RBF网络只训练中间层和输出层。 ART网络竞争学习(competitive learning)是神经网络中一种常用的无监督学习策略。网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。(胜者通吃，winner-take-all)原则。 ART(adaptive resonance theory,自适应谐振理论)网络是竞争学习的一个重要代表， 网络构成：比较层、识别层、识别阈值和重置模块构成。 其中，比较层接收输入样本，并将其传递给识别层神经元；识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类。 接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元。 竞争机制：计算输入向量和每个识别层神经元所对应的模式类的代表向量之间的距离，小者获胜，获胜神经元向其他识别层神经元发送信号，抑制其激活。 若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本归入该代表向量所属类别，同时网络连接权值更新，使得以后在接收相似输入样本式该模式类会计算出更大的相似度，从而使得获胜神经元有更大可能获胜。 若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量设置为当前输入向量。 识别阈值作用：较高，输入样本会被分成更多，分类较为精细；识别阈值较低，会产生比较少、比较粗略的模式类。 ART网络更好的缓解竞争学习中的可塑性-稳定性窘境(stability-plasticity dilemma)。 可塑性：神经网络要有学习新知识的能力 稳定性：神经网络在学习新知识时要保持对旧知识的记忆 这就使得ART网络具有一个很重要的优点：可进行增量学习(incremantal learning)或在线学习(online learning)。 现在ART网络发展成了一个算法族，包括能处理实值输入的ART2网络、结合模糊处理的FuzzyART网络，以及可进行监督学习的ARTMAP网络等。 SOM网络SOM(self-organizing map，自组织映射)网络是一种竞争学习型的无监督神经网络，能将高维输入数据映射到低维空间（二维），同时保持输入数据在高维空间的拓扑结构，即： 将高维空间中的相似样本点映射到网络输出层中的邻近神经元。 SOM网络中的输出层神经元以矩阵方式排列在二维空间，每个神经元有一个权向量，网络在接收到输入向量后，会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置。 SOM训练的目标时为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的。 作用： 聚类 特征提取 类似性和典型性的表现 输入数据的相互关系 信息压缩 训练过程： 接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为获胜者，最佳匹配单元(best matching unit)； 最佳匹配单元及其邻近神经元的权向量被调整，以使得这些权向量与当前输入样本的距离缩小，不断迭代，直至收敛。 算法步骤： 网络初始化：对权值用较小的随机数设定 输入向量的加载：对输入向量$\pmb X = [x_1,x_2,\dots,x_N]^T$ 计算映射层的权值和输入向量的距离，映射层第$j$个神经元和输入向量的距离为：$d_j = \sum\limits_{i=1}^M (x_i-w_{ji})$，$w_{ji}$为权值。 与权值向量的距离最小的神经元选择，若$d_j$最小，则称之为“胜出神经元”，即为$d_j^*$ 权值学习，胜出的神经元和位于其领域的其他神经元的权值更新：$\Delta w_{ji} = \eta \times h(j,j^)\times (x_i-w_{ji})$。其中$\eta$为学习常数，$h(j,j^)$为领域函数，$h(j.j^) = \exp \left[ - \frac{|j -j^|^2}{\sigma^2}\right]$，$\sigma$是高斯窗函数的窗大小，随着学习进行而减小。 返回2，重复2-5步骤，直至收敛。 级联相关网络一般的神经网络模型：假定网络的结构事先固定，训练的目的是利用训练样本来确定合适的连接权重、阈值等参数。 结构自适应网络：网络的结构也是学习的目标之一，并希望能在训练过程中找到最符合数据特点的网格结构，级联相关(cascade-correlation)网络是其中代表。【ART网络的隐含层神经元数目在训练过程中可以增长，也是一种结构自适应神经网络】 级联相关网络的训练过程如下图所示： 新的隐节点加入时，红色的连接权通过最大化新节点的输出与网络误差之间的相关性来进行训练。 级联相关网络两个主要成分：“级联”和“相关”。 级联是指建立层次连接的层级结构。开始训练时，网络只有输入层和输出层，处于最小拓扑结构；随着训练的进行，新的隐层神经元逐渐加入，从而创建起层级结构。 相关是指通过最大化新的神经元的输出与网络误差之间的相关性来训练相关参数。 与一般前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经元数目，且训练速度较快，但在数据较小时容易陷入过拟合。 Elman网络递归神经网络(recurrent neural networks)允许网络中出现环状结构，从而使得一些神经元输出反馈回来作为输入信号，这使得网络在$t$时刻的输出状态不仅与$t$时刻的输入有关，还与$t-1$时刻的网络状态有关，从而能处理与时间有关的动态变化。 Elman网络是递归神经网络之一，其结构如下图所示： 隐含层的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐含神经元在下一时刻的输入，隐含层神经元通常采用sigmoid激活函数，网络的训练常通过推广的BP算法进行。 Boltzmann机神经网络中有一类型是为网络状态定义一个“能量”，能量最小化时网络达到理想状态，网络的训练过程就是在最小化这个能量函数。 Boltzmann机是一种“基于能量的模型”(energy-based-model)，常见结构如下图所示： 神经元分为两层：隐层和显层 显层用于表示数据的输入和输出，隐层可以理解为数据的内在表达。 Boltzmann机中的神经元都是布尔型，只能取0、1两种状态，状态1表示激活，状态0表示抑制。令向量$\pmb s \in \{0,1\}^n$表示$n$个神经元状态，$w_{ij}$表示神经元$i$与$j$之间的连接权，$\theta_i$表示神经元$i$的阈值，则状态向量$\pmb s$所对应的Boltzmann机能量定义为： $E(\pmb s) = - \sum\limits_{i=1}^{n-1} \sum\limits_{j=i+1}^{n}w_{ij}s_is_j - \sum\limits_{i=1}^n \theta_is_i$ 若网络中的神经元以任意不依赖于输入值的顺序进行更新，则网络最终将达到Boltzmann分布，此时状态向量$\pmb s$出现的概率将仅由其能量与所有可能状态向量的能量决定，即： $P(\pmb s) = \frac{e^{-E(\pmb s)}}{\sum\limits_{\pmb t} e^{-E(\pmb t)}}$ Boltzmann机训练过程就是将每个训练样本视为一个状态向量，使其出现的概率尽可能大。 标准Boltzmann机是一个全连接图，训练网络的复杂度很高，现实中常使用受限Boltzmann机(Restricted Boltzmann Machine，RBM)，仅保留显层和隐层之间的连接，将Boltzmann机结构由完全图简化为二部图。 RBM通常采用“对比散度”(contrastive divergence，CD)算法进行训练。假定网络中有$d$个显层神经元和$q$个隐层神经元，令$\pmb v$和$\pmb h$分别表示显层和隐层的状态向量，则由于同一层内不存在连接，有： $P(\pmb v|\pmb h) = \prod\limits_{i=1}^d P(v_i| \pmb h)$ $P(\pmb h|\pmb v) = \prod\limits_{j=1}^q P(h_i| \pmb v)$ CD算法对每个训练样本$\pmb v$，先计算出隐层神经元状态的概率分布，然后据此采样得到$\pmb h$；此后，类似从$\pmb h$产生$\pmb v’$，再从$\pmb v’$产生$\pmb h’$；连接权更新公式为： $\Delta w= \eta\left( \pmb v \pmb h^T - \pmb v’ \pmb h^{‘T}\right)$ 深度学习理论上，参数越多，模型复杂度越高，容量(capability)越大，从而能完成更复杂的学习任务。 增加模型复杂度方法： 增加隐含层数目 增加隐含层神经元数目 前者更有效些，不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。 对于多隐含层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散(diverge)，无法收敛到稳定状态。 有效训练多隐含层神经网络方法一般有以下两种： 无监督逐层训练(unsupervised layer-wise training)：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐节点训练完，输出作为下一层的输出来训练，称为预训练(pre-training)。全部预训练结束，再对整个网络进行微调(fine-tuning)训练。一个典型例子：深度信念网络(deep belief network,DBN)，把大量参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。 权共享(weight sharing)：令同一层神经元使用完全相同的连接权重，典型的例子是卷积神经网络(convolutional neural network,CNN)，这样做大大减小需要训练的参数数目。 深度学习可以理解为一种特征学习(feature learning)或者表示学习(representation learning)，无论DBN还是CNN，都是通过多个隐含层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使得原来的只是通过单层映射难以完成的任务变为可能。 即通过多层处理，逐渐将初始“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单模型来完成复杂的学习任务。 传统任务，样本的特征需要人类专家来设计，特征的好坏对泛化性能至关重要，而深度学习为全自动数据分析带来可能，可以通过学习自动产生更好的特征。 吴恩达深度学习笔记 支持向量机支持向量机基础SVM（support vector machine），通俗来讲是一种二类分类器，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略是间隔最大化，最终转化为一个凸二次规划问题的求解。 分类标准的起源：logistic回归给定一些数据点，分别属于两个不同的类，找到一个线性分类器将其分为两类，用$x$表示数据点，$y$表示类别（$y\in \{-1,1\}$，分别代表两个不同的类），一个线性分类器的学习目标是要在$n$维的数据空间找到一个超平面（hyper plane）,这个超平面的方程表示 为：$$\begin{equation}w^Tx +b=0\end{equation}$$ 定义（Logistic回归）： 从特征学习出一个0/1模型，该模型是将特性的线性组合作为自变量，由于自变量的取值范围是$[-\infty , \infty]$，因此使用sigmoid函数将自变量映射到$(0,1)$上，映射后的值被认为是属于$y=1$的概率。 假设函数：$$h_{\theta}(x) = g(\theta^T x)= \frac{1}{1+e^{-\theta^Tx}}$$其中，$x$是$n$维特征向量，函数$g$是Logistic函数。而$g(z) = \frac{1}{1+e^{-z}}$图像如下图所示，将无穷映射到$(0,1)$。 假设 就是属于$y=1$的概率：$$\begin{equation}\begin{aligned}&amp; P(y=1|x;\theta)=h_{\theta}(x) \\&amp; P(Y=0|X;\theta)=1-h_{\theta}(x)\end{aligned}\end{equation}$$当判断一个新的特征属于哪一类时，只需要求出$h_{\theta}(x)$即可，若$h_{\theta}(x) &gt;0.5 \implies y=1$，否则$y=0$。 线性分类函数和Logistic回归形式化表示为：$$h_{\theta}(x) = g(\theta^Tx)=g(w^Tx+b)$$进一步简化为$g(z)=g(w^Tx+b)$，将其简单映射到$y=-1$和$y=1$上。映射关系如下：$$\begin{equation}g(x)=\begin{cases}1 &amp;z \ge 0\\-1 &amp; z&lt;0\end{cases}\end{equation}$$ 线性分类一个例子一个二维平面，两种不同的数据，这些数据线性可分，用一条直线将这两类数据分开，这条直线相当于一个超平面，超平面一边的数据点所对应的$y=-1$，另一边对应$y=1$。 这个超平面可以用分类函数$f(x) = w^Tx +b$表示，有： $f(x)=0 \qquad x位于超平面上$ $f(x)&gt;0 \qquad y=1$ $f(x)&lt;0 \qquad y=-1$ 几点说明： 二类问题，分类标签$y$只取任意两个值。 大部分数据不是线性可分，这个时候满足条件的超平面不存在。 进行分类时，遇到一个新的数据点$x$，代入$f(x)$，如果$f(x)&gt;0$，则将$x$类别划为-1，反之，划为1。 如何确定超平面，这个超平面应该是最适合分类两类数据的直线，“最合适”的标准是这条直线离直线的两边的数据的间隔最大。即要寻找最大间隔的超平面。 函数间隔与几何间隔在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点$x$到距离超平面的远近^1 ^1: 这里“远近”是一个相对概念，如果计算点到超平面的距离为：$\frac{w^Tx +b}{||w||}$ 通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确，即可以用$y(w^Tx+b)$的正负性来判定或表示分类的正确性。 ==函数间隔（function margin）== 定义为：$$\hat{\gamma} = y(w^T x +b)=yf(x)$$超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔最小值便是超平面$(w,b)$关于训练数据集$T$的函数间隔，即：$$\hat{\gamma} = \min \hat{\gamma}_i,(i=1,\dots,n)$$这种定义存在一个问题：当$w,b$成比例的改变，如$2w,2b$，则函数间隔的值变为原来的2倍，但是超平面没有改变。需要对法向量加一些约束条件。这样得到： ==几何间隔（geometrical margin）== 定义为： 假定对于一个点$x$，令其垂直投影到超平面上的对应点为$x_0$，$w$是垂直于超平面的一个向量，$\gamma$是样本$x$到分类间隔的距离。如下图所示： 有：$x=x_0 + \gamma \frac{w}{||w||}$，其中$||w||$表示的是二阶范数。 因为$x_0$是超平面上的点，满足$f(x_0) =0$，代入超平面的方程$w^Tx +b =0$，即：$$\gamma = \frac{w^Tx +b}{||w||}= \frac{f(x)}{||w||}$$为了得到$\gamma$的绝对值，令$\gamma$乘上对应的类别$y$，即可得到几何间隔$\tilde{\gamma}$：$$\tilde{\gamma} = y\gamma = \frac{\hat{\gamma}}{||w||}$$可见：几何间隔$\tilde{ \gamma }$就是函数间隔$\hat{\gamma}$除以$||w||$，而且函数间隔$y(w^Tx+b) = yf(x)$实际上就是$|f(x)|$。$\frac{|f(x)|}{||w||}$是直观上的点到超平面的距离。 最大间隔分类器定义对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）越大。因此，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个间隔值。 函数间隔不适合用来最大化间隔值，因为超平面固定后，可以等比例的缩放$w,b$的值，这样使得$f(x) = w^Tx +b$的值任意大。几何间隔因为除了$||w||$，使得缩放的时候几何间隔$\tilde{\gamma}$的值不会改变，只是随着超平面变动而变动，所以，==要找的最大间隔分类超平面是找的几何间隔==。 最大间隔分类器(maximum margin classifier)的目标函数定义为：$$\max \tilde{\gamma}$$同时需要满足一些条件，有：$$y_i(w^Tx_i +b) = \hat{\gamma}_i \ge \hat{\gamma},i=1,\dots,n$$由于几何间隔$\tilde{\gamma} = y\gamma = \frac{\hat{\gamma}}{||w||}$，如果令函数间隔$\hat{\gamma}=1$^2，则有$\tilde{\gamma} = \frac{1}{||w||}$，且$y_i(w^Tx_i +b) \ge 1,i=1,\dots,n$，因此上述目标函数转化为：$$\max \frac{1}{||w||} ,\quad s.t.\quad y_i(w^Tx_i +b) \ge 1,i=1,\dots,n$$ 中间实线即是寻找到的最优超平面^3（Optional Hyper Plane)，其到两条虚线的距离相等，这个距离即是几何间隔$\tilde{\gamma}$，两条虚线之间的距离等于$2\tilde{\gamma}$，虚线上的点即是==支持向量==，它们满足$y(w^Tx +b) =1$，而对于所有不支持向量的点，则显然有$y(w^Tx+b) &gt;1$。 支持向量机深入从线性可分到线性不可分给定训练样本集：$$\Omega = \{(x_i,y_i)|i=1,2\dots,n\}\subset \mathcal{R}^m \times\{-1,1\}$$其中，$x_i$表示样本，$y_i$表示标签，分为-1和1两类。 假设样本集$\Omega$是线性可分的，即存在超平面：$$w^Tx +b =0 ,\quad x \in\mathcal{R}^m$$对任意一个$(x_i,y_i) \in \Omega$，满足：$$\begin{cases}w^Tx_i +b \ge +1,当y_i = +1\\w^Tx_i +b \le -1,当y_i = -1\end{cases}$$称该平面为==最优超平面==。 上式可统一写为：$$y_i (w^Tx_i +b) \ge 1,\quad s.t. \quad i=1,2,\dots,n$$其中，$w=(w_1,w_2,\dots,w_m)^T \in \mathcal{R}^m$，$x_i =(x_{i1},x_{i2},\dots,x_{im})^T \in \mathcal{R}^m$ 内积：$w^Tx_i = \sum\limits_{j=1}^m w_j x_{ij}$ 空间$\mathcal{R}^m$的点$t=(t_1,\dots,t_m)^T \in \mathcal{R}^m$到超平面的距离为：$$d= \frac{|w^T·t+b|}{||w||},\quad ||w|| = \sqrt{w^Tw}$$超平面“margin”定义：记margin = $d_+ + d_{-}$，表示正负两类样本。有：$$d_{+} = \min \left\{\frac{|w^Tx_i +b|}{||w||} \right\},\quad s.t. \quad i \in \{1,2,\dots,m|y_i = +1\}$$ $$d_{-} = \min \left\{\frac{|w^Tx_i +b|}{||w||} \right\},\quad s.t. \quad i \in \{1,2,\dots,m|y_i = -1\}$$ 当存在点使得$w^Tx_i +b = \pm 1$时，$d_{+} = d_{-} = \frac{1}{||w||}$，此时：$${\rm margin} = \frac{2}{||w||}$$【定义】：对于给定的线性可分样本集$\Omega$，如果存在分离超平面$w^Tx+b=0$，使得margin最大，则称该平面为最优超平面。即目标函数转化为：$$\min \frac{1}{2}||w||^2 \implies margin 最大,\quad s.t. \quad y_i(w^Tx_i +b)\ge 1,i=1,\dots,n$$此时，目标函数是二次，约束条件是线性的，即是一个凸二次规划问题。所以，在一定的约束条件下，目标最优，损失最小。 由于这个问题结构的特殊性，可以通过拉格朗日对偶性(Lagrange Duality)变换到对偶变量(Dual Variable)的优化问题，即通过求解与原问题等价的对偶问题(Dual Problem)得到原始问题的最优解^4。 具体过程如下： 给每个约束条件加上拉格朗日乘子$\alpha$，得到：$$\mathcal{L}(w,b,\alpha) = \frac{1}{2}||w||^2-\sum\limits_{i=1}^n \alpha_i[y_i(w^Tx_i +b)-1]$$令：$$\min\limits_{w,b}\theta(w) =\min\limits_{w,b} \max \limits_{\alpha_i \ge 0}\mathcal{L}(w,b,\alpha) = p’$$显然当所有目标函数满足时，有$\theta(w) = \frac{1}{2}||w||^2$，当然$\alpha_i \ge 0 ,(i=1,\dots,n)$，否则$\theta(w)$会无穷大。 此处$p’$是这个问题的最优值，且和最初的问题是等价的，如果直接求解，有$w,b$两个参数，且$\alpha_i$是不等式约束，求解过程不好做。将上式改为：$$\max\limits_{\alpha_i \ge 0}\min\limits_{w,b} \mathcal{L}(w,b,\alpha) = d’$$这样可以先求$\mathcal{L}$对$w,b$的极小，再求$\mathcal{L}$对$\alpha$的极大。 交换后的新问题是原始问题的对偶问题，最优值用$d’$表示，且有$d’ \le p’$，在满足某些条件下，这两者相等^5，这样就可以通过求解对偶问题来间接求解原始问题[^6]。 [^6]: 关于对偶问题的最优等价性问题，参见Stephen Boyd的《Convex Optimization》中第五章。 K.K.T条件一般地，一个最优化数学模型能够表示成下列标准形式：$$\begin{equation}\begin{aligned}\min \quad &amp; f(x)\\\\s.t.\quad &amp; h_j(x) =0, \quad j=1,\dots,p \\\\\quad &amp; g_k(x)\le 0,\quad k=1,\dots,q \\\\\quad &amp;x \in \mathcal{X} \subset \mathcal{R}^n\end{aligned}\end{equation}$$其中$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p,q$分别是等式约束和不等式约束的数量。 两点说明： 凸优化的概念：$\mathcal{X} \subset \mathcal{R}^n$为一个凸集，$f:\mathcal{X} \rightarrow \mathcal{R}$为一凸函数。凸优化就是要找出一点$x’ \in \mathcal{X}$，使得每一个$x \in \mathcal{X}$满足$f(x’) \le f(x)$。 KKT条件的意义：它是一个非线性规划(Nonlinear Programming)问题能有最优化解法的充要条件。 【定义】（K.K.T条件）： 对于上面的优化问题，满足：$$\begin{equation}\begin{aligned}&amp;h_j(x’) = 0,\quad j=1,\dots,p\\\\&amp;g_k(x’) \le 0,\quad k=1,\dots,q\\\\&amp;\Delta f(x’) + \sum\limits_{j=1}^{p} \lambda_j \Delta h_j(x’) + \sum\limits_{k=1}^q \mu_k\Delta g_k(x’)=0\\\\&amp;\lambda_j \ne 0,\mu_k \ge 0\\\\&amp;\mu_k g_k(x’) =0\end{aligned}\end{equation}$$则为满足$KKT$条件。 求解这个对偶问题，分为三个步骤：首先让$\mathcal{L}$关于$w,b$最小化，然后求对$\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。 对偶问题求解$$\mathcal{L}(w,b,\alpha) = \frac{1}{2}||w||^2-\sum\limits_{i=1}^n \alpha_i[y_i(w^Tx_i +b)-1]$$ （1）首先固定$\alpha$，然$\mathcal{L}$关于$w,b$的最小化。$$\begin{equation}\begin{aligned}&amp;\frac{\partial \mathcal{L}}{\partial w}=0 \implies w= \sum\limits_{i=1}^n \alpha_iy_ix_i \\\\&amp;\frac{\partial \mathcal{L}}{\partial b}=0 \implies \sum\limits_{i=1}^n \alpha_iy_i=0\end{aligned}\end{equation}$$代入原式有：$$\begin{equation}\begin{aligned}\mathcal{L}(w,b,\alpha)&amp;=\frac12 w^Tw -\sum\limits_{i=1}^n \alpha_iy_iw^Tx_i -\sum\limits_{i=1}^n \alpha_iy_ib +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= \frac12 w^T\sum\limits_{i=1}^n \alpha_iy_ix_i -\sum\limits_{i=1}^n \alpha_iy_iw^Tx_i -\sum\limits_{i=1}^n \alpha_iy_ib +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= \frac12 w^T\sum\limits_{i=1}^n \alpha_iy_ix_i -w^T\sum\limits_{i=1}^n \alpha_iy_ix_i -\sum\limits_{i=1}^n \alpha_iy_ib +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= -\frac12 w^T\sum\limits_{i=1}^n \alpha_iy_ix_i -\sum\limits_{i=1}^n \alpha_iy_ib +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= -\frac12 w^T\sum\limits_{i=1}^n \alpha_iy_ix_i -b\sum\limits_{i=1}^n \alpha_iy_i +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= -\frac12 \left(\sum\limits_{i=1}^n \alpha_iy_ix_i\right)^T\sum\limits_{i=1}^n \alpha_iy_ix_i -b\sum\limits_{i=1}^n \alpha_iy_i +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= -\frac12 \sum\limits_{i=1}^n \alpha_iy_i(x_i)^T \sum\limits_{i=1}^n \alpha_iy_ix_i -b\sum\limits_{i=1}^n \alpha_iy_i +\sum\limits_{i=1}^n \alpha_i \\\\&amp;= -\frac12 \sum\limits_{i,j=1}^n \alpha_iy_i(x_i)^T \alpha_jy_jx_j -b\sum\limits_{i=1}^n \alpha_iy_i +\sum\limits_{i=1}^n \alpha_i \\\\&amp;=\sum\limits_{i=1}^n \alpha_i -\frac{1}{2}\sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jx_i^Tx_j\end{aligned}\end{equation}$$此时拉格朗日函数只包含一个变量$\alpha_i$。 （2）求对$\alpha$的极大。$$\begin{equation}\begin{aligned}\max\limits_{\alpha}&amp;\quad \sum\limits_{i=1}^n \alpha_i - \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jx_i^Tx_j\\\\s.t. &amp;\quad \alpha_i \ge 0,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$求出$\alpha_i$，从而根据：$$\begin{equation}\begin{aligned}w’ &amp;= \sum\limits_{i=1}^n \alpha_i y_ix_i\\\\b’ &amp;= -\frac{\max\limits_{i:y_i=-1}{w’}^Tx_i +\min\limits_{i:y_i=-1}{w’}^Tx_i}{2}\end{aligned}\end{equation}$$即可求出$w,b$，最终得到最优分离超平面和分类决策函数。 （3）利用SMO算法^7求解对偶问题中的拉格朗日乘子$\alpha$。 线性不可分对于一个数据点$x$进行分类，实际上就是通过代入到$f(x) = w^Tx +b$算出结果，然后根据正负号来进行类别划分。 前面推导出：$$\begin{equation}\begin{aligned}w’ &amp;= \sum\limits_{i=1}^n \alpha_i y_ix_i\\\\b’ &amp;= -\frac{\max\limits_{i:y_i=-1}{w’}^Tx_i +\min\limits_{i:y_i=-1}{w’}^Tx_i}{2}\end{aligned}\end{equation}$$因此分类函数为：$$\begin{equation}\begin{aligned}f(x)&amp;= (\sum\limits_{i=1}^n \alpha_i y_ix_i)^Tx +b \\\\&amp;=\sum\limits_{i=1}^n \alpha_i y_i\left&lt;x_i,x\right&gt; +b\end{aligned}\end{equation}$$其中$\left&lt;x_i,x\right&gt;$表示向量内积。 可见，==对于新点$x$的预测，只需要计算它与训练数据点的内积即可，这也是之后使用Kernel进行非线性推广的前提==。 事实上，所有非Support Vector所对应的系数$\alpha$都是等于零，因此对于新点的内积计算实际上只是针对少量的“支持向量”而不是所有的训练数据。 因为，分类完全是由超平面决定的，非支持向量的样本点不会参与分类问题的计算，所以非支持向量所对应的拉格朗日乘子$\alpha$等于零。 所谓的支持向量机就是maximum margin hyper plane classifier。到目前为止，SVM还很弱，只能处理线性情况。通过Kernel可以推广到非线性的情况。 核函数特征空间的隐式映射通过将数据映射到高维空间，来解决原始空间上线性不可分问题。 在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。 ==原始方法==： 线性学习器学习一个非线性关系，需要选择一个非线性特征集，将数据写成新的表达形式，等价于应用一个固定的非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器。 假设：$$f(x) = \sum\limits_{i=1}^N w_i \phi_i(x) +b$$其中$\phi : \mathcal{X} \rightarrow \mathcal{F}$是从输入空间到某个特征空间的映射。 建立非线性学习器分两步： 首先使用非线性映射将数据变换到一个特征空间$\mathcal{F}$ 然后在特征空间使用线性学习分类器 由于对偶形式是线性学习器的一个重要性质，因此，决策规则可以用测试点和训练点的内积表示：$$f(x)=\sum\limits_{i=1}^l \alpha_i y_i \left&lt; \phi(x_i),\phi(x)\right&gt; +b$$==核方法==： 在特征空间直接计算内积$\left&lt; \phi(x_i),\phi(x)\right&gt; $，将上面两个步骤融合在一起建立一个非线性的学习器。 【定义】（核：Kernel）： 核是一个函数$K$，对所有的$x,z \in \mathcal{X}$，满足$K(x,z) = \left&lt; \phi(x_i),\phi(x)\right&gt; $，其中$\phi : \mathcal{X} \rightarrow \mathcal{F}$是从输入空间$\mathcal{X}$到内积特征空间$\mathcal{F}$的映射。 核函数：如何处理非线性数据 如图所示两类数据，线性不可分，理想的分界应该是一个圆圈而不是一条直线（超平面）。用$X_1,X_2$表示这个二维平面的两个坐标。一条二次曲线（圆是一种特殊的二次曲线）的方程可以写成：$$a_1X_1+a_2X_1^2+a_3X_2+a_4X_2^2+a_5X_1X_2+a_6=0$$构造成一个五维空间，其中五个坐标值分别为：$$Z_1 =X_1,Z_2 = X_1^2,Z_3 =X_2 ,Z_4 = X_2^2,Z_5 =X_1X_2$$上式可以写成：$$\sum\limits_{i=1}^5a_iZ_i +a_6 =0$$ 关于新的坐标$Z$，正是一个hyper plane方程。即做一个映射$\phi : \mathcal{R}^2 \rightarrow \mathcal{R}^5$，将$X$按照上面的规则映射为$Z$，那么在新的空间中原来的数据变成线性可分，从而能够使用之前的线性分类算法进行处理。==$Kernel$方法处理非线性问题的基本思想==。 假设两个向量$x_1 = (\eta_1,\eta_2)^T$和$x_2 = (\xi_1,\xi_2)^T$，而$\phi(·)$即五维空间的映射，则映射过后的内积为：$$\left&lt;\phi(x_1),\phi(x_2)\right&gt; = \eta_1\xi_1 + \eta_1^2\xi_1^2 + \eta_2\xi_2+\eta_2^2\xi_2^2 +\eta_1\eta_2\xi_1\xi_2$$又因为：$$(\left&lt;x_1,x_2\right&gt; +1)^2 =2\eta_1\xi_1 + \eta_1^2\xi_1^2 + 2\eta_2\xi_2+\eta_2^2\xi_2^2 +2\eta_1\eta_2\xi_1\xi_2 +1 \tag{Ⅰ}$$可见，将某几个维度线性缩放一下，再加上一个常数维度，该式就等于上式。 这个式子和映射：$$\phi(x_1,x_2) = (\sqrt{2}x_1,x_1^2,\sqrt{2}x_2,x_2^2,\sqrt{2}x_1x_2,1)^T \tag{Ⅱ}$$之后的内积$\left&lt;\phi(x_1),\phi(x_2)\right&gt;$的结果一致的，两者的区别再于： 一个（Ⅱ）是映射到高维空间中，然后再根据内积公式进行计算 另一个（Ⅰ）直接再原来的低维空间进行计算，不需要显式地写出映射后的结果 当维度爆炸时，（Ⅱ）已经无法计算，然而（Ⅰ）依旧能够处理，甚至时无穷维度的情况也没有问题。 ==把计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数(Kernel Function)==，例如，上面例子中的，核函数为：$$K(x_1,x_2) =(\left&lt;x_1,x_2\right&gt; +1)^2$$ 核函数能简化映射空间中的内积运算，在SVM中需要计算的数据向量总是以内积形式出现。有：$$f(x)=\sum\limits_{i=1}^n\alpha_iy_iK(x_i,x)+b$$转化为如下对偶问题计算而得：$$\begin{equation}\begin{aligned}\max\limits_{\alpha}&amp;\quad \sum\limits_{i=1}^n \alpha_i - \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jK(x_i,x_j)\\\\s.t. &amp;\quad \alpha_i \ge 0,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$ 几个核函数 多项式核 $K(x_1,x_2) = (\left&lt;x_2,x_2\right&gt; +R)^d$。所对应的映射空间的维度是$C_{m+d}^d$，其中$m$是原始空间的维度。 高斯核 $K(x_1,x_2) = \exp (-\frac{||x_1-x_2||^2}{2\sigma^2})$。通过调控参数$\sigma$，高斯核具有相当高的灵活性。如将$\sigma$选的很小，理论上可以将任意数据映射为线性可分，但是可能会有严重的过拟合问题。 线性核 $K(x_1,x_2) = \left&lt;x_1,x_2\right&gt;$。即原始空间上的内积。主要目的是将“映射后的空间中的问题”和“映射前空间中的问题”两者的形式上统一起来。(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的。 核函数本质 实际中遇到的线性不可分的样本，把样本特征映射到高维空间上，相关特征在高维空间被分开，达到分类的目的。 但是映射到高维空间，会造成维度爆炸问题。 此时需要核函数，核函数存在的价值是，虽然将特征从低维到高维的转换，但是它事先在低维上计算，将实质的分类效果表现在高维上，这样避免了直接在高维空间上的复杂计算。 使用松弛变量处理outliers方法 因为数据本身带有噪声，而不是数据的非线性结构，如上图中黑圈圈起来的点，偏离正常位置很远的数据点，称之为outlier。 在原来的SVM模型中，outlier存在有可能会造成很大的影响，因为hyper plane本身只有少数的support vector组成的，如果这些support vector中又存在outlier，影响很大。 如上图所示，这个outlier的出现，使得分离超平面不得不被挤歪，变成图中的黑色虚线所示，同时margin也相应的变小。更严重的是，如果这个outlier再往右移动一些距离，将无法构造出能将数据分开的超平面来。 在SVM中，允许数据点在一定的程度上偏离超平面，（上图中的黑色实线所对应的距离即outlier的偏离距离）。如果把这个outlier移动回来，刚好落在原来的超平面上，而不会使得超平面发生变形。 在有松弛的情况下，outlier点也属于支持向量，同时对于不同的支持向量，拉格朗日参数的值也不同。对于远离分离平面的点值为0；对于在边缘上的点值在$[0,\frac1L]$之间，其中$L$为训练数据集的大小；对于outlier数据和内部的数据值为$\frac1L$。 原来的约束条件为：$$y_i(w^Tx_i +b) \ge 1，\quad i=1,\dots,n$$现在考虑到outlier问题，约束条件变为：$$y_i(w^Tx_i +b) \ge 1 - \xi_i，\quad i=1,\dots,n$$其中$\xi_i$称为松弛变量(slack variable)，==对应数据点$x_i$允许偏离(functional margin)的的量。== 同时在原来的目标函数后加上一项，使得$\xi_i$的总和最小。（因为如果允许$\xi_i$任意大，则任意的超平面都是符合条件了）。即：$$\min \quad \frac12||w||^2 + C\sum\limits_{i=1}^n \xi_i$$其中$C$是一个参数，用于==控制目标函数的”寻找margin最大的超平面“和”保证数据点偏差量最小“之间的权重。== 注意到，$\xi$是需要优化的变量之一，$C$是一个事先确定好的常量。完整式子为：$$\begin{equation}\begin{aligned}\min &amp;\quad \frac12||w||^2 +C\sum\limits_{i=1}^n \xi_i\\\\s.t. &amp;\quad y_i(w^Tx_i +b) \ge 1 - \xi_i，\quad i=1,\dots,n\\\\&amp;\quad \xi_i \ge 0,\quad i=1,\dots,n\end{aligned}\end{equation}$$将约束条件加入到目标函数中，得到新的拉格朗日函数，如下：$$\mathcal{L}(w,b,\alpha,r) = \frac12 ||w||^2 +C\sum\limits_{i=1}^n \xi_i - \sum\limits_{i=1}^n \alpha_i [y_i(w^Tx_i +b)-1+\xi_i]-\sum\limits_{i=1}^n r_i \xi_i$$先是$\mathcal{L}$对$w,b$和$\xi$的最小化：$$\begin{equation}\begin{aligned}&amp;\frac{\partial \mathcal{L}}{\partial w}=0 \implies w= \sum\limits_{i=1}^n \alpha_iy_ix_i \\\\&amp;\frac{\partial \mathcal{L}}{\partial b}=0 \implies \sum\limits_{i=1}^n \alpha_iy_i=0 \\\\&amp;\frac{\partial \mathcal{L}}{\partial \xi_i}=0 \implies C-\alpha_i -r_i =0, \quad i=1,\dots,n\end{aligned}\end{equation}$$然后将$w$带回$\mathcal{L}$化简得到：$$\max\limits_{\alpha}\quad \sum\limits_{i=1}^n \alpha_i - \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jx_i^Tx_j$$但是增加了约束条件：$$\begin{equation}\begin{aligned}\begin{cases}C-\alpha_i -r_i =0 \\\\r_i \ge 0 \\\\\end{cases}\quad\implies&amp; \alpha_i \le C\end{aligned}\end{equation}$$综上，整个问题转变为：$$\begin{equation}\begin{aligned}\max\limits_{\alpha}&amp;\quad \sum\limits_{i=1}^n \alpha_i - \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jx_i^Tx_j\\\\s.t. &amp;\quad 0 \le\alpha_i \le C,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$唯一的区别是，多了个约束条件$\alpha_i \le C$的上限。 同样Kernel化的非线性形式同样，即为：$$\begin{equation}\begin{aligned}\max\limits_{\alpha}&amp;\quad \sum\limits_{i=1}^n \alpha_i - \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jK(x_i,x_j)\\\\s.t. &amp;\quad 0 \le \alpha_i \le C,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$==【小结】：== 通俗来说，SVM本质上是一个分类方法，用$w^Tx+b$定义分类函数，然后求$w,b$，为寻找最大margin，找到最优分离超平面，引出$ \min \frac{1}{2}||w||^2$，继而引入拉格朗日乘子$\alpha$，转化为对拉格朗日乘子$\alpha$的求解。 求解过程涉及一系列的最优化或凸二次规划问题。这样求$w,b$与求$\alpha$等价，而$\alpha$的求解可以使用一种快速学习算法SMO。 至于核函数，是为了处理非线性情况，若直接映射到高维空间计算会有维度爆炸问题，因此在低维计算，然后将实质效果在高维表现。 SMO算法SMO算法用于求解对偶问题的序列最小最优化算法。$$\begin{equation}\begin{aligned}\max\limits_{\alpha}&amp;\quad \sum\limits_{i=1}^n \alpha_i - \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jx_i^Tx_j\\\\s.t. &amp;\quad 0 \le\alpha_i \le C,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$等价于求解：$$\begin{equation}\begin{aligned}\min\limits_{\alpha}\Psi(\overrightarrow{\alpha})&amp;=\min\limits_{\alpha} \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jK(x_i,x_j)-\sum\limits_{i=1}^n \alpha_i \\\\s.t. &amp;\quad 0 \le \alpha_i \le C,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$ 参考John C. Platt的文章《Sequential Minimal Optimization: A Fast Algorithm for Training SVM》。 SMO算法推导： 定义特征到结果的输出函数：$$u=\overrightarrow{w}·\overrightarrow{x}-b$$与$f(x) = w^Tx +b$性质一样。 重新定义原始优化问题如下：$$\min\limits_{w,b}\frac12 ||w||^2 ,\quad s.t. \quad y_i(\overrightarrow{w}·\overrightarrow{x}-b) \ge 1,\forall i$$求导得：$$\overrightarrow{w} = \sum\limits_{i=1}^N y_i \alpha_i\overrightarrow{x_i}\\\\b= \overrightarrow{w}·\overrightarrow{x_k}-y_k ,\quad \alpha_k &gt;0$$代入$u=\overrightarrow{w}·\overrightarrow{x}-b$中，可得：$$u= \sum\limits_{j=1}^Ny_j \alpha_jK(\overrightarrow{x_j},\overrightarrow{x})-b$$引入拉格朗日乘子转化为对偶问题，得到：$$\begin{equation}\begin{aligned}\min\limits_{\alpha}\Psi(\overrightarrow{\alpha})&amp;=\min\limits_{\alpha} \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_j\left&lt;x_i,x_j\right&gt;-\sum\limits_{i=1}^n \alpha_i \\\\s.t. &amp;\quad 0 \le \alpha_i ,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$加入松弛变量后，模型修改为：$$\min\limits_{w,b,\xi} \frac12 ||\overrightarrow{w}||^2 +C\sum\limits_{i=1}^n \xi_i \quad s.t. \quad y_i(\overrightarrow{w}·\overrightarrow{x}-b) \ge 1-\xi_i,\forall i \\\\0\le \alpha_i \le C, \forall i$$最终问题变为：$$\begin{equation}\begin{aligned}\min\limits_{\alpha}\Psi(\overrightarrow{\alpha})&amp;=\min\limits_{\alpha} \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jK(x_i,x_j)-\sum\limits_{i=1}^n \alpha_i \\\\s.t. &amp;\quad 0 \le \alpha_i \le C,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$接下来要解决的问题是：==在$\alpha_i =\{ \alpha_1,\dots,\alpha_n\}$上求上述目标函数的最小值==。 每次从中任意抽取两个乘子$\alpha_1$和$\alpha_2$，然后固定除去这两个以外的其他乘子$\{\alpha_3,\dots,\alpha_n\}$，使得目标函数只是关于$\alpha_1$和$\alpha_2$的函数。这样不断从一堆乘子中任意抽取两个求解，不断地迭代求解子问题，最终达到求解原问题的目的^8。 如果存在不满足K.K.T条件的$\alpha_i$，则需要更新这些$\alpha_i$，这是第一个约束条件。此外，还要满足：$$\sum\limits_{i=1}^n \alpha_i y^{(i)} =0$$具体过程如下： 假设选取了初始值$\{\alpha_1,\alpha_2,\dots,\alpha_n\}$中的$\alpha_1，\alpha_2$，固定剩余参数$\{\alpha_3,\dots,\alpha_n\}$，此时$\Psi$就是$\alpha_1,\alpha_2$的函数。并假定两个乘子再更新之前分别是 $\alpha_1^{old},\alpha_2^{old}$，更新之后分别是$\alpha_1^{new},\alpha_2^{new}$，并且满足条件：$$\alpha_1^{old} y^{(1)} + \alpha_2^{old} y^{(2)} =\alpha_1^{new} y^{(1)} + \alpha_2^{new} y^{(2)}= -\sum\limits_{i=3}^n\alpha_iy^{(i)} = \zeta$$其中，$\zeta$为常数，因为$\{\alpha_3,\dots,\alpha_n\}$为固定值，且有约束条件$\sum\limits_{i=1}^n \alpha_i y^{(i)} =0$。 这两个因子不好同时求解，先求解乘子$\alpha_2^{new}$，再用其表示$\alpha_1^{new}$。 ==首先要确定$\alpha_2^{new}$的取值范围。== 假设它的上下边界分别为$H$和$L$。有：$$L \le \alpha_2^{new} \le H \tag{1}$$结合约束条件$$0 \le \alpha_i \le C,i=1,\dots,n \\\ \tag{2}$$ $$\alpha_1^{old} y^{(1)} + \alpha_2^{old} y^{(2)} =\alpha_1^{new} y^{(1)} + \alpha_2^{new} y^{(2)}= \zeta \tag{3}$$ 求取$\alpha_2^{new}$的取值范围。 ① 当$y^{(1)} \ne y^{(2)}$时，即一个等于-1，一个等于1，根据约束条件$(3)$可得：$\alpha_1^{old} - \alpha_2^{old} = \zeta$。可以表示为一条斜率为1的直线。 $\alpha_1,\alpha_2$既要在矩形方框内，也要在直线上，所以有：$L= \max \{0,-\zeta \}$，$H=\min \{C, C-\zeta\}$ ② 当$y^{(1)} = y^{(2)}$时，根据约束条件$(3)$可得：$\alpha_1^{old} +\alpha_2^{old} = \zeta$。 所以有：$L= \max \{0,\zeta -C\}$，$H=\min \{C,\zeta\}$ 因此根据$y^{(1)},y^{(2)}$异同号，可以得到$\alpha_2^{new}$的上下界分别为：$$\begin{equation}\begin{aligned}\begin{cases}L=\max \{0,\alpha_2^{old} -\alpha_1^{old}\} ,H=\min\{ C,C+\alpha_2^{old} -\alpha_1^{old}\} \quad {\rm if} \quad y^{(1)} \ne y^{(2)} \\\\L=\max \{0,\alpha_2^{old} +\alpha_1^{old} -C\} ,H=\min\{ C,\alpha_2^{old} +\alpha_1^{old}\} \quad {\rm if} \quad y^{(1)} = y^{(2)}\end{cases}\end{aligned}\end{equation}$$ ==然后再分析目标函数==。$$\begin{equation}\begin{aligned}\min\limits_{\alpha}\Psi(\overrightarrow{\alpha})&amp;=\min\limits_{\alpha} \frac12 \sum\limits_{i,j=1}^n \alpha_i \alpha_jy_iy_jK(x_i,x_j)-\sum\limits_{i=1}^n \alpha_i \\\\s.t. &amp;\quad 0 \le \alpha_i \le C,i=1,\dots,n \\\\&amp;\quad \sum\limits_{i=1}^n \alpha_iy_i =0\end{aligned}\end{equation}$$这个目标函数的子问题（即只含有$\alpha_1,\alpha_2$当作参数）可以表达为：$$\Psi = \frac12 K_{11}\alpha_1^2 + \frac12 K_{22}\alpha_2^2 +sK_{12}\alpha_1\alpha_2 + y_1 \alpha_1v_1 +y_2\alpha_2v_2-\alpha_1-\alpha_2 +\Psi_{constant} \tag{4}$$其中：$$K_{ij} = K(\overrightarrow{x_i}，\overrightarrow{x_j}) \\\\v_i = \sum\limits_{j=1}^n y_i \alpha_j’ K_{ij} = u_i +b’ -y_1\alpha_1’K_{1i} - y_2\alpha_2’K_{2i}$$其中，$\alpha_1’,\alpha_2’$表示某次迭代之前的原始值，因此是常数，$\alpha_1,\alpha_2$是待求的变量。$\Psi_{constant}$是常数。 与之对应的K.K.T条件为：$$\alpha_i =0 \iff y_iu_i \ge 1 \\\\0 &lt; \alpha_i &lt; C \iff y_iu_i =1 \\\\\alpha_i =C \iff y_iu_i \le 1$$这个K.K.T条件说明，在两条间隔线外面的点，对应的前面的系数$\alpha_i =0$，在两条间隔线里面的对应的$\alpha_i =C$，在两条间隔线上的对应的系数$\alpha_i$在0与$C$之间。 考虑到对于第二个约束条件$\alpha_1^{old} y^{(1)} + \alpha_2^{old} y^{(2)} =\alpha_1^{new} y^{(1)} + \alpha_2^{new} y^{(2)}= -\sum\limits_{i=3}^n\alpha_iy^{(i)} = \zeta$，两边乘以$y^{(1)}$，可得：$$y^{(1)}[\alpha_1^{old} y^{(1)} + \alpha_2^{old} y^{(2)} ]=y^{(1)}[\alpha_1^{new} y^{(1)} + \alpha_2^{new} y^{(2)}]= -y^{(1)}\sum\limits_{i=3}^n\alpha_iy^{(i)}$$可简写成：$$\alpha_1 + s\alpha_2 = \alpha_1’ + s\alpha_2’ =w$$其中，$w= -y^{(1)}\sum\limits_{i=3}^n\alpha_i’ y^{(i)}$ 因此，可以用$\alpha_2$来表示$\alpha_1$，即$\alpha_1 = w- s\alpha_2$。 然后把式子$\alpha_1 = w- s\alpha_2$代入式子$(4)$可得：$$\Psi = \frac12 K_{11}(w-s\alpha_2)^2 + \frac12 K_{22}\alpha_2^2 +sK_{12}(w-s\alpha_2)\alpha_2 \\\ \quad+ y_1 (w-s\alpha_2)v_1 +y_2\alpha_2v_2-w+s\alpha_2-\alpha_2 +\Psi_{constant} \tag{5}$$这样式$(5)$只有变量$\alpha_2$，然后对其求导可得：$$\frac{d\Psi}{d\alpha_2} = -sK_{11}(w-s\alpha_2) +K_{22}\alpha_2 -s^2K_{12}\alpha_2 +sK_{12}(w-s\alpha_2)-y_1v_1s+s+y_2v_2-1$$如果$\Psi$的二阶导大于0（凹函数），那么一阶导数为0，即极小值，假设一阶导数为0，上式可简化为：$$\alpha_2(K_{11} +K_{22} -2K_{22})=s(K_{11} -K_{12})w +y_2(v_1-v_2) +1-s$$其中：$$s=y_1y_2 \\\\\alpha_1 +s\alpha_2= \alpha_1’ +s\alpha_2’ =w \\\\K_{ij} = K(\overrightarrow{x_i},\overrightarrow{x_j})\\\\v_i =\sum\limits_{j=3}^{n}y_j \alpha_j’ K_{ij} = u_i +b’ -y_i \alpha_1’K_{1i} -y_2\alpha_2’ K_{2i}$$代入上式可得到：$$\alpha_2^{new,unclipped} (K_{11}+K_{22} -2K_{22})=\alpha_2^{old}(K_{11}+K_{22} -2K_{22}) +y_2(u_1-u_2+y_2-y_1)$$令$E_i = u_i -y_i$（表示预测值和真实值之差），$\eta =K(\overrightarrow{x_1},\overrightarrow{x_1}) +K(\overrightarrow{x_2}),\overrightarrow{x_1} -2K(\overrightarrow{x_1},\overrightarrow{x_2})$ 上式两边同时除以$\eta$，得到一个关于单变量$\alpha_2$的为剪辑的解：$$\alpha_2^{new,unclipped} = \alpha_2 + \frac{y_2(E_1-E_2)}{\eta}$$==这个解没有考虑到约束条件$0 \le \alpha_2 \le C$，即是为剪辑时的解。== 加入该约束条件后，得到剪辑过后的$\alpha_2^{new}$的解为：$$\begin{equation}\alpha_2^{new} =\begin{cases}H,\quad \alpha_2^{new,unclipped}&gt;H \\\\\alpha_2^{new.unclipped}, L \le \alpha_2^{new,unclipped } \le H \\\\L, \quad \alpha_2^{new,unclipped}&lt; L\end{cases}\end{equation}$$其中，$L,H$分别是$\alpha_2^{new}$的上下界，即：$$\begin{equation}\begin{aligned}\begin{cases}L=\max \{0,\alpha_2^{old} -\alpha_1^{old}\} ,H=\min\{ C,C+\alpha_2^{old} -\alpha_1^{old}\} \quad {\rm if} \quad y^{(1)} \ne y^{(2)} \\\\L=\max \{0,\alpha_2^{old} +\alpha_1^{old} -C\} ,H=\min\{ C,\alpha_2^{old} +\alpha_1^{old}\} \quad {\rm if} \quad y^{(1)} = y^{(2)}\end{cases}\end{aligned}\end{equation}$$ 求出$\alpha_2^{new}$后，则$\alpha_1^{new} = \alpha_1^{old} +s(\alpha_2^{old} - \alpha_2^{new}) = \alpha_1^{old} +y_1y_2(\alpha_2^{old} - \alpha_2^{new}) $ ==选择乘子$\alpha_1,\alpha_2$步骤：== 对于$\alpha_1 $，即第一个乘子，可以通过如下办法： 因为K.K.T条件为：$$\begin{equation}\begin{aligned}\alpha_i =0 &amp;\iff y_iu_i \ge 1 \quad表明\alpha_1在边界内部，因为正确分类点满足y_if(x_i) \ge 0。\\\ 0 &lt; \alpha_i &lt; C &amp;\iff y_iu_i =1 \quad 表明\alpha_i是支持向量，在边界上。\\\\\alpha_i =C &amp;\iff y_iu_i \le 1 \quad 表明\alpha_i在两条边界之间。\end{aligned}\end{equation}$$而最优解需要满足K.K.T条件，即上述三个条件都得满足，以下几种情况出现将会不满足： $y_iu_i \le 1$，但是$\alpha_i &lt; C$，则是不满足的，而原本$\alpha_i =C$。 $y_iu_i \ge 1$，但是$\alpha_i &gt; 0$，则是不满足的，而原本$\alpha_i =0$。 $y_iu_i = 1$，但是$\alpha_i = C$或$\alpha_i = 0$，则是不满足的，而原本$0&lt;\alpha_i &lt;C$。 也就是，如果存在不满足K.K.T条件的$\alpha_i$，那么需要更新这些$\alpha_i$。 对于$\alpha_2$，可以寻找满足条件：$\max |E_i -E_j|$的乘子。 ==更新$b$== 对于$b$，在满足下述条件：$$\begin{equation}b =\begin{cases}b_1,\quad 0&lt; \alpha_1^{new} &lt; C \\\\b_2,\quad 0&lt; \alpha_2^{new} &lt; C\\\\\frac{(b_1+b_2)}{2}, \quad otherwise\end{cases}\end{equation}$$进行如下更新：$$\begin{equation}\begin{aligned}b_1^{new} = b^{old} -E_1-y_1(\alpha_1^{new} -\alpha_1^{old})K(x_1,x_1) -y_2(\alpha_2^{new} -\alpha_2^{old})K(x_1,x_2) \\\\b_2^{new} = b^{old} -E_2-y_1(\alpha_1^{new} -\alpha_1^{old})K(x_1,x_2) -y_2(\alpha_2^{new} -\alpha_2^{old})K(x_2,x_2)\end{aligned}\end{equation}$$且每次更新完两个乘子的优化后，都需要再重新计算$b$，及对应的$E_i$的值。 最优更新完所有的$\alpha_i,y$和$b$，得到模型，即可求出分类函数：$$f(x) = \sum\limits_{i=1}^n \alpha_i y_i \left&lt;x_i,x\right&gt; +b$$ SMO算法的步骤 ： 假定在某一次迭代中，需要更新$x_1,x_2$所对应的拉格朗日乘子$\alpha_1,\alpha_2$，这个小规模的二次规划问题可以写成：$$L_s =\max\limits_{\alpha} \left\{(\alpha_1+\alpha_2) + \sum\limits_{i=3}^n \alpha_i - \frac12 || \alpha_1 y_1 \phi(x_1) + \alpha_2y_2\phi(x_2) + \sum\limits_{i=3}^n \alpha_iy_i\phi(x_i)|| \right\} \\\\s.t. \quad \alpha_1y_1 +\alpha_2y_2 = -\sum\limits_{i=3}^n \alpha_iy_i \quad 0&lt; \alpha_i &lt; C, \forall i$$==首先选取要更新的乘子：== step1：先“扫描”所有乘子，把第一个违反K.K.T条件的作为更新对象，令其为$\alpha_2$。 step2：在所有不违反K.K.T条件的乘子中，选取使得$|E_1-E_2|$最大的$\alpha_1$进行更新，使得能在最大限度增大目标函数的值。类似梯度下降法。【==其中，$E_i = u_i -y_i$，而$u= \overrightarrow{w}·\overrightarrow{x} -b$。所以得到的$E$代表函数$u_i$对输入$x_i$的预测值与真实输出类标记$y_i$之差。==】 ==然后更新拉格朗日乘子$\alpha_1,\alpha_2$：== step1：计算上下界$L$和$H$：$$\begin{equation}\begin{aligned}\begin{cases}L=\max \{0,\alpha_2^{old} -\alpha_1^{old}\} ,H=\min\{ C,C+\alpha_2^{old} -\alpha_1^{old}\} \quad {\rm if} \quad y^{(1)} \ne y^{(2)} \\\\L=\max \{0,\alpha_2^{old} +\alpha_1^{old} -C\} ,H=\min\{ C,\alpha_2^{old} +\alpha_1^{old}\} \quad {\rm if} \quad y^{(1)} = y^{(2)}\end{cases}\end{aligned}\end{equation}$$step2：计算$L_s$的二阶导数：$$\eta =2\phi(x_1)^T\phi(x_2) -\phi(x_1)^T\phi(x_1) -\phi(x_2)^T\phi(x_2)$$step3：更新$L_s$：$$\alpha_2^{new} = \alpha_2^{old} -\frac{y_2(e_1-e_2)}{\eta} \\\\e_i = g^{old}(x_i) -y_i$$step4：计算变量$\alpha_2$：$$\begin{equation}\alpha^{temp} =\begin{cases}H,\quad \alpha_2^{new}&gt;H \\\\\alpha_2^{new}, L \le \alpha_2^{new} \le H \\\\L, \quad \alpha_2^{new}&lt; L\end{cases}\end{equation}$$step5：更新$\alpha_1$：$$\alpha_1^{new} = \alpha_1^{old} +y_1y_2(\alpha_2^{old} - \alpha^{temp})$$ ==最后每次更新完两个乘子优化后，都需要再重新计算$b$，以及对应的$E_i$的值。== ==小结：== SMO算法基本思想是将Vapnik在1982年提出的Chunking方法推到极致，SMO算法每次迭代只选出两个分量$\alpha_i$和$\alpha_j$进行调整，其它分量 则保持不变，在得到解$\alpha_i$和$\alpha_j$后，再利用$\alpha_i$和$\alpha_j$进行改进其它分量。 与通常的分解算法相比较，尽管需要更多的迭代次数，但是每次迭代的计算量比较小，所以该算法表现出快速的收敛性，且不需要存储核矩阵，没有矩阵运算。 支持向量深入理解 优化搜索贪心算法算法思想 贪心算法总是做出在当前看来最好的选择（贪心选择），并不从整体上进行最优考虑，所作出的选择只是局部最优选择。 虽不能对所有的问题都得到整体最优解，但对很多问题能产生整体最优解，如最小生成树问题。在一些情况下，即使贪心算法不能得到整体最优解，其最终结果却是最优解的很好的近似。 贪心算法不是对所有问题都能得到整体最优解，选择的贪心策略必须是具备==无后效性==，即某个状态以后的过程不会影响以前的状态，至于当前状态有关。 贪心策略适用的前提是：局部最优策略能导致产生全局最优解。 最优子结构：当一个问题的最优解包括其子问题的最优解时，称此问题具有最优子结构性质。 基本思路 建立数学模型来描述问题 把求解的问题分成若干个子问题 对每一个子问题求解，得到子问题的局部最优解 把子问题的局部最优解合成原来解问题的一个解 实现过程 从问题的某一个初始解出发； 123456789while（能朝着给定总目标前进一步）&#123;​ 利用可行的决策，求出可行解的一个解元素；&#125;由所有解元素组合成问题的一个可行解。 旅行商问题TSP(Traveling Salesman Problem)即旅行商问题，假设假设有一个旅行商人要拜访n个城市，他必须选择所要走的路径，路径的限制是每个城市只能拜访一次，而且最后要回到原来出发的城市。路径的选择目标是要求得的路径长度为所有路径之中的最小值。TSP是一个典型的组合优化问题，且是一个==NP完全难题==。即TSP问题目前尚不能找到一个多项式时间复杂度算法来求解。 P类问题：所有可以在多项式时间内求解的判定问题。 判定问题：回答结果输出为yes或no的问题。 时间复杂度：衡量当问题规模变大后，程序执行的所需时间增长多快。如$O(1)$表示常数级别，即不管问题规模变大多少倍，所耗时间不会改变；$O(n^2)$表示平方级别，即当问题规模增大至2倍时，所耗费时间则放大到4倍；$O(2^n)$表示指数级别，即当问题规模倍数扩大时，所用时间会呈指数放大。 多项式时间：指$O(1),O(\log n),O(n^2)$等这类可用多项式表示的时间复杂度，通常认为计算机可解决的问题只限定于多项式时间内，而$O(2^n),O(n!)$这类非多项式级别的问题，其复杂度计算机接受不了。 NP类问题：所有非确定性多项式时间内可解的判定问题。 NP类问题将问题分为求解和验证两个阶段，问题的求解是非确定性的，无法在多项 式时间内得到答案，而问题的验证却是确定的，能够在多项式时间里确定结果。 如：是否存在一个公式可以计算下一个质数？这个问题的答案是无法直接计算出来，但是一旦某人给出了一个公式，可以在多项式时间里对这个公式进行验证。 NP完全问题：NP(Non-deterministic Polynomial)的缩写，即多项式复杂程度的非确定性问题，也称为NP-C问题。是NP中一类特殊的问题。 这类问题的每个问题的复杂度与整个类的复杂度有关联性，假如其中任意一个问题在多项式时间内可解，则这一类问题都是多项式时间可解。 在决策树算法中，寻找最优决策树是一个NP完全问题，决策树问题无法利用计算机在多项式时间内，找出全局最优的解。因此，大多数决策树算法都采用==启发式算法==，如贪心算法，来指导对假设空间的搜索。即决策树算法最后的结果，是在每一步、每一个节点上做局部最优选择，无法保证为全局最优。 解决方案 最优的方法是==穷举法==，但是穷举法效率不高。目前针对于如何通过一个多项式时间复杂度的算法快速求出先后次序，主流方法是采用一些随机的、启发式的搜索算法，比如==遗传算法、模拟退火算法、粒子群算法等==。 但上述的这些算法都有一个缺点，不一定能找到最优解，只能收敛（近似逼近）最优解，得到一个次优解，因为本质上都是随机算法，都以类似“一定概率接受或舍去”的思路去筛选解。 TSP问题求解大概是由两步构成的： 计算两两城市之间的最短路径：利用类似Dijkstra、Flord、A星的算法找出最短路线。 计算最短巡回路径：利用类似遗传算法、蚁群算法的搜索算法求巡回拜访的次序。 动态规划算法 DP(Dynamic Programming)通常用于求解具有某种最优性质的问题，其基本思想是将待求解问题分解为若干个子问题，先求子问题，然后由这些子问题再得到原问题的解。【类似贪心算法】 DP是一种求解TSP问题的很好的方法。 实例操作 假设现在有四个城市，分别是0，1，2，3。之间的来往代价如下图所示： 写成矩阵的形式：$$C=\pmatrix{0 &amp;3 &amp;6 &amp;7\ 5&amp;0&amp;2&amp;3\\6&amp;4&amp;0&amp;2\\3&amp;7&amp;5&amp;0}$$其中第一行表示从城市0到城市0，1，2，3四个的代价。以此类推。 ==下面简单推导一下动态规划问题==： 用$V’$表示一个点的集合，假如从定点$s$出发，$d(i,V’)$表示当前到达顶点$i$，经过$V’$集合中所有顶点一次最小花费。 当$V’$为仅包含起点的集合，即：$d(s,\{s\}) =0$。 其它情况，则对子问题求最优解。需要在$V’$这个城市集合中，尝试每一个城市的结点，并求出最优解。$$d(i,V’+\{i\}) =\min\limits_{k\in V’}\{d(k,V’) +c_{ki}\}$$ 最后的求解方式：$$Answer = \min\limits_{i\in S}\{d(i,S) +c_{is}\}$$其中$S$为包含所有点的集合。 模拟退火算法简介爬山法是一种贪心算法，对于一个优化问题，其大致图像如下所示： 其目标是找到函数的最大值，若初始化时，初始点的位置在$C$处，则会寻找附近的局部最大值$A$处，由于$A$点是一个局部最大值，对于爬山法来讲，该算法无法跳出局部最大值。 若初始点选在$D$处，则根据爬山法，则会找到全局最大值点$B$点。这也说明基于贪婪的爬山法是否能够取得全局最优解与初始值选取有很大的关系。 模拟退火算法(Simulated Annealing，SA)的思想借鉴于固体的退火原理。当固体的温度很高的时候，内能比较大，固体的内部粒子处于快速无序运动，当温度慢慢降低的过程中，固体的内能减小，粒子慢慢趋于由徐，最终，当固体处于常温时，内能达到最小，此时，粒子最为稳定。 SA从某一个较高的温度出发，称为初始温度。伴随着温度参数不断下降，算法中的解趋于稳定，但是，这个稳定解可能是局部最优解，此时，SA会以一定的概率跳出这个局部最优解，以寻找目标函数的全局最优解。如上图所示，寻找到$A$点处的解，SA会以一定的概率跳出这个解，如跳到$D$点重新寻找，这样在一定程度上增加了寻找全局最优解的可能性。==即局部最优解能概率性地跳出并最终趋于全局最优。== 相关概念退火：将固体加热到足够高的温度，使得分子呈随机排列状态，然后逐步降温使之冷却，最后分子以低能状态排列，达到某种稳定状态。 退火过程： 加温过程：增强粒子运动，消除系统原先可能存在的非均匀态。 等温过程：对于与环境换热而温度不变的封闭系统，系统的状态自发变化总是朝着自由能减少的方向进行，当自由能达到最小时，系统达到平衡。 冷却过程：使粒子热运动减弱并渐趋下降，从而得到低能的晶体结构。 Boltzmann分布： 同一温度，分子停留在能量小的状态的概率大于停留在能量大的概率。 温度越高，不同能量状态对应的概率相差越小，温度足够高时，各状态对应的概率基本相同。 随着温度下降，能量最低状态对应的概率越来越大，温度趋于0时，其状态趋于1。 Metropolis准则： 固体在恒定温度下达到热平衡的过程可以用Metropolis方法加以模拟。 ==温度恒定时==：若温度为$T$，当前状态$i$ ，新状态$j$，若$E_i &lt; E_j$，则接受$j$为当前状态；否则，若概率$p= e^{ \frac{-(E_j-E_i)} {k_B \times T}}$大于$[0,1)$区间的随机数，则仍接受状态$j$为当前状态；若不成立则保留状态 $i$为当前状态。 ==温度变化时：==在高温下，可接受当前状态能量差较大的新状态；在低温下，只接受与当前状态能量差较小的新状态。 算法原理SA算法来源于固体退火原理，将固体加温到充分高，再让其逐渐冷却，加温时，固体内部粒子随温度升高变为无序状态，内能增大，逐渐冷却的粒子渐趋有序，在每一个温度都达到平衡态，最后在常温时达到基态，内能减为最小。 根据Metropolis准则，粒子温度在$T$时趋于平衡的概率为$e^{-\frac{\Delta E}{kT}}$，其中$E$为温度$T$时的内能，$\Delta E$为其变化量，$k$为Boltzmann常数。 用固体退火模拟组合优化问题，将内能$E$模拟为目标函数值$f$，温度$T$演化为控制参数$t$，即得到解组合优化问题的模拟退火算法： 由初始解$i$和控制参数初始值$t$开始，对当前解重复”产生新解 →计算目标函数差→接受或舍弃“的迭代，并逐步衰减$t$值，算法终止时的当前解即为所得的近似最优解。 这是==基于蒙特卡洛迭代求解发的一种启发式随机搜索过程==。 退火过程由冷却进度表(Cooling Schedule)控制，包括控制参数的初值$t$及其衰减因子$\Delta t$、每个$t$值时的迭代次数$L$和停止条件$S$。 算法步骤step 1：由一个产生函数从当前解产生一个位于解空间的新解。为便于后续的计算和接受，减少算法的耗时，通常选择由当前新解经过简单地变换即可产生新解的方法，如对构成新解的全部或部分元素进行置换、互换等，注意到产生新解的变换方法决定了当前新解的领域结构，因而对冷却进度表的选取有一定的影响。 step 2：计算新解所对应的目标函数差。目标函数差仅由变换部分产生，所以目标函数差的计算最好按增量计算。这也是计算目标函数差最快的方法。 step 3：判断新解是否被接受，判断的依据时一个接受准则。常用的是Metropolis准则：若$\Delta t’ &lt;0$则接受$S’$作为当前新的解$S$，否则以概率$e^{-\frac{\Delta t’}{T}}$接受$S’$作为新的当前解$S$。 step 4：新解被确定后，用新解代替当前解。只需要将当前解对应于产生新解时的变换部分予以实现，同时修正目标函数值即可。 此时，当前解实现了一次迭代，在此基础上开始下一轮试验，而当新解被判定为舍弃时，则在原当前解的基础上继续下一轮试验。 ==附一matlab源码==： 12345678910111213141516171819202122232425262728293031323334353637383940function [xo,fo] = Opt_Simu(f,x0,u,l,kmax,q,TolFun)% 模拟退火算法求函数 f(x)的最小值点， 且 l &lt;= x &lt;= u% f为待求函数，x0为初值点，l，u分别为搜索区间的上下限，kmax为最大迭代次数% q为退火因子，TolFun为函数容许误差%%%%算法第一步根据输入变量数，将某些量设为缺省值if nargin &lt; 7 TolFun = 1e-8;endif nargin &lt; 6 q = 1;endif nargin &lt; 5 kmax = 100;end%%%%算法第二步，求解一些基本变量N = length(x0); %自变量维数x = x0;fx = feval(f,x); %函数在初始点x0处的函数值xo = x;fo = fx;%%%%%算法第三步，进行迭代计算，找出近似全局最小点for k =0:kmax Ti = (k/kmax)^q; mu = 10^(Ti*100); % 计算mu dx = Mu_Inv(2*rand(size(x))-1,mu).*(u - l);%步长dx x1 = x + dx; %下一个估计点 x1 = (x1 &lt; l).*l +(l &lt;= x1).*(x1 &lt;= u).*x1 +(u &lt; x1).*u; %将x1限定在区间[l,u]上 fx1 = feval(f,x1); df = fx1- fx; if df &lt; 0||rand &lt; exp(-Ti*df/(abs(fx) + eps)/TolFun) %如果fx1&lt;fx或者概率大于随机数z x = x1; fx = fx1; end if fx &lt; fo xo = x; fo = fx1; endendfunction x = Mu_Inv(y,mu)x = (((1+mu).^abs(y)- 1)/mu).*sign(y); 进化计算进化计算概述Darwin被认为是进化理论和共同起源法则的奠基人，Lamarck可能是把生物进化理论化的第一人。 Larmarck的进化理论是关于遗传的，如获得性状的遗传，其主要观点是生物个体在生命周期中逐渐适应环境，并将性状传给后代，后代也能不断适应。 Darwin理论认为：自然选择发生在繁殖算子中，即”最好“的父代个体有更多的机会被选择来产生后代，形成新的种群；随机改变通过变异(mutation)算子来实现。 进化计算(Evolutionary Computing)是指进化过程中作为计算模型的问题的解决系统，如自然选择、适者生存、繁殖等都是这个问题解决系统的组成部分。 一般进化算法主要受以下几个部分的影响： 编码：与染色体一样，对问题进行解编码 适应度函数：用于求适应度的函数，表征个体的生存能力 初始化：种群的初始化 选择：选择算子 繁殖(reproduction)：繁殖算子 ==一般进化算法步骤== 令代数计数器$t=0$： 创建和初始化$n_x$维群体$\Phi(0)$，包含$n_s$个个体 while 终止条件不为真 do ​ 获得每个个体$x_i(t)$的适应度值$f(x_i(t))$； ​ 采用复制算法来产生后代； ​ 选择新群体$\Phi(t+1)$； ​ 进入下一代，即$t=t+1$； end 进化算法这些步骤不断迭代，直到满足某个终止条件。每一次的迭代称为一个世代(generation)。 进化算法各部分实现的不同，形成不同的进化计算方法： 遗传算法(Genetic Algorithms,GA)，以基因进化为模型。 遗传编程(Genetic Programming,GP)，以遗传算法为模型，但个体为程序（表示为树）。 进化规划(Evolutionary Programming,EP)，来源于对进化中自适应行为的模拟（如表型进化）。 进化策略(Evolution Strategies,ES)，用于对进化过程中的控制变量进行建模，如进化的进化。 差分进化(Differential Evolution,DE)，类似于遗传算法，不同之处在其所使用繁殖机制。 文化进化(Cultural Evolution,CE)，用于对种群文化的进化以及对文化如何影响个体的基因和表现型的进化的建模。 协同进化(Co-evolution,CoE)，模拟初始”愚蠢的“个体如何通过合作或竞争来获取必要的性状以求得生存的演化过程。 ==染色体表示== 生物体拥有某些能影响生存和繁殖能力的性状，是由包含在生物细胞核中的染色体的一长串信息决定的。每个染色体包含大量的基因，基因是一种遗传单位，通过控制蛋白质的合成来决定生物个体的生理和解剖表现。每个生物个体都有唯一的基因序列。基因的另一种形式称为等位基因(allele)。 进化计算中，每个个体都代表是一个优化问题的备选解，个体的性状由染色体（或称基因组）表示。 性状是指最优化问题所搜索的变量，每个需要优化的变量称为基因——携带信息的最小单位。 个体的性状分为两类进化信息：基因型和表现型。 基因型描述了个体的基因组成，继承于父代；表现型是一个个体在特定环境中所表现出的行为特征，定义了个体的样貌。 设计进化算法的一个重要步骤就是找到备选解的合适的表示方案（如染色体）。搜索算法的效率与复杂程度依赖于这个表示方案。不同的方法往往使用不同的表示方案。多数进化算法把解表示为某种数据类型的向量，但遗传编程是把个体表示为树的形式。 ==初始化种群== 进化算法是一种基于种群的随机搜索算法，每个进化算法都会维护一个由备选解构成的种群。 产生初始种群的标准方法是在可行域中产生随机值，并分配到每个染色体的每个基因。 随机选择的目标是确保初始种群为整个搜索空间的均匀表示。 初始种群的大小会影响计算复杂性和空间探索能力。大量的个体能增加多样性，进而提高种群的空间探索能力。当然，计算量也会 更大。 在使用小个体数量的种群时，如果增加变异频率，则可迫使进化算法探索更多的搜索空间。 ==适应度函数== 在进化算法中，为确定一个个体的生存能力，用一个数学函数来表征染色体所表示的解有多好，即适应度函数。 进化算子，如选择、交叉、变异和精英选择，都会用适应度函数来评估染色体。如选择父代个体用于交叉时，选择算子会更倾向最适应的个体，而变异算子会倾向最不适应的个体。 ==选择== 选择算子的主要目标是获得更好的解，通过进化算法的两个步骤来完成： 新种群的选择：在每一代的结束，一个由备选解构成的新种群会被选择作为下一代种群。新种群可以仅从后代中选择，或者同时从后代与父代中选择，选择算子的目标是确保优良个体能存活到下一代。 繁殖：后代通过交叉或变异算子生成。在交叉中，优良个体应该有更多的机会去繁殖，以确保后代包含最好个体的基因，在变异中，选择机制关注”劣势“个体，目的是引入更好的性状，以增加劣势个体的生存能力。 选择方法包括：随机选择、比例选择、锦标赛选择、排序选择、玻尔兹曼选择、$(\mu^+,\lambda)$选择、精英选择和名人堂(hall of fame)。 ==繁殖算子== 繁殖是从选择的父代中应用交叉和变异算子生存子代的过程。 交叉是通过组合随机选择两个或多个父代个体的基因物质生成新个体的过程 变异是随机改变染色体基因值得过程，主要目标是使得种群引入新得基因物质，提高基因得多样性，注意不能破坏高适应度个体的优良基因，因此变异通常都是以较低的概率进行。另一方面，变异的概率与个体的适应度成反比：适应度越低的个体，变异越高 繁殖可以使用替换机制，即当且仅当新产生的子代个体的适应度优于它的父代个体时，才进行替换。 ==终止条件== 最简单的终止条件是限制进化算法执行的代数或是调用适应度函数的次数。 其他收敛准则有： 当连续几代内都没有提高时则终止 当种群中没有改变时终止 当得到一个可接受的解时终止 当目标函数斜率接近0时终止 遗传算法遗传算法概念遗传算法(Genetic Algorithm,GA)起源于对生物系统所进行研究的计算机模拟。它是模仿自然界生物进化机制发展起来的随机全局搜索和优化方法，借鉴于达尔文的进化论和孟德尔的遗传学说。 其本质是一种高效、并行、全局搜索的方法，能在搜索过程中自动获取和积累有关搜索空间的知识，并自适应地控制搜索过程以求得最佳解。 一些相关术语： 基因型(genotype)：性状染色体的内部表现 表现型(phenotype)：染色体决定的性状的外部表现，或者说，根据基因型形成的个体的外部表现 进化(evolution)：种群逐渐适应生存环境，品质不断得到改良。生物的进化是以种群的形式进行 适应度(fitness)：度量某个物种对于生存环境的适应程度 选择(selection)：以一定的概率从种群选择若干个个体，一般，选择过程是一种基于适应度的优胜劣汰的过程。 复制(reproduction)：细胞分裂时，遗传物质DNA通过复制而转移到新产生的细胞中，新细胞就继承就细胞的基因 交叉(crossover)：两个染色体在某一个相同位置处DNA被切断，前后两串分别交叉组合形成两个新的染色体。或称为基因重组或杂交 变异(mutation)：复制时可能(很小的概率)产生某些复制差错，变异产生新的染色体，表现出新的性状 编码(coding)：DNA中遗传信息在一个长链上按一定的模式排列。遗传编码可看作从表现型到基因型的映射 解码(decoding)：基因型到表现型的映射 个体(individual)：染色体带有特征的实体 种群(population)：个体的集合，该集合内个体数为种群的大小 遗传算法的应用诸如询路问题，8数码问题，囚犯困境，动作控制，找圆心问题（在一个不规则的多边形中，寻找一个包含在该多边形内的最大圆圈的圆心），TSP问题，生产调度问题，人工生命模拟等。 以袋鼠为例： 遗传算法中每一条染色体，对应着遗传算法的一个解决方案，一用适应性函数(fitness function)来衡量这个解决方案的优劣。从一个基因组到其解的适应度形成的一个映射，可以把遗传算法的过程看成一个在多元函数里面求最优解的过程。 在这个多维曲面里有很多的“山峰 ”，而这些“山峰”所对应的就是局部最优解，其中会有一个“山峰”的海拔最高，即是全局最优解。遗传算法的任务就是尽量爬到最高峰，而不是陷落在小山峰。（如果问题的适应度评价越小越好，那么全局最优解就是函数的最小值，遗传算法要找的就是“最深的谷底”。 如求解函数$f(x) = xsin(10\pi x) +2 , x\in[1,2]$，的函数最大值： “袋鼠跳”问题： 把函数曲线可以看成一个个山峰和山谷组成的山脉，把设想所得到的每一个解就是一只袋鼠（能跳），希望不断的向着更高处跳去，直到跳到最高的山峰。 几种“袋鼠跳”的方式： 爬山法(最速上升爬山法)：从搜索空间中随机产生邻近的点，从中选择对应解最优的个体，替换原来的个体，不断重复上述过程。爬山法只对“邻近”的点作比较，常常只能收敛到离开初始位置比较近的局部最优解上。对于存在很多局部最优点的问题，通过一个简单的迭代找出全局最优解的机会很小。 模拟退化法：在金属热加工过程中，当金属的温度超过它的熔点(melting point)时，原子会激烈地随机运动。原子的这种运动趋向于寻找能力极小状态。在这个能力变迁过程中，开始时，温度会非常高，使得原子具有很高的能量。随着温度不断降低，金属逐渐冷却，金属中的原子的能量越来越小，最后达到所有可能的最低点。利用模拟退火时候，让算法从较大的跳跃开始，使得它具有足够的“能量”逃离可能“路过”的局部最优解而不至于限制在其中，当它停在全局最优解附近的时候，逐渐地减小跳跃量，以便得到全局最优解。 遗传算法：模拟物竞天择地生物进化过程，通过维护一个潜在解的群体执行多方向的搜索，并支持这些方向上的信息构成和交换。是以面为单位的搜索，比以点为单位的搜索，更能发现全局最优解。 遗传算法实现过程首先寻找一种对问题的潜在解进行“数字化”编码的方案。然后用随机数初始化一个种群（建立表现型和基因型的映射关系 ），种群里面的个体即是这些数字化的编码。 接下来，通过适当的解码过程之后，用适应度函数对每一个基因个体做一次适应度评价。 用选择函数按照某种规定择优选择。 让个体基因变异，然后产生子代。 遗传算法并不保证能获得问题的最优解，但是使用遗传算法的最大优点在于不必去了解和操心如何去“找”最优解，而只要简单的“否定”一些表现不好的个体就行。 一般步骤： 开始循环直至找到满意的解： 评估每条染色体所对应的个体的适应度。 遵照适应度越高，选择概率越大的原则，从种群中选择两个个体作为父方和母方。 抽取父母双方的染色体，进行交叉，产生子代。 对子代的染色体进行变异。 重复2，3，4步骤，直至新种群的产生。 生成候选方案的初始群体。生成初始群体最简单的办法就是随机生成大量“个体”（个体基因）。 计算当前群体中各个个体的适应度。 选择一定数量适应度最高的个体作为下一代的父母。 将选出的父母进行配对。用父母进行重组产生出后代，伴有一定的随机突变概率，后代加入形成新一代群体。选出的父母不断产生后代，直到新的群体数量达到上限（即与初始群体数量一样）。新的群体成为当前群体。 转到第2步。 结束循环。 基因编码方式： 假设只有“0”和“1”两种碱基，用一条链条把它们有序地串连在一起，因为每一个单位都能表现出1 bit的信息量，所以一条足够长的染色体就能勾勒出一个个体的所有的特征。即==二进制编码法== 当个体特征较为复杂时，需要大量的编码才能精确地描述 ，相应的解码过程（相当于生物学中的DNA翻译过程，把基因型映射到表现型的过程。）将过于复杂，为改善遗传算法的计算复杂性、提高运算效率，提出了==浮点数编码==，还有一种编码方式是==符号编码==。 编码的目的是建立表现型到基因型的映射关系，而表现型一般就被理解为个体的特征。 适应度函数(fitness function)： 自然界生物竞争过程往往包括两个方面：生物间相互搏斗以及生物与客观环境的搏斗过程。需要指定一个衡量标准（比如选择袋鼠所在的海拔高度作为它们的适应度评分，适应度函数直接返回函数值。） 选择函数(selection): 越适应的个体就越可能繁衍后代（概率上来说）。包括轮盘赌(Roulette Wheel Selection)选择法，精英选择机制。 遗传变异 是子代区别于父代的根本原因。包括基因重组(recombination)(交叉(crossover))与基因突变(mutation)。 小结基因编码、基因适应度评估、基因选择、基因变异。 编码原则： 完备性（completeness)：问题空间的所有解都能表示为所设计的基因型。 健全性(soundness)：任何一个基因型都对应于一个可能的解。 非冗余性(non-redundancy)：问题空间和表达空间一一对应。 适应度函数重要性： 适应度函数的选取直接影响遗传算法的收敛速度以及能否找到最优解，一般而言，适应度函数是由目标函数变换而得的。 适应度函数设计不当可能会出现欺骗问题： 进化初期，个别超常个体控制选择过程； 进化末期，个体差异太小导致陷入局部极值。 选择作用：优胜劣汰，适者生存。 交叉作用：保证种群的稳定性，朝着最优解的方向进化。 进化作用：保证种群的多样性，避免交叉可能产生的局部收敛。 差分进化算法概念差分进化算法(Differential Evolution,DE)是一种基于群体差异的启发式随机搜索算法，通过群体内个体之间的相互合作与竞争产生的群体智能来指导优化搜索的方向。 相比于进化算法，DE保留了基于种群的全局搜索策略，采用实数编码、基于差分的简单变异操作和一对一的竞争生存策略，降低了遗传操作的复杂性。同时，DE特有的记忆能力使得其可以动态跟踪当前的搜索情况，以调整其搜索策略，具有较强的全局收敛能力和鲁棒性，且不需要借助问题的特征信息，适用于求解一些利用常规的数学规划方法无法求解的复杂环境中的优化问题。 该算法的基本思想是：从一个随机产生的初始种群开始，通过把种群中任意两个个体的向量差与第三个个体求和来产生新个体，该操作称为变异。然后变异产生的新个体与当代种群中相应的个体相比较，进行参数混合，生成试验个体，这一过程称为交叉。如果试验个体的适应度优于当前个体的适应度，则在下一代中就用试验个体取代旧个体，否则保存旧个体，这一过程称为选择。通过不断地进化，保留优良个体，淘汰劣质个体，引导搜索向最优解逼近。 算法流程一般演化算法的过程： 标准DE流程： 初始化种群 $x_{i,j}(0) = x_{ij}^L +rand_{ij}(0,1)(x_{ij}^U - x_{ij}^L),i=1,2,\dots,M;j=1,2,\dots,n$ 其中$x_{ij}^L$是$x$下限，$x_{ij}^U$是$x$上限，$rand$表示随机规则。 变异 从群体中随机选择3个染色体$x_{p1},x_{p2},x_{p3}$且$(i \ne p_1 \ne p_2 \ne p_3)$，则： $v_{ij}(t+1) = x_{p_{1j}}(t) +\eta(x_{p_{2j}}(t) -x_{p_{3j}}(t))$ 其中$v_{ij}(t+1)$表示变异后的个体，第$t+1$代。$\eta$表示随机化规则。 交叉 交叉操作是为了增加群体的多样性，具体操作如下： $u_{ij}(t+1) = \cases{v_{ij}(t+1) ,rand_{ij} \le CR 或j = rand(i) \ x_{ij}(t) ,rand_{ij} &gt; CR 且j \ne rand(i)}$ 其中$u_{ij}(t+1)$是试验个体，$v_{ij}(t+1)$是变异个体，$x_{ij}(t)$是目标个体（第t代）,$CR$是交叉概率，$rand_{ij}$是在[0,1]之间的随机小数，$CR$为交叉概率，$CR \in [0,1]$，$rand(i) \in [1,n]$之间的随机整数，这种交叉策略可确保$x_i(t+1)$至少有一分量由$x_i(t)$的相应分量贡献。 选择 为了确定$x_i(t)$是否成为下一代的成员，比较向量$u_i(t+1)$和目标向量$x_i(t)$的评价函数： $x_i(t+1) = \cases{u_i(t+1),f(u_i(t+1))\le f(x_i(t)) ,i=1,2,\dots,M\ x_i(t)，otherwise}$ DE主要涉及群体规模popsize、缩放因子F以及交叉概率cr三个参数的设定。 popsize：一般介于5×n与10×n之间，不能烧鱼，否则无法进行变异操作。 F：决定了偏差向量的放大倍数。过小可能导致局部早熟，一般在[0,2]之间选择。 cr：控制着一个实验个体参数来自于随机选择的变异个体而不是原来个体的概率，cr越大该发生交叉的概率越大。 DE应用实例利用差分进化算法求函数$Z=3\times cos(xy) +x+y$，其中$x \in [-4,4],y\in[-4,4]$。 step1: 计算目标函数值 1234567function z = obj_function(pop)%计算目标函数值% pop input 种群% z output 目标函数值z = 3 * cos(pop(:,1).*pop(:,2)) +pop(:,1) +pop(:,2);end step2: 初始化种群，目标函数有两个参数，生成每个个体有两个基因的种群。 123456789unction pop = init(popsize,chromlength,xl,xu)%生成初始种群% popsize input 种群规模% chromlength input 染色体长度% xl input x下限% xu input x上限% pop output 种群pop = rand(popsize,chromlength) * (xu-xl) +xl;end step3: 变异操作 123456789101112131415function mutationpop = mutation(pop,F)%变异函数% pop input 种群% F input 缩放因子% mutationpop output 变异后种群[popsize,chromlength] = size(pop);mutationpop = zeros(popsize,chromlength);for i = 1:popsize r = randperm(popsize); index = find(r ~= i); rn = r(index(1:3)); r0 = rn(1);r1 = rn(2);r2 = rn(3); mutationpop(i,:) = pop(r0,:) +F .* (pop(r1,:) - pop(r2,:));endend step4: 交叉操作 1234567891011121314function crossoverpop = crossover(pop,mpop,cr)% 交叉% pop input 种群% mpop input 变异后的种群% cr input 交叉概率% crossoverpop output 交叉后的种群[popsize,chromlength] = size(pop);crossoverpop = mpop;r = rand(popsize,chromlength);index = find(r &gt; cr);crossoverpop(index) = pop(index);jrand = randi(chromlength,1,popsize);crossoverpop(sub2ind(size(crossoverpop),[1:popsize],jrand))=mpop(sub2ind(size(mpop),[1:popsize],jrand));end 交叉操作后约束边界 12345678910function newpop = constrictboundary(pop,xl,xu)%约束边界（边界吸收）% pop input 种群% xl input 自变量最小值（包含）% xu input 自变量最大值（包含）%newpop output 约束边界后的种群newpop = pop ;newpop(newpop &lt; xl) = xl;newpop(newpop &gt; xu) = xu;end step5: 选择操作来寻找最优值 123456789function newpop = selection(pop,npop)%选择（最小值优化）% pop input 种群1（原始种群）% npop input 种群2（变异-交叉后的种群）% newpop output 选择后的种群newpop = pop;index = find(obj_function(npop) &lt;= obj_function(pop));newpop(index,:) = pop(index,:);end 实验结果 输出结果为： bestX = -1.772014 bestY = 1.945060 bestZ = -2.688422 进化规划(EP)概述一个全局最小化问题可以用一对$(S,f)$来形式化，其中$S\in R^n$为有限集。 $f: S \rightarrow R$是一个$n$维实函数。 问题为寻找一个点$x_{min} \in S$，使得$f(x_{min})$是$S$上的全局最小，即： $\forall x \in S:f(x_{min}) &lt; f(x)$ $f$不一定是连续的，但一定是有限的。 经典EPBack Schwefd提出的经典进化规划算法： 产生$\mu$个个体，并设$k=1$，每个个体都是一个实向量对$(x_i,\eta_i)$，$\forall i \in \{1,2,\dots,\mu \}$，其中$x_i$为目标变量，$\eta_i$为高斯变量的标准差。 对每一个个体$(x_i,\eta_i)$，$\forall i \in \{1,2,\dots,\mu \}$，度量其适应值，即$f(x_i)$。 每一个父代$(x_i,\eta_i),i=1,2,\dots,\mu$，由如下方法产生单个后代$(x_i’,\eta_i’)$： 变异 ：$\cases {x_i’(j) = x_i(j) + \eta_i(j) \times N_j(0,1) \ \eta_i’(j) = \eta_i(j) \exp (\tau’ N(0,1) + \tau N_j(0,1))}$。 其中$j$表示向量的第$j$个分量，$N(0,1)$表示一维高斯随机数；$\tau ‘= \left(\sqrt{2\sqrt{n}}\right)^{-1}$，$\tau = \left(\sqrt{2n}\right)^{-1}$ 计算每个子代$(x_i’,\eta_i’)$的适应值。 选择。对于每个个体，在所有的父代和子代中，随机选择$g$个对手比较适应度大小，如果该个体的适应度不差于对手，则得分一次。 在$(x_i,\eta_i)$和$(x_i’,\eta_i’)$总集合中选择$\mu$个得分最多的个体进入下一代。 若满足算法停止准则，则停止；否则，$k=k+1$，转入3。 方法改进1999年，Yao引入了柯西随机数，即： $f_t(x) = \frac{1}{\pi} ·\frac{t}{t^2 +x^2}$，$-\infty &lt;x &lt; \infty$。 变异：$x_i’(j) = x_i(j) + \eta_i(j) \times \delta_j$。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建FTP服务器教程]]></title>
    <url>%2Fblog%2F2019%2F05%2F25%2F%E6%90%AD%E5%BB%BAFTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[搭建FTP服务器搭建FTP服务器可以方便完成电脑数据的高效传输和访问。 主要思路：在台式机上搭建FTP服务器，然后使用笔记本访问。 前提条件：台式机和笔记本处于同一局域网，或者连接上同一路由器。 win-win台式机是win系统，FTP服务器搭建过程 出现笔记本无法访问数据文件，问题在于防火墙，需要设置台式机允许指定的端口 win-ubuntuLinux下搭建FTP服务器(ubuntu 16.04) 安装 vsftpd软件包，设置配置文件，需要在root权限下进行修改配置文件 重启vsftpd： sudo vsftpd restart 添加ftp用户：sudo useradd -d /var/myftp ftpuser 设置用户密码：sudo passwd ftpuser 设置为姓名拼音 将ftp用户名添加到user_list和chroot_list白名单中，即： vim /etc/vsftpd.user_list vim /etc/vsftpd.chroot_list 创建需要的文件夹，设置文件夹权限 mkdir home/myftp/upload 简单粗暴操作：chmod -R 777 /myftp linux访问出现530错误： vim /etc/pam.d/vsftpd，注释掉 #auth required pam_shells.so 局域网文件共享win10配置]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX基本操作]]></title>
    <url>%2Fblog%2F2019%2F05%2F14%2FLaTeX-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1、概述1.数学公式环境 ​ TEX有两种数学公式，一种是夹杂在行文段落中的公式，行内（inline）数学公式，或正文（in-text）数学公式；另一种是单独占据整行居中展示，称为显示（diaplayed）数学公式（或行间公式、列表公式）。 ​ 在TEX中，行内公式一般在前后单个$符号表示。 ​ $$可以插入行间公式。 ​ 使用equation环境对行间公式进行编号。 2.控制序列 ​ 键盘不能直接表示的符号或者起着特定作用的皆有命令，类似转义，叫做控制序列（control sequence）。比如求和符号Σ对应的命令为\sum 3.分组 ​ 使用{……}将内容包含起来视作整体，比如上下标很长的时候。 LATEX中还定义了命令格式与环境格式的方式输入行内公式，即使用命令 \或是math环境括起一个行内公式。 基本的显示公式是不带编号的，在TEX中可以用连续两个美元符号$$界定。 latex提供带自动编号的数学公式，可以用equation环境表示，公式后还可以带引用的标签。 \begin{equation} a+b=b+a\label{eq:commutative} \end{equation} $\text{被减数}-\text{减数}=\text{差}$ 已知变量有$a$,$b$,$S$,$T$ 2、数学结构2.1上标和下标TEX中上标用字符^表示，下标用特殊字符_表示。 $10^n$,$a_i$, 当上下标多于一个字符时，需要使用分组确定上下标范围，如： $A_{ij}=2^{i+j}$ 上下标同时使用，也可以嵌套使用。其中同时使用时，上下标先后次序不重要；嵌套使用时，外层一定要使用分组。如： $A_i^k = B^k_i$ $K_{n_i} = K_{2^i} = 2^{n_i}$ 这里数学公式中的空格（包括单个换行）不起实际作用，适当的空格可以将代码分隔的好看一些。 数学公式中的撇号’是一种特殊的上标，表示用符号\prime(即‘)作为上标。 $a=a’$,$b_0’ = b_0’’$, ${c’}^2 = (c’)^2$ 撇号可以与下标混用，也可以连续使用，但不能与上标直接混用。 latex默认的字体没有直接表示角度的符号，可以用符号\circ（即。）的上标表示 $A = 90^\circ$ 多数的数学算子的上下标，位置在正上方或正下方 $ \max_n f(n) = \sum_0^n A_i$ $\int_a^b f(x)$ 在上下标前面用\limits命令会使上下标在正上正下方，使用\nolimits使上下标在角上。 $\iiint\limits_D \mathrm{d}f = \max\nolimits_D g$ $\int_0^1 f(t) \mathrm{d} t = \iint_D g(x,y) \mathrm{d}x\mathrm{d}y$ 在左上、左下角加角标，在角标字符前面使用空的分组，给空分组加角标 ${}_m^n H$ 但是这种方法不标准，可能间距和对齐不合理，此时可用mathtools宏包的 \prescript{上标}{下标}{元素}来处理 $\prescript{n}{m}(H)_i^j &lt; L$ 给个别算子标记，不影响算子的上下限，可以用amsmath提供的 \sideset 命令 $\sideset{_a^b}{_c^d} \sum_{i=0}^n A_i = \sideset{}{‘} \prod_k f_i $ 还提供了\overset和 \underset命令，用来给任意符号的上下方添加标记 $\overset{}{X}$,$\underset{}{X}$, $\overset{*}{\underset{\dag}{X}}$ TEX的上下标互不影响，因此$A_m^n$而不是$A_m{}^n$。 $A_m{}^n$,还可以使用“幻影”phantom $A_m^{\phantom{m}n}$ $M\indices{^a_b}$ 输入化学方程式，使用 \ce命令输入化学式 $\ce{2H2 + O2 -&gt;[\text{燃烧}] 2H2O}$ 2.2上下画线与花括号使用\overline和\underline命令在公式上方和下方划横线 $$\overline{a+b} = \overline a + \overline b$$ $\underline a = (a_0, a_1, a_2, \dots)$ 此结构可以任意嵌套或与其他数学结构组合 $\overline{\underline{\underline a} +\overline{b}^2} - c^{\underline n}$ amsmath提供了在公式上下加箭头的命令 $\overleftarrow{a+b}$ $\overrightarrow{a+b}$ $\overleftrightarrow{a+b}$ $\underleftarrow{a+b}$ $\vec x = \overrightarrow{AB}$ 使用\overbrace和 \underbrace带上花括号 $\overbrace{a+b+c} = \underbrace{1+2+3}$ $\overbrace{a_0,a_1,\dots,a_n}^{\text{共 n+1项}} = {\underbrace{0,0,\dots,0}_{n},1}$。 $a+\overbrace{b+{\underbrace{c+d}_n}}^m+e+f$。 使用\rlap重叠，同时用\phantom占位，实现交错括号 $a+\rlap{\overbrace{\phantom{a+b+c}}} a +\underbrace{b+c+d}+f$ 上取整和下取整表示： $\lceil x \rceil$ $\lfloor x \rfloor$ 2.3分式使用\frac{分子}{分母} $\frac{1}{2} + \frac 1a = \frac{2+a}{2a}$。 $\frac{1}{\frac 12 (a+b)} + \frac 1a = \frac{2+a}{2a}$。 指定较大或较小的分式，\dfrac和\tfrac分别指定显示格式（display style）和正文格式（text style） $\tfrac 12 f(x) = \frac{1}{\tfrac 1a + \dfrac 1b + c}$。 连分式，使用\cfrac专用于输入连分式 $\cfrac{1}{1+\cfrac{2}{1+\cfrac{3}{1+x}}} = $。 $\sfrac 1a +b$, $1/(a+b)$ $1/a +b$, 使用\binom来输入二项式系数，用法于\frac类似 $(a+b)^2 = \binom 20 a^2 + \binom 21 ab + \binom 22 b^2$。 使用\genfrac{左括号}{右括号}{线宽}{大小}{分子}{分母}。线宽和大小为零表示默认指，大小可以是0，1，2，3，分别表\displaystyle \textstyle \scriptstyle \scriptscriptstyle $\genfrac {[} {]} {0pt} {} {n} {1} = (n-1)! ,\qquad n&gt;0$ 2.4根式使用\sqrt得到，同时可以带一个可选参数[]，表示开方次数 $\sqrt 4 = \sqrt[3] {8}$ 嵌套根式和其他数学结构 $\sqrt[n]{\frac{x^2 + \sqrt 2}{x+y}}$。 若开方次数不是简单的整数，或者被开方的内容过长（甚至超过一行），通常不使用根式的形式，而是该为等价的指数形式。 $(x^p + y^q)^{\frac{1}{1/p+1/q}}$。 开方次数的排版位置不合适，可以用\uproot和\leftroot命令进行调整，命令参数是整数，移动的单位是很小的一段距离。 $\sqrt[\uproot{10}\leftroot{-2}n]{\frac{x^2 + \sqrt 2}{x+y}}$。 根式的高度是随着内容改变的，但当几个根式并列时，有时需要调整统一的高度，可以使用\vphantom占位， $\sqrt{\frac 12} &lt; \sqrt{\vphantom{\frac12} 2 }$。 数学支架\mathstrut表示有一个圆括号高度和深度的支架，它常用来平衡不同高度和深度的字母。 $\sqrt b \sqrt y$,调整为： $\sqrt{\mathstrut b} \sqrt{\mathstrut y}$。 $\sqrt[4]{\frac xy}$ 2.5矩阵矩阵matrix是用\matrix和\pmatrix排版的， matrix环境： a b c d bmatrix环境 [a b] vmatrix环境： |a b| pmatrix环境： （a b） Bmatrix环境： {a b} Vmatrix环境： ║ a b║ 在矩阵环境中，不同的列用符号“&amp;”隔开，行用两条\分隔，矩阵的每列中的元素居中对齐。 $A = \pmatrix{ a_{11} &amp; a_{12} &amp; a_{13} \ 0 &amp; a_{22} &amp; a_{23} \ 0 &amp; 0 &amp; a_{33}}$. 矩阵中经常使用各种省略号，包括\dots, \vdots, \ddots等 $A= \pmatrix{a_{11} &amp; \dots &amp; a_{1n} \ &amp; \ddots &amp; \vdots \ 0 &amp; &amp; a_{nn}}_{n\times n}$。 通过两个$方式输出其他形似矩阵 $$A= \begin{bmatrix} a_{11} &amp; \dots &amp; a_{1n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{nn} \end{bmatrix}_{n\times n}$$ $$A= \begin{Vmatrix} a_{11} &amp; \dots &amp; a_{1n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{nn} \end{Vmatrix}_{n\times n}$$ 写增广矩阵，可以用array命令来处理$$A= \left[ \begin{array}{c|cc} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 0 &amp; 3 &amp; 4 \end{array} \right]$$ $$A= \left[ \begin{array}{cc|c} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 0 &amp; 3 &amp; 4 \end{array} \right]$$ $$A= \left[ \begin{array}{c:cc} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 0 &amp; 3 &amp; 4 \end{array} \right]$$ $$A= \left[ \begin{array}{c:c:c} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 0 &amp; 3 &amp; 4 \end{array} \right]$$ 分块矩阵$$A=\begin{pmatrix} \begin{matrix} 1&amp;0 \\0&amp;1 \end{matrix} &amp;{\large 0}\\ {\large 0}&amp; \begin{matrix} 1&amp;0\\0&amp;-1 \end{matrix} \end{pmatrix} $$在行内公式，有时候需要很小的矩阵，可以用smallmatrix环境得到，需要手工在外面添加 括号。如： 复数$z=(x,y)$也可用矩阵$$\left(\begin{smallmatrix}x&amp; -y \\y &amp;x\end{smallmatrix}\right)$$来表示 在上下标特别是求和式的上下限中，有时候需要使用好几行的内容，此时可以用amsmath提供的\substack命令排版，效果相当于只有一列的没有括号的矩阵。 $\sum_{\substack{0&lt;i&lt;n \\0&lt;j&lt;i}} A_{ij}$$$\max_n f(n) = \sum_{\substack{0&lt;i&lt;n\\0&lt;j&lt;i}} A_{ij}$$ $$\max_n f(n) = \sum_{\substack{0&lt;i&lt;n\\0&lt;j&lt;i\\k&lt;1000}} X(i,j,k)$$ matrix等矩阵环境默认至多只有10列，最大计算由计数器MaxMatrixCols控制，可以通过\setcounter等计数器命令临时或全局调整。（目测typora没有这限制）$$\begin{Bmatrix}0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1&amp;1\\0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1&amp;1\end{Bmatrix}$$ $$\left[\matrix{ &amp;1 &amp;2 &amp;3\\ 1 &amp;A &amp;B &amp;C\cr 2 &amp;D &amp;E &amp;F\cr} \right] $$ 注意一个问题：$$\left[\begin{matrix}V_A \\V_B \\V_C \\\end{matrix}\right] =\left[\begin{matrix}1 &amp; 0&amp; \dots &amp; L \\\\-cosψ &amp; sinψ &amp; \dots &amp; L \\\\\vdots&amp; \vdots&amp; \ddots &amp; \vdots \\-cosψ &amp; -sinψ &amp; \dots &amp; L\end{matrix}\right]\left[\begin{matrix}V_x \\\\V_y \\\\W \\\\\end{matrix}\right]$$或者采用： $\left[\begin{matrix}V_A \\V_B \\V_C \\\end{matrix}\right]$ 矩阵可以在hexo博客中很好的被渲染。 2.6 特殊字符各种希腊字母表： $\alpha,$$\theta,\tau,\beta,\vartheta,\pi,\upsilon,\gamma,\iota,\varpi,\phi,\delta,\kappa,\rho,$$\varphi,\epsilon,\lambda,\varrho,\chi,\varepsilon,\mu,\sigma,\psi,\zeta,\nu,\varsigma,$$\omega,\eta,\xi,\Gamma,\Lambda,\Sigma,\Psi,\Delta,\Xi,\Upsilon,\Omega,\Theta,\Pi,\Phi,o$. 省略号：$\dots$ $\cdots$ 求积$\prod$ 求和：$\sum_1^n$ 积分：$\int_1^n$ 极限： $lim_{x \to \infty}$ $\hat{\theta}$,加^号 $\widehat{x}$ $\Join$ $\overline{x}$，加横线 $\widetilde{x}$加波浪线 $\dot{x}$，加一个点 $\ddot{x}$，加两个点 latex公式编辑情况下对数字、字母加粗方式：\pmb $\pmb x=[x_1,x_2,……,x_n]^T$。 $\pmb 3!$ 实数集 $\mathbb{R}^n$ \pm 表示$\pm$ $\pmb x \in \cases{w_1\\w_2}$ $\Arrowvert$ $\arrowvert$ $\forall$ $\exists$ $\mathcal{XABC}$ $||x||_{\infty} = \max \limits_{i}|x_i|$ $\sim$ $\rm Laplace$ 2.7 数学公式输入latex大括号及多行公式，array中的lr表示居右，居左，居中。 $$\begin{equation}\left\{ \begin{array}{lr} a_1x+b_1y+c_1z=l \\ a_2x+ b_2y+c_2z=m\\ a_3x+b_3y+c_3z=n \end{array}\right.\end{equation}$$ $$\begin{equation}\left\{ \begin{array}{lr} x=\dfrac{3\pi}{2}(1+2t)\cos(\dfrac{3\pi}{2}(1+2t)) \\ y=s, &amp; 0\leq s\leq L,|t|\leq1.\\ z=\dfrac{3\pi}{2}(1+2t)\sin(\dfrac{3\pi}{2}(1+2t)) \end{array}\right.\end{equation}$$ $$\begin{equation}\begin{aligned}p(x|\mu,\sigma^2)&amp;=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\}\\&amp;=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\{-\frac{1}{2\sigma^2}x^2+\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2\}\\&amp;= h(x)g(\eta)exp(\eta^T u(x))\end{aligned}\end{equation}$$用两条“\\”表示换行，&amp;表示对齐点。 分段函数输入：$$\begin{equation}k(x-y)=\begin{cases}1: &amp;\mbox{$|x_i-y_j| \leq \frac{1}{2} $(i=1,…,D)}\\0: &amp;\mbox{otherwise}\end{cases}\end{equation}$$ 公式中的空格表示： 两个quad空格： $a\qquad b$,两个m宽度 $a\quad b$, 一个m宽度 $a b$, 1/3m宽度 $a\;b$, 2/7m宽度 $a\,b$, 1/6m宽度 $ab$, 没有空格 $a!b$, 缩进1/6m宽度 2.8块级公式编号直接使用块级代码$$x^n+y^n=z^n$$不会生成编号，而使用\tag{…}标签就可以生成对应的编号。$$x^n +y^n =z^n \tag{1.1}$$公式引用直接用$编号$即可。 如对于：$$a^2 + b^2 =c^2 \tag{1.2}$$由公式$(1.2)$即可得到结论。 2.9详细的数学公式输入见此处更多$\LaTeX$数学公式见此处 2.10下标如果是数学符号，直接在_前加上\limits命令放在正下方 $f(x,y) = \frac{mn}{\sum\limits_{(s,t) \in S_{x,y}} \frac{1}{g(X,y)}} $ 如果是普通符号，用\mathop先转成数学符号再用\limits 对于双美元符号之间的公式，默认是放在正下方，不需要加\limits。 3、画图表12345graph TDA[hard edge] --&gt;B(ROUND EDGE)B--&gt;C(DECISION)C--&gt;|ONE| D[RESULT ONE]C--&gt;|TWO| E[RESULT TWO] 12345graph LRA[HARD EDGE] --&gt;B(ROUND EDGE)B--&gt;C&#123;DECISION&#125;C--&gt;|ONE| D[RESULT ONE]C--&gt;|TWO| E[RESUALT TWO] 123456789st=&gt;start: startop=&gt;operation: your operationcond=&gt;condition: yes or no?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no) -&gt;op 123456789101112131415161718192021222324st=&gt;start: 开始op1=&gt;operation: 确定控制参数t=0op2=&gt;operation: 随机产生初始种群pop(0)op3=&gt;operation: 对初始种群进行评价op4=&gt;operation: 对于当前种群pop(t)，for(int i=0;i&lt;pop_size;i++)&#123; 对第i个体进行差分变异产生相应的变异个体， 然后和变异个体实施交叉操作， 生成试验个体&#125;op5=&gt;operation: 对试验个体组成的临时种群进行评价op6=&gt;operation: 通过选择操作确定下一代新种群pop(t+1)op7=&gt;operation: t=t+1cond=&gt;condition: 是否满足终止条件sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输出最优解e=&gt;end: 输出最优解st-&gt;op1-&gt;op2-&gt;op3-&gt;condcond(no,down)-&gt;op4-&gt;op5op5-&gt;op6-&gt;op7op7(right)-&gt;condcond(yes)-&gt;e 123ALICE-&gt;BOB: HELLO ,HOW ARE YOU?NOTE RIGHT OF BOB: BOB THANKSBOB-&gt;ALICE: I AM GOOD THANKS 123456789101112sequenceDiagramALICE-&gt;&gt;BOB: HELLO BOB,HOE ARE YOU?alt is sickBOB-&gt;&gt;ALICE: NOT SO GOOD :(else is wellBOB-&gt;&gt;ALICE: FEELING FRESH LIKE A DAISYendopt extra responseBOB-&gt;&gt;ALICE: THANKS FOR ASKINGend 123456789101112131415161718192021222324252627ganttdateformat 2019-03-24title adding gantt diagram functionality to memaid Completed task : des1, 3d Active task : des2, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d %%Create tests for renderer :2d %% Add to mermaid :1d section Documentation Describe gantt syntax :active, a1, after des1, 3d Add gantt diagram to demo page :after a1 , 20h Add another diagram to demo page :doc1, after a1 , 48h section Last section Describe gantt syntax :after doc1, 3d Add gantt diagram to demo page : 20h Add another diagram to demo page : 48h $$\begin{array}{c|lcr}n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\\\\hline1 &amp; 1.97 &amp; 5 &amp; 12 \\\\2 &amp; -11 &amp; 19 &amp; -80 \\\\3 &amp; 70 &amp; 209 &amp; 1+i \\\\\end{array}$$ 4、代办事宜todo列表 Cmd Markdown 开发 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 支持以 PDF 格式导出文稿 新增Todo列表功能 语法参考 改进 LaTex 功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 语法参考 七月旅行准备 准备邮轮上需要携带的物品 浏览日本免税店的物品 购买蓝宝石公主号七月一日的船票 使用带有 [ ] 或 [x] （未完成或已完成）项的列表语法撰写一个待办事宜列表，并且支持子列表嵌套以及混用Markdown语法。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的数学基础]]></title>
    <url>%2Fblog%2F2019%2F05%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[范数范数(norm)通常用来衡量向量大小。 形式上，$L^p$范数的定义如下： $||x||_p = \left( \sum \limits_{i} |x_i|^p\right)^{\frac{1}{p}}$，其中$p\in \mathbb{R},p \geq 1$。 范数（包括$L^p$范数）是将向量映射到非负值的函数。直观上，向量$x$的范数衡量从原点到点$x$的距离。 范数是满足下列性质的任意函数： $f(x) = 0 \implies x=0$ $f(x+y) \geq f(x)+f(y)$，三角不等式 $\forall \alpha \in \mathbb{R},f(\alpha x) =|\alpha|f(x)$ 当$p=2$时，$L^2 = ||x||_2 =\sqrt{ \sum\limits_{i} |x_i|^2}$范数称为欧几里得范数(Euclidean norm)。表示从原点出发到向量$x$确定的点的欧几里得距离。简化为$||x||$，可以通过点积$X^TX$来计算。 平方$L^2$范数$||x||^2_2$在数学和计算上比$L^2$范数方便，对$x$中的每个元素的导数只取决于相应的元素，而$L^2$范数对每个元素的导数和整个向量有关。 当$p=1$时，$L^1=||x||_1 = \sum\limits_{i}|x_i|$范数，当零和非零元素之间的差异非常重要时，会使用$L^1$范数。即每当$x$中的某个元素从0增加到$\epsilon $，对应的$L^1$范数也会增加$\epsilon$。 当$p \rightarrow \infty$时，$L^{\infty}$范数称为最大范数(max norm)。表示向量中具有最大幅值的元素的绝对值： $||x||_{\infty} = \max \limits_{i}|x_i|$ 在深度学习中，衡量矩阵的大小，最常见的做法是使用Frobenius范数： $||A||_F = \sqrt{\sum\limits_{i,j} A_{i,j}^2}$，类似于向量的$L^2$范数。 两个向量的点积(dot product)用范数表示为： $\pmb x^T \pmb y = ||\pmb x||_2||\pmb y||_2 cos \theta$ 奇异值分解回顾特征值和特征向量定义如下： $Ax = \lambda x$ 其中$A$是一个$n \times n$的实对称矩阵，$x$是一个$n$维向量，$\lambda$是矩阵$A$的一个特征值，$x$是矩阵$A$的特征值$\lambda$所对应的特征向量。 求出特征值和特征向量，这样可以将矩阵$A$进行特征分解。例如：矩阵$A$的$n$个特征值$\lambda_1 \le \lambda_2 \le \dots \le \lambda_n$，以及这$n$个特征值所对应的特征向量$\{w_1,w_2,\dots,w_n\}$，如果这$n$个特征向量线性无关，则矩阵$A$可以用下式的特征分解来表示： $A= W \sum W^{-1}$ 其中$W$是这$n$个特征向量张成的$n\times n$维矩阵，$\sum$表示这$n$个特征值为主对角线的$n\times n$维矩阵。 一般会将$W$的这$n$个特征向量进行标准化，即满足$||w_i||_2 =1$，即$w_i^Tw_i =1$，此时$W$的$n$个特征向量为标准正交基，满足$W^TW =I$，即$W^T = W^{-1}$，$W$称为正交阵。此时上式可写成： $A= W \sum W^T$ 注意：要进行特征分解，矩阵$A$必须是方阵，如果行列不同，需要对矩阵进行分解，需要使用SVD。 SVD定义SVD也是对矩阵进行分解一种方式，但不要求矩阵为方阵。假设矩阵$A$是一个$m\times n$的矩阵，定义矩阵$A$的SVD为： $A = U \sum V^T$ 其中$U$是一个$m \times m$的矩阵；$\sum$是一个$m \times n$的矩阵，除了主对角线上的元素以外全部为0，主对角线上的每个元素称为奇异值；$V$是一个$n\times n$的矩阵。 $U$和$V$都是正交阵(又称 酉矩阵)，即满足：$U^TU =I ,V^TV =I$。 所以主要问题是如何求出SVD分解后的$U,V,\sum$三个矩阵。 1、将矩阵$A$的转置和$A$做矩阵乘法，得到$n \times n$方阵$A^T A$。这样可以进行特征分解，得到的特征值和特征向量满足下式： $(A^T A) v_i = \lambda _i v_i$ 得到矩阵$A^TA$的$n$个特征值和对应的$n$个特征向量$v$，将$A^TA$所有特征向量张成一个$n \times n$的矩阵$V$，即为所求的矩阵$V$。一般将$V$中的每个特征向量称为$A$的右奇异向量。 2、将矩阵$A$和$A$的转置做矩阵乘法，得到$m \times m$的方阵$AA^T$。同样进行特征分解，得到： $(AA^T)u_i = \lambda_i u_i$ 得到矩阵$AA^T$的$m$个特征值和对应的$m$个特征向量$u$。将所有特征向量张成一个$m \times m$的矩阵$U$，即为所求的矩阵$U$。一般将矩阵$U$称为$A$的左奇异向量。 3、奇异值矩阵$\sum$，只需要求出对角线上的奇异值$\sigma$即可，其它位置都是0。 $A = U \Sigma V^T \implies AV = U\Sigma V^T V \implies AV = U\Sigma \ \implies Av_i = \sigma_iu_i \implies \sigma_i = Av_i /u_i$ 求出每个奇异值，进而得到奇异值矩阵$\Sigma$ 证明$A^TA$的特征向量组成就是SVD中的$V$矩阵： $A= U \Sigma V^T \implies A^T = V \Sigma^T U^T \implies A^TA = V\Sigma^T U^TU\Sigma V^T= V\Sigma^2 V^T$ 其中：$U^TU =I,\Sigma^T \Sigma = \Sigma^2$，此处的$\Sigma$表示奇异值矩阵 结合特征分解$A= W\Sigma W^T$，（注：此处的$\Sigma$表示特征值矩阵）可以看出： $A^TA$的特征向量组成就是SVD中的$V$矩阵，同理可证$AA^T$矩阵的特征向量组成是$U$矩阵。 特征值矩阵等于奇异值矩阵的平方，即：$\sigma_i = \sqrt{\lambda_i}$，可以通过求出特征值取平方根来求奇异值。 SVD计算举例假设矩阵定义为： $A= \pmatrix{0&amp;1\\1&amp;1\\1&amp;0}$ $A^TA =\pmatrix{0&amp;1&amp;1\\1&amp;1&amp;0} \pmatrix{0&amp;1\\1&amp;1\\1&amp;0} =\pmatrix{2&amp;1\\1&amp;2}$ $AA^T = \pmatrix{0&amp;1\\1&amp;1\\1&amp;0}\pmatrix{0&amp;1&amp;1\\1&amp;1&amp;0} = \pmatrix{1&amp;1&amp;0\\1&amp;2&amp;1\\0&amp;1&amp;1}$ 求出$A^TA$的特征值和特征向量为： $\pmatrix{2-\lambda &amp;1 \\1&amp; 2-\lambda} = (2-\lambda)^2-1=0 \implies \lambda_1=3,\lambda_2=1$ 当$\lambda_1=3$时，$\pmatrix{-1&amp;1\\1&amp;-1} \stackrel{\text{初等行变换}} {\sim} \pmatrix{-1 &amp;1\\0 &amp;0} \implies $特征向量为$\pmatrix{1 \\1}$,正交归一化得$v_1=\left[\begin{matrix}\frac{1}{\sqrt{2}} \frac{1}{\sqrt{2}}\end{matrix}\right]^T$,所以： $\lambda_1 =3;v_1 =\left[\begin{matrix}\frac{1}{\sqrt{2}} \frac{1}{\sqrt{2}}\end{matrix}\right]^T;\lambda_2=1;v_2=\left[\begin{matrix}-\frac{1}{\sqrt{2}} \frac{1}{\sqrt{2}}\end{matrix}\right]^T$ 同理可得$AA^T$得特征值和特征向量为： $$\lambda_1=3;\begin{equation}u_1 =\left[\begin{matrix}1/\sqrt{6} \\\\2/\sqrt{6} \\\\1/\sqrt{6}\end{matrix}\right]\end{equation}$$ $$\lambda_2=1;u_2 = \left[\begin{matrix}1/\sqrt{2} \\\\0 \\\-1/\sqrt{2} \end{matrix}\right]$$ $$\lambda_3=0;u_3 = \left[\begin{matrix}1/\sqrt{3} \\\-1/\sqrt{3} \\\\1/\sqrt{3} \end{matrix}\right]$$ 利用$Av_i = \sigma_iu_i,i=1,2$求得奇异值为： $$\left[\begin{matrix}0&amp;1\\1&amp;1\\1&amp;0\end{matrix}\right] \left[\begin{matrix}1/\sqrt{2}\\1/\sqrt{2}\end{matrix}\right] = \sigma_1 \left[\begin{matrix}1/\sqrt{6} \\\ 2/\sqrt{6} \\\ 1/\sqrt{6}\end {matrix}\right] \implies \sigma_1 =\sqrt{3}$$ $$\left[\begin{matrix}0&amp;1\\1&amp;1\\1&amp;0\end{matrix}\right] \left[\begin{matrix}-1/\sqrt{2}\\1/\sqrt{2}\end{matrix}\right]= \sigma_2 \left[\begin{matrix}1/\sqrt{2} \\\\0\\\ -1/\sqrt{2}\end{matrix}\right]\implies \sigma_2 =1$$ 也可以直接通过$\sigma_i= \sqrt{\lambda_i}$得到奇异值为$\sqrt{3}$和1。 最终得到矩阵$A$得奇异值分解为： $$A= U \Sigma V^T = \left[\begin{matrix}1/\sqrt{6} &amp; 1/\sqrt{2} &amp; 1/\sqrt{3} \\\ 2/\sqrt{6} &amp;0 &amp;-1/\sqrt{3} \\\ 1/\sqrt{6} &amp;-1/\sqrt{2} &amp;1/\sqrt{3}\end{matrix}\right] \left[\begin{matrix}\sqrt{3}&amp;0\\\ 0&amp;1\\\\0&amp;0\end{matrix}\right] \left[\begin{matrix}1/\sqrt{2} &amp; 1/\sqrt{2} \\\ -1/\sqrt{2} &amp;1/\sqrt{2}\end{matrix}\right]$$ SVD的一些性质对于奇异值，与特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小的顺序排列，而且奇异值的减少特别快，在很多的情况下，前10%甚至1%的奇异值的和占全部的奇异值之和的99&amp;以上的比例。 可以用最大的$k$个奇异值和对应的左右奇异向量来近似描述矩阵，即： $A_{m\times n} = U_{m\times m}\Sigma_{m\times m} V^T_{n\times n} \approx U_{m\times k} \Sigma_{k\times k} V^T_{k\times n}$ 其中$k$要比$n$小很多，也就是一个大的矩阵$A$可以用三个小矩阵$U_{m\times k},\Sigma_{k\times k},V^T_{k\times n}$表示。 这个重要的性质，SVD可以用于PCA(主成分分析)降维，来做数据压缩和去噪，也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时可以用于NLP中的算法，比如潜在语义索引(LSI)。 SVD用于减噪(noise reduction)SVD用于数据分析(data analysis)SVD用于PCA要用PCA降维，需要找到样本协方差矩阵$X^TX$的最大的$d$个特征向量，然后用这最大的$d$个特征向量张成的矩阵来做低维投影降维。 这个过程中需要先求出协方差矩阵$X^TX$，当样本数多，样本特征数也多的时候，这个计算量很大。 SVD也可以得到协方差矩阵$X^TX$最大的$d$个特征向量张成的矩阵，但是SVD有个好吃，可以不先求出协方差矩阵$X^TX$，也能求出右奇异矩阵$V$，即PCA算法可以不用做特征分解，而是做SVD来完成，这个方法在样本量很大的时候很有效。 假设我们的样本是$m\times n$的矩阵$X$，如果我们通过SVD找到了矩阵$XX^T$最大的$d$个特征向量张成的$m×d$维矩阵$U$，则我们如果进行如下处理： $X′_{d×n}=U^T_{d×m}X_{m×n}$ 可以得到一个$d×n$的矩阵$X’$,这个矩阵和我们原来的$m×n$维样本矩阵$X$相比，行数从$m$减到了$d$，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。 给定一个对称矩阵$M$，可以找到一些相互正交$v_i$，满足$Mv_i$就是沿着$v_i$方向的拉伸变换，满足：$$Mv_i = \lambda_i v_i$$其中$\lambda_i$是拉伸尺度(scalar)，从几何上看，$M$对向量$v_i$进行了拉伸，映射变换。$v_i$称作矩阵$M$的特征向量(eigenvector)，$\lambda_i$称为矩阵$M$的特征值(eigenvalue)。 对称矩阵的特征向量是相互正交的。 ==奇异矩阵与非奇异矩阵== 首先是方阵（前提条件） 若方阵A的行列式等于0，即$|A|=0$，A为奇异矩阵，不等于0，为非奇异矩阵。 一些性质： A(n×n)为奇异矩阵（singular matrix），则A的秩rank(A) &lt; n；如果为非奇异矩阵(nonsingular matrix)，则A的秩rank(A) = n，满秩。 一个方阵非奇异当且仅当它的行列式值不等于零 一个方阵非奇异当且仅当它代表的线性变换是自同构的 一个矩阵半正定当且仅当它的每一个特征值大于等于零 一个矩阵正定当且仅当它的每个特征值都大于零 一个矩阵非奇异当且仅当它的秩为n（满秩） $|A| \ne 0$可知矩阵A可逆，即：可逆矩阵等价于非奇异矩阵。 原文：奇异值分解(singular value decomposition) 译文：SVD 矩阵、向量求导法则1、矩阵$\pmb Y= F(x)$对标量$x$求导 相当于对每个元素求导： $$\begin{equation}\frac{d\pmb Y}{dx} =\left[\begin{matrix}\frac{dF_{11}(x)}{dx} &amp; \frac{dF_{12}(x)}{dx} &amp;\dots &amp; \frac{dF_{1n}(x)}{dx} \\\ \vdots &amp; \vdots &amp; \dots &amp; \vdots \\\ \frac{dF_{m1}(x)}{dx} &amp; \frac{dF_{m2}(x)}{dx} &amp; \dots &amp; \frac{dF_{mn}(x)}{dx}\end{matrix}\right]\end{equation}$$ 2、标量$y$对列向量$\pmb x$求导 对$m\times 1$向量求导后还是$m\times 1$向量，注意是偏导;如果是对行向量$1\times m$求导，则结果为$1 \times m$的行向量。 $$y =f(\pmb x) \rightarrow \frac{dy}{d\pmb x} = \left[\begin{matrix}\frac{\partial f}{\partial x_1} \\\ \frac{\partial f}{\partial x_2}\\\ \vdots \\\ \frac{\partial f}{\partial x_m}\end{matrix}\right]$$ 此时的向量称为梯度向量。$\frac{\partial y}{\partial \pmb x}$为标量$y$在空间$\mathbb{R}^m$的梯度，该空间以$\pmb x$为基。 3、行向量$\pmb y^T$对列向量$ \pmb x$求导 $1\times n$向量对$m\times 1$向量求导后是$m\times n$矩阵；将$\pmb y$的每一列对$ \pmb x$求偏导，将各列构成一个矩阵。 $$\begin{equation}\frac{d\pmb y^T}{d\pmb x} =\left[\begin{matrix}\frac{\partial f_1(x)}{\partial x_1} &amp;\frac{\partial f_2(x)}{\partial x_1} &amp; \dots &amp; \frac{\partial f_n(x)}{\partial x_1} \\\ \frac{\partial f_1(x)}{\partial x_2} &amp; \frac{\partial f_2(x)}{\partial x_2} &amp; \dots &amp;\frac{\partial f_n(x)}{\partial x_2} \\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\ \frac{\partial f_1(x)}{\partial x_m}&amp; \frac{\partial f_2(x)}{\partial x_m} &amp; \dots &amp; \frac{\partial f_n(x)}{\partial x_m}\end{matrix}\right]\end{equation}$$ 重要结论： $\frac{d\pmb x^T}{d\pmb x} =\pmb I$ $\frac{d(\pmb A\pmb x)^T}{d\pmb x} = \pmb A^T$，其中$\pmb A$是与$\pmb x$无关的矩阵。 4、列向量$\pmb y$对行向量$\pmb x^T$求导 转化为行向量$\pmb y^T$对列向量$ \pmb x$的导数，然后转置。 注意$n\times 1$向量对$1\times m$向量求导结果为$n\times m$矩阵。 $$\begin{equation}\frac{d\pmb y}{d\pmb x^T} =\left(\frac{d\pmb y^T}{d\pmb x}\right)^T =\left[\begin{matrix}\frac{\partial f_1(x)}{\partial x_1} &amp;\frac{\partial f_2(x)}{\partial x_1} &amp; \dots &amp; \frac{\partial f_n(x)}{\partial x_1} \\\ \frac{\partial f_1(x)}{\partial x_2} &amp; \frac{\partial f_2(x)}{\partial x_2} &amp; \dots &amp;\frac{\partial f_n(x)}{\partial x_2} \\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\ \frac{\partial f_1(x)}{\partial x_m}&amp; \frac{\partial f_2(x)}{\partial x_m} &amp; \dots &amp; \frac{\partial f_n(x)}{\partial x_m}\end{matrix}\right]^T\end{equation}$$ 重要结论： $\frac{d\pmb x}{d\pmb x^T} =\pmb I$ $\frac{d(\pmb A \pmb x)}{d\pmb x^T} = \pmb A$ 5、向量函数（函数组成的向量）$\pmb y =\left[\begin{matrix}y_1 \\\\y_2 \\\ \vdots \\\ y_n\end{matrix}\right]$关于向量$\pmb x =\left[\begin{matrix} x_1 \\\\x_2 \\ \vdots \\ x_n\end{matrix}\right]$的导数记为：$$\begin{equation}\frac{\partial \pmb y}{\partial \pmb x} =\left[\begin{matrix}\frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} &amp; \dots &amp; \frac{\partial y_1}{\partial x_n} \\\ \frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} &amp; \dots &amp; \frac{\partial y_2}{\partial x_n} \\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\ \frac{\partial y_n}{\partial x_1 } &amp; \frac{\partial y_n}{\partial x_2} &amp; \dots &amp; \frac{\partial y_n}{\partial x_n}\end{matrix}\right]\end{equation}$$ 称为Jacobian矩阵（雅可比矩阵）。 重要结论： $\frac{\partial \pmb y}{\partial \pmb x} = \left(\frac{\partial y} {\partial \pmb x}\right)^T$ 这里面$\nabla f = \frac{\partial f}{\partial \pmb x} $，记作grad f，是一个m维向量。Hessian矩阵记为$\nabla^2 f$，其中$(\nabla^2 f)_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$，是一个$m\times m$矩阵，相当于$\pmb x$到$\nabla f$的雅可比矩阵。$\nabla ^2 f = \nabla(\nabla f)$。 6、向量积对列向量$\pmb x$求导运算法则 注意与标量求导不同。 $\frac{d(\pmb u^T \pmb v)}{d\pmb x} = \frac{d(\pmb u^T)}{d\pmb x}· \pmb v + \frac{d(\pmb v^T)}{d\pmb x}·\pmb u$ 重要结论： $\frac{d(\pmb x^T \pmb x)}{d\pmb x} = \frac{d(\pmb x^T)}{d\pmb x}·\pmb x + \frac{d(\pmb x^T)}{d\pmb x}· \pmb x= 2 \pmb x$ $\frac{d(\pmb x^T \pmb A \pmb x)}{d\pmb x} = \frac{d(\pmb x^T)}{d\pmb x}·\pmb A \pmb x + \frac{d(\pmb x^T \pmb A^T)}{d\pmb x}· \pmb x= (\pmb A+\pmb A^T)\pmb x$ 7、矩阵$\pmb Y$对列向量$\pmb x $求导 将$\pmb Y= \left[\begin{matrix}y_{11} &amp; y_{12} &amp; \dots &amp; y_{1n} \\\ \vdots &amp; \vdots &amp; \dots &amp; \vdots \\\ y_{n1} &amp; y_{n2} &amp; \dots &amp; y_{nn}\end{matrix}\right] $对$ \pmb x = \left[\begin{matrix}x_1 \\\\x_2 \\\ \vdots \\\ x_n\end{matrix}\right]$的每一个分量求偏导，构成一个超向量。注意该向量的每一个元素都是一个矩阵。 $$\begin{equation}\pmb Y = \pmb F( \pmb x) \rightarrow \frac{d\pmb Y}{d \pmb x} =\left[\begin{matrix}\frac{\partial \pmb F}{\partial x_1} \\\ \frac{\partial \pmb F}{\partial x_2} \\\ \vdots \\\ \frac{\partial \pmb F}{\partial x_n}\end{matrix}\right]=\left[\begin{matrix}\frac{\partial y_{11}}{\partial x_1} &amp; \frac{\partial y_{12}}{\partial x_2} &amp; \dots &amp; \frac{\partial y_{1n}}{\partial x_n} \\\ \frac{\partial y_{21}}{\partial x_1} &amp; \frac{\partial y_{22}}{\partial x_2} &amp; \dots &amp; \frac{\partial y_{2n}}{\partial x_n} \\\ \vdots &amp; \vdots &amp; \dots &amp; \vdots \\\ \frac{\partial y_{n1}}{\partial x_1 } &amp; \frac{\partial y_{n2}}{\partial x_2} &amp; \dots &amp; \frac{\partial y_{nn}}{\partial x_n}\end{matrix}\right]\end{equation}$$ 若向量$\pmb y = \left[\begin{matrix}y_1 \\\\y_2\\\ \vdots \\\ y_n\end{matrix}\right]$，对于标量$x$的求导，即$\pmb y$的每一个元素分别对$x$求导，表示为： $$\begin{equation}\frac{\partial \pmb y}{\partial x} =\left[\begin{matrix}\frac{\partial y_1}{\partial x} \\\\\frac{\partial y_2}{\partial x} \\\ \vdots \\\\\frac{\partial y_n}{\partial x}\end{matrix}\right]\end{equation}$$ 8、标量$y$对矩阵$ \pmb X$的导数 类似标量$y$对列向量$ \pmb x$的导数，把$y$对每个$ \pmb X $的元素求偏导，不用转置。称为梯度矩阵。 $$\begin{equation}\frac{dy}{d\pmb X} =\left[\begin{matrix}\frac{\partial f}{\partial x_{11}} &amp;\frac{\partial f}{\partial x_{12}}&amp; \dots &amp; \frac{\partial f}{\partial x_{1n}} \\\ \frac{\partial f}{\partial x_{21}} &amp; \frac{\partial f}{\partial x_{22}} &amp;\dots &amp; \frac{\partial f}{\partial x_{2n}} \\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\ \frac{\partial f}{\partial x_{m1}} &amp; \frac{\partial f}{\partial x_{m2}} &amp;\dots &amp; \frac{\partial f}{\partial x_{mn}}\end{matrix}\right]\end{equation}$$ 重要结论： $\frac{d\pmb u^T \pmb X \pmb v}{d \pmb X} =\pmb u· \pmb v^T$ $\frac{d\pmb u^T \pmb X^T \pmb X \pmb u}{d\pmb X} = 2\pmb X \pmb u · \pmb u^T$ $\frac{d[(\pmb X \pmb u -\pmb v)^T(\pmb X \pmb u- \pmb v)]}{d\pmb X} = 2(\pmb X \pmb u - \pmb v)\pmb u^T$ 这部分证明见矩阵迹求导部分 9、矩阵$\pmb Y$对矩阵$\pmb X$求导 设$\pmb Y = \left[\begin{matrix}y_{11} &amp;\dots &amp; y_{1n} \\\ \vdots &amp; \ddots &amp; \vdots \\\ y_{m1} &amp; \dots &amp; y_{mn} \end{matrix}\right]= \left[\begin{matrix}\pmb y_1^T \\\ \pmb y_2^T \\\ \vdots \\\ \pmb y_m^T\end{matrix}\right]$是$m\times n$的矩阵 $\pmb X = \left[\begin{matrix}x_{11} &amp; \dots &amp; x_{1q} \\\ \vdots &amp; \ddots &amp; \vdots \\\ x_{p1} &amp; \dots &amp; x_{pq}\end{matrix}\right]=\left[\begin{matrix}\pmb x_1 &amp; \dots &amp; \pmb x_q\end{matrix}\right]$是$p\times q$的矩阵，有： $$\begin{equation}\frac{\partial \pmb Y}{\partial \pmb X} =\left[\begin{matrix}\frac{\partial \pmb Y}{\partial \pmb x_1} &amp; \dots &amp; \frac{\partial \pmb Y}{\partial \pmb x_q}\end{matrix}\right] =\left[\begin{matrix}\frac{\partial \pmb y_1^T}{\partial \pmb X} \\\ \vdots \\\ \frac{\partial \pmb y_m^T}{\partial \pmb X}\end{matrix}\right]=\left[\begin{matrix}\frac{\partial \pmb y_1^T}{\partial \pmb x_1} &amp; \dots &amp;\frac{\partial \pmb y_1^T}{\partial \pmb x_q} \\\\\vdots &amp; \ddots &amp; \vdots \\\ \frac{\partial \pmb y_m^T}{\partial \pmb x_1} &amp; \dots &amp; \frac{\partial \pmb y_m^T}{\partial \pmb x_q}\end{matrix}\right]\end{equation}$$ 10、向量求导的链式法则 雅可比矩阵的传递性：若多个向量的依赖关系为$\pmb u \rightarrow \pmb v \rightarrow \pmb w$，则：$\frac{\partial \pmb w}{\partial \pmb u} = \frac{\partial \pmb w}{\partial \pmb v}\frac{\partial \pmb v}{\partial \pmb u}$。 证明：只需要逐个元素求导，即$\frac{\partial w_i}{\partial u_j} = \sum \limits_k \frac{\partial w_i}{\partial v_k}\frac{\partial v_k}{\partial u_j}$。 如果中间变量都是向量，最后结果是一个实数，如依赖关系$\pmb x \rightarrow \pmb v \rightarrow \pmb u \rightarrow f$，因为：$\frac{\partial \pmb f}{\partial \pmb x} =\frac{\partial \pmb f}{\partial \pmb u} \frac{\partial \pmb u}{\partial \pmb v}\frac{\partial \pmb v}{\partial \pmb x}$。 根据$\pmb f$退化是雅可比矩阵和函数导数的关系，有：$\frac{\partial \pmb f}{\partial \pmb x} = \frac{\partial f} {\partial \pmb x^T}$，$\frac{\partial \pmb f}{\partial \pmb u} = \frac{\partial f} {\partial \pmb u^T}$； 可得如下链式法则：$\frac{\partial f}{\partial \pmb x^T} =\frac{\partial f}{\partial \pmb u^T} \frac{\partial \pmb u}{\partial \pmb v}\frac{\partial \pmb v}{\partial \pmb x}$。【此处是将导数作为行向量，即以$\frac{\partial f} {\partial \pmb x^T}$的形式。如果是列向量，需要将公式两边同时转置即可。】 如：$y = f(\pmb u) ,\pmb u = g(\pmb x)$,则：$\frac{\partial f} {\partial \pmb x} =\left( \frac{\partial \pmb u}{\partial \pmb x}\right)^T\frac{\partial f}{\partial \pmb u}$，或写作：$\nabla _{\pmb x} f = (\nabla_{\pmb x}\pmb u)^T \nabla _{\pmb u} f$。 11、向量数乘求导公式 $\nabla_{\pmb x}[\alpha(\pmb x)\pmb f(\pmb x)] = \pmb f(\pmb x) \nabla_{\pmb x^T}\alpha(\pmb x) + \alpha(\pmb x) \nabla_{\pmb x} \pmb f(\pmb x)$ 推导：$\frac{\partial \alpha f_i}{\partial x_j} = f_i \frac{\partial \alpha}{\partial x_j} + \alpha \frac{\partial f_i}{\partial x_j}$ 向量数乘的结果还是个向量，相当于是向量对向量的求导，结果是一个雅可比矩阵，形状是$\pmb f$的维度×$\pmb x$的维度。 12、矩阵迹求导 涉及到的公式都是对矩阵$X$求导。 迹的性质： 线性性质：$tr(\sum_i c_i A_i) = \sum_i c_i tr(A_i)$ 转置不变性：$tr(A) = tr(A^T)$ 轮换不变性：$tr(A_1A_2…A_3) = tr(A_2A_3…A_nA_1) = \dots = tr(A_n A_1 \dots A_{n-2}A_{n-1})$ 特别地，$tr(AB) = tr(BA)$。 【注】：轮换不变性不等于交换性， 例如$tr(ABC) = tr(BCA) =tr(CAB)$，但是一般情况下，$tr(ABC) \neq tr(ACB)$。 基本公式： $\nabla tr(A^T X) = \nabla tr(AX^T) =A$。 推导：逐个元素求导验证：$\frac{\partial tr(A^TX)}{\partial x_{ij}} = \frac{\partial \sum_{i,j} (a_{ij}x_{ij})}{\partial x_{ij}} = a_{ij}$。 同理可得：$\nabla tr(A X) = \nabla tr(XA) =A^T$ $tr(a) = a$（a为实数） $tr(ABC) = tr(CAB)= tr(BCA)$ $tr(AB) =tr(BA)$ $\frac{\partial tr(AB)}{\partial A} = B^T$ $\frac{\partial tr(ABA^TC)}{\partial A} = CAB + C^T AB^T$【重要公式】 即：$\nabla tr(XAX^TB) =B^TXA^T + BXA$ 推导： $\nabla tr(XAX^TB) = \nabla tr(XAX_c^T B) + \nabla tr(X_c AX^TB)$ 【该式表示分别对X和$X^T$求导】 $=(AX_c^TB)^T + \nabla tr(BX_cAX^T) \ = B^T X_c A^T + BX_cA \ = B^TXA^T + BXA$ 【$X_c$表示将$X_c$此次出现视作常数】 $\nabla (a^TXb) = ab^T$ 推导：$LHS = \nabla tr(a^TXb) = \nabla tr(Xba^T) =ab^T = RHS$ 【注】：将实数看成是1×1的矩阵的迹是常用技巧 $\nabla (a^TX^TXa) = 2Xaa^T$ 推导：使用核心公式。 $$\begin{equation}\begin{aligned}LHS &amp;= \nabla tr (a^Ta^TX_c a) + \nabla tr(a^TX_c^T X a) \\\ &amp;= \nabla tr(X_c aa^T X^T) + \nabla tr(aa^TX_c^TX) \\\ &amp;= X_c aa^T + (aa^TX_c^T)^T \\\ &amp;=X_caa^T + X_c aa^T \\\\&amp;= 2Xaa^T \\\\&amp;= RHS\end{aligned}\end{equation}$$ $\nabla [(Xa-b)^T(Xa-b)] = 2(Xa-b)a^T$ 推导：左边括号展开$$\begin{equation}\begin{aligned}LHS = \nabla tr[a^TX^TX a - a^TX^Tb -b^TX a +b^Tb] \\\ \nabla tr(a^TX^TXa) = 2Xaa^T \\\ \nabla tr(a^TX^Tb) = \nabla tr(ba^TX^T) =ba^T \\\\\nabla tr(b^TXa) = \nabla tr(Xab^T) = (ab^T)^T = ba^T \\\ \nabla tr(b^Tb) =0 \\\ LHS =2Xaa^T -2ba^T = 2(Xa-b)a^T = RHS\end{aligned}\end{equation}$$ $\nabla ||XA^T -B||_F^2 =2(XA^T -B)A$ 注意：$||A||_F^2 = A^TA$，特别地，$\nabla ||X||_F^2 = \nabla(X^TX) = 2X$ $tr(BA^T) = A^TB$ 推导：$tr(BA^T) = \sum_i B_iA_i = A^TB$ 12、行列式求导 $\nabla _X|X| = |X|(X^{-1})^T$。实数对矩阵求导，结果是和矩阵$X$同型的矩阵。 参考资料机器学习中的矩阵和向量求导 线性代数同济版笔记 高数—线代-概率论基础 机器学习中的数学-MIT大牛综述]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客搭建过程]]></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fhexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Hexo博客搭建的一些教程Hexo+Next添加菜单分类界面hexo博客添加分类界面教程 Hexo的命令发布文章：hexo new post &quot;article title&quot;生成：hexo g部署：hexo d在部署前先生成：hexo d -g删除文章：直接删除就可以了，无需其他的操作 注意在部署前安装：npm install hexo-deployer-git --save Hexo博客内容部署到Github pages123456789 C:\..\blog\hexo $ hexo clean INFO Deleted database. INFO Deleted public folder. C:\..\blog\hexo $ hexo g -d INFO Start processing Hexo博客域名更换xxx.github.io的免费域名较长，更换到自己的私人域名，购买解析私人域名之后，在Github pages的custom domain添加解析的域名并保存。 注意：只有将repo名称设置为xxx.github.io之后，更换私人域名，其他的新建的repo会自动更换到私人域名。 为防止每次部署之后，域名变化，可以在博客根目录的hexo文件下\blog\hexo\source\CNAME，添加个人域名。 备份本地博客的文件由于 Github 上保存的只是生成的网页静态文件，因此需要新建一个分支保存本地原始文件，方便在不同的电脑上写博客。需要解决备份问题。 机制是这样的，hexo d上传部署到GitHub的是hexo编译后的文件，用来生成网页的，不包含源文件。即上传的是在本地目录里自动生成的.deploy_git里面。其它文件，包括写在source里面的，和配置文件，主题文件，都没有上传到GitHub上。 所以可以利用git的分支管理，将源文件上传到GitHub的同一仓库的另一个分支即可。 创建两个分支：master 与 source，在博客目录下： 123456789$ cd blog$ git init #在当前目录新建一个git代码库 $ git checkout -b hexo #新建一个hexo分支，并切换到该分支$ git add . #添加blog目录所有文件$ touch README #新建文件，push不能是空仓库$ git add README$ git commit -m "blog source hexo" #添加更新说明$ git remote add origin https://github.com/yourname/yourname.github.io.git$ git push origin hexo //将新的分支发布到GitHub上 这样GitHub项目的库里会多出一个source分支，是用于多终端同步的关键部分。 出现以下错误：error：failed to push some refs to 解决方法：git pull --rebase origin master。把远程库中的更新合并到本地库中。 在另一个终端中更新博客，只需要将GitHub上source分支clone下来，进行初次相关配置。 12345678910111213141516171819202122232425262728# 将Github中hexo分支clone到本地$ git clone -b hexo https://github.com/yourname/yourname.github.io.git# 切换到刚刚clone的文件夹内$ cd yourname.github.io# cheackout 远程代码到本地hexo分支$ git checkout -b hexo origin/hexo# 注意，这里一定要切换到刚刚clone的文件夹内执行，安装必要的所需组件，不用再init$ npm install# 新建一个.md文件，并编辑完成自己的博客内容$ hexo new post "new blog name"$ hexo d -g# 经测试每次只要更新sorcerer中的文件到Github中即可，因为只是新建了一篇新博客$ git add source$ touch README$ git add README$ git commit -m "update blog"$ git remote add origin https://... .git# 更新分支$ git push -u origin +hexo# push更新完分支之后将自己写的博客对接到自己搭的博客网站上，同时同步了Github中的master#出现更新被拒绝，当前分支的最新提交落后于其对应的远程分支$ git fetch origin #获取远程更新$ git merge origin/master # 把更新内容合并到本地分支$ git push -u origin +hexo Hexo博客添加RSS订阅功能RSS(really simple syndication)简易的信息聚合，在互联网上被广泛采用的内容包装和投递协议。是一种描述同步内容的格式，使用xml格式。当网站内容更新时，可以通过订阅RSS源在RSS阅读器上获取更新的信息。 在hexojs用户下的仓库中有两个RSS功能的npm包： 1hexo-migrator-rss 1hexo-generator-feed 其中第二个是生成RSS文件的包，第一个是从RSS迁移所有的文章到source/_posts文件夹中。 首先安装： 1npm install hexo-generator-feed 然后在_config.yml文件中配置该插件 1234feed: type: atom #RSS的类型 path: atom.xml #文件路径，默认是atom.xml limit: 20 #展示文章的数量，使用0或false代表展示全部 还有一种方法是在next主题的_config.yml文件中有个rss的配置，直接设置为true即可。 配置好后运行hexo g就可以在博客目录的public文件夹下发现atom.xml文件。 然后运行hexo d部署到静态界面上，在个人主页处可以看到RSS的订阅图标。点击这个图表可以出现RSS订阅的地址，可以添加到RSS阅读器上查看博客的最新文章。 RSS使用首先需要一个RSS阅读器，推荐inoreader，邮箱注册，登陆。 添加订阅源，具体可见必须的RSS订阅源有哪些？ hexo博文置顶1、安装插件npm install hexo-generator-index-pin-top --save 在需要置顶的文章的Front-matter中加上top。： 123456---title: 2018date: 2018-10-25 16:10:03top: 10 //或者 top: true--- 2、设置置顶标志 打开：/themes/next/layout/_macro/post.swig，定位到&lt;div class=&quot;post-meta&quot;&gt;标签下，插入以下代码： 12345&#123;% if post.top %&#125; &lt;i class="fa fa-thumb-tack"&gt;&lt;/i&gt; &lt;font color=7D26CD&gt;置顶&lt;/font&gt; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125; hexo博客插入图片1、图床 储存图片的服务器，支持创建图片的对外链接地址便于引用。使用时只要引入图片的绝对地址就可以。 推荐比较多的国内图床： 七牛云储存新注册用户可免费使用 10G 存储空间。 极简图床其实也是依赖七牛云储存账号的。 2、本地图片 在 _config.yml 文件中设置 post_asset_folder: true，开启资源文件夹功能，该功能支持用户通过相对路径标签引用资源。 执行 hexo new [layout] &lt;title&gt; 创建一篇新的文章，会发现 source/_posts 下自动生成了一个和 md 文件同名的目录（也可以自己手动创建），这就是用于存放与文章有关的图片文件夹。 在markdown文件中插入图片是只需要图片名就可，但是在本地无法看到图片。 hexo更改背景及更改字体颜色1、更改背景 找到博客目录下/themes/next/source/css_custom/custom.styl，在末尾加上以下代码： 1234567body&#123; background:url(/images/bg.jpg); background-size:cover; background-repeat:no-repeat; background-attachment:fixed; background-position:center;&#125; 其中要更换的背景图片放入/image文件夹，命名为bg.jpg。 background-size属性，可以根据实际需求设置图片比例。 其中url里也可以换成图床地址，如果用Github作为图床，要将地址中的blob换成raw。 2、更换menu字体颜色 找到博客目录下theme\next\source\css_common\components\header\site-meta.styl文件，在.brand{}里添加以下代码： 1color: #ffb6c1 !important; #ffb6c1为颜色RGB代码 颜色RGB代码查询 同时改上下的两条线的颜色还需要打开themes\next\source\css_schemes\Mist_header.styl，更改i{}中background： 1background: #ffb6c1 !important; 找到博客目录下theme\next\source\css_common\components\header\menu.styl文件，在a{}里面添加以下代码： 1color: #ffb6c1 !important; 3、更改侧边栏字体颜色 找到博客目录下\themes\next\source\css_custom\custom.styl，加上以下代码： 123#sidebar &#123; p,span,a &#123;color: #ffb6c1;&#125;&#125; 侧边栏换其它图片作为背景，可以添加参数： 1234567#sidebar &#123; background:url(/images/background.png); background-size: cover; background-position:center; background-repeat:no-repeat; p,span,a &#123;color: #ffb6c1;&#125;&#125; hexo插入音频/视频1、插入音频 方法一：使用iframe标签 比如从网易云音乐网页版生成外链播放器获得外链，复制HTML代码，在markdown文档中想要插入的位置添加。(有的歌版权保护，无法使用该方法) auto=0表示不自动播放。 具体参考此文 方法二：加载相关JS文件 首先安装插件：npm install hexo-tag-aplayer 以next主题为例，编辑：\themes\next\layout\_partials目录下的header.swig文件，引入Aplayer.js。添加如下代码： 123&lt;link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css"&gt;&lt;script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"&gt;&lt;/script&gt; 编辑同目录下的footer.swig，引入Aplayer.js。添加如下代码： 1&lt;script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"&gt;&lt;/script&gt; 在文章中想要添加歌曲的位置使用如下代码： 1&lt;div class="aplayer" data-id="22773511" data-server="netease" data-type="song" data-mode="single"&gt;&lt;/div&gt; 其中各参数含义如下： 主要参数 值 data-id 歌曲/歌单 ID data-server netease(网易云音乐)，tencent(qq音乐)，kugou(酷狗)，xiami(虾米) data-type song(歌曲)，album(专辑)，playlist(歌单)，search(搜索) data-mode random(随机)，single(单曲)，circulation(列表循环)，order(列表) data-autoplay false(手动播放)，true(自动播放) 2、插入视频 通用方法： 直接把视频文件放到资源文件夹中，与markdown同名文件，然后在插入位置直接写HTML代码： 123&lt;video width="100%" height="400" src="movie.mp4" controls="controls"&gt;The `&lt;video&gt;` tag is not supported by your browser.&lt;/video&gt; 其中src=&quot;movie.mp4&quot;文视频文件名。 插入YouTube视频： 在 YouTube 视频的下方的“共享”中，选择嵌入即可获取类似这样的链接： 1&lt;iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7gJ8mGFjeqA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt; 使用embed标签插入视频： 在优酷，腾讯等网站，找到分享按钮，复制类似如下的HTML代码：/embed开头的代码，直接粘贴在markdown需要插入的位置。 也可以使用iframe形式。 使用hexo插件插入视频： 与aplayer类似，首先安装：npm install hexo-tag-dplayer --save 找到视频源，复制shipping链接地址。 具体参考此文 侧边栏嵌入网易云音乐生成外链接后，复制HTML代码。 在根目录下的/themes/next/layout/_macro/sidebar.swig文件（侧边栏布局文件）中选择要添加的播放器的位置，然后粘贴外连接。 hexo博客插入天气插件登陆中国天气网 选择自定义插件—&gt;自定义样式——&gt;生成代码，然后会生成这样一段代码: 12345&lt;!-- jiangxj --&gt;&lt;script type="text/javascript"&gt;WIDGET = &#123;FID: 'nZ3is4SF7P'&#125;&lt;/script&gt;&lt;script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"&gt;&lt;/script&gt; 将代码放置在：/themes/next/layout/_custom/header.swig hexo主页文章添加阴影修改themes/next/source/css/_custom/custom.styl，添加以下内容” 12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125; 其中202, 203, 203表示RGB三个通道灰度值，可以参考此处修改。 blog居中调整(基于Next主题)此处使用的是Mist场景。 在themes\next\source\css\ _schemes\Mist\ _posts-expanded.styl这个文件里面进行相关的修改。 1、标题居中： 1.post-title, 改成 123.post-title&#123; text-align: center;&#125; 2、详情居中： 详情 即为 “发表于 | 分类于 | 阅读次数” 等 ，而在.post-meta中text-align: site-meta-text-align所以需要修改 ​site-meta-text-align为center。在\themes\next\source\css\ _variables\Mist.styl文件中找到 ​site-meta-text-align = left; 并修改为 ​site-meta-text-align = center。 3、标签居中： 将.post-tags里面的 text-align: left; 改成text-align: center;。 修改代码区主题hexo站点_config.yml修改： 12345highlight: enable: true line_number: true auto_number: true tab_replace: 文字自动检测默认不启动，改成true使其起作用。 主题_config.yml修改： 1234567highlight_theme: normal#有如下几种方式：#normal#night#night eighties#night blue#night bright 5.1后加入了新的方式，在custom.styl中修改。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗攻击论文笔记整理]]></title>
    <url>%2Fblog%2F2019%2F04%2F20%2F%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB_%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Paper1_The Limitations of Deep Learning in Adversarial Settings本文主要介绍了一种新的对抗样本攻击方法，利用输入特征到输出值之间的对抗性显著图，达到只需要修改少量的输入值即可误分类的目的，属于targeted攻击。 摘要深度学习利用大型数据集和计算效率高的训练算法，在各种机器学习任务中胜过其他方法。然而，深层神经网络训练阶段的不完善使它们容易受到敌对样本的攻击:对手精心设计的输入，目的是造成深层神经网络分类错误。在此工作中，我们将针对深度神经网络(DNNs)的对手空间形式化，并在精确理解输入和输出之间映射关系的基础上，引入一类新的算法来构造对抗性样本。 简介深度学习容易受到对抗样本的攻击，本文作者介绍了一种针对任何非循环前馈神经网络的攻击方法-JSMA(Jacobian Saliency Map)。该方法利用雅可比矩阵，计算从输入到输出的显著图，只需要修改一小部分的输入特征就能达到改变输出结构的目的。 文章首先对深度学习的攻击模型进行了详细的分类，包括攻击目标和攻击能力；接着详细介绍了雅可比显著图的攻击样本生成方法，主要利用了雅克比矩阵和显著图等概念；最后，通过在MNIST数据集上进行评估测试，不仅验证了攻击方法的成功，而且提出了一系列度量攻击难易程度的metrics，提出了可能的防御方法。 深度学习攻击模型的分类论文详细介绍了深度学习系统攻击模型的目标与能力，并给出了相对应的参考文献。 攻击目标深度学习应用系统中，模型的完整性(intergrity)非常重要，与模型的预测结果的准确性相关。深度学习的攻击模型主要攻击深度学习模型的完整性。具体来说，深度学习的攻击的就是努力找到一个输入样本$X^*$，从而导致分类结果出错。本文考虑以下四种影响模型完整性的攻击目标： Confidence Reduction: 降低分类输出结果的置信度值，因此会引入分类歧义; Misclassification: 将分类结果修改为除了原始标签外的其他任何标签值; Targeted Misclassification: 产生输入，使得该输入的分类结果是一个特定的目标类; Source/Target Misclassification: 针对一个原始样本和目标类，产生一个扰动量，使得当扰动量加到原始样本中，其分类结果是特定的目标类。 攻击能力攻击者通常伴随着不同的攻击能力。对抗样本攻击发生在模型的预测阶段（非训练阶段，攻击时模型已经训练完成），根据攻击者的获得信息的不同，按照攻击能力的大体递减顺序，分为以下五类： Training Data &amp; Network Architecture: 这是最强的攻击，可以同时模拟网络的结构和分析训练集数据。 Network Architecture: 这种攻击了解深度学习网络结构及其参数，包括网络层数、激活函数、层间权重、偏差等。本文的攻击假设属于此类。 Training Data: 攻击者可以收集替代数据集，这些替代数据集可以从用于训练DNN的原始数据的分布中采样。一种典型的攻击方法是攻击者使用太呆数据集来训练深度学习网络，用来近似原始的深度学习网络结构。 Oracle: 攻击者可以把神经网络当作一个“Oracle”。攻击者可以通过提供输入样本得到对应的输出预测值。原则上，攻击者通过观察输入和输出间的区别，从而自适应地构造对抗样本。 Samples: 攻击者可以有能力收集神经网络分类器的输入和对应的输出，但不同的是，攻击者不能通过修改输入来观察输出的差异。这是最差的攻击能力。 JSMA攻击方法本文针对的是非循环前馈(acyclic feedforward)DNN网络，设计的一种新的基于雅可比显著图(Jacobian Saliency Map)的攻击样本生成方法。该方法需要计算前向导数，以便于构建一个对抗性显著图，从而确定输入特征子空间与攻击目标之间的关联性。通过这种方法扰动输入空间的特征值，可以很快达到使得攻击者误分类的目的。 直观理解假设考虑一个简单的一层神经网络，通过在训练集上训练，得到布尔AND函数，即输入样本$X=\{x_1,x_2\}$，输出为$F(X) = x_1 ∩ x_2$。当输入样本$X \in [0,1]^2$时，整个神经网络学习得到的函数$F$如下左图所示，输入值在0（蓝色）和1（黄色）之间有非常明显的断层。 进一步计算函数$F$对输入值每个特征$\{x_1,x_2\}$的前向导数，即为： $J_F(X) = \left [ \frac{\partial F(X)}{\partial x_1},\frac{\partial F(X)}{\partial x_2}\right]$ 前向倒数的示意图如上右图所示。直观上，前向导数越大的地方越容易构建对抗样本。如右图中$X=(1,0.37)$和$X^* =(1,0.43)$两点。距离非常近$\delta_{x_2} = 0.05$，但是两者在神经网络函数的输出完全不同。 由此可以看出： 输入值极小的扰动可以致使神经网络输出的不同； 输入域不同region中找到对抗样本的难易程度不同； 前向导数可以减少对抗样本的搜索范围。 推广到一般的神经网络限定在非循环的DNN，且其中每个激活函数都是可微分的。构造对抗样本的伪代码： 构造算法的输入分别是： 正常样本$X$，目标标签$Y^{*}$，非循环DNN输出$F$，最大的扰动量参数为$\gamma$，特征变量参数$\theta$。 算法返回值为对抗样本$X^{*}$。 算法的主要步骤如下： 计算前向导数$J_F(X^{*})$ 基于前向导数构造显著图$S$ 利用参数$\theta$修改输入特征$i_{max}$ 详细步骤如下： step1.计算DNN的前向导数 计算公式为： $\frac{\partial F_j(X)}{\partial x_i} = \left( W_{n+1,j}· \frac{\partial H_n}{\partial x_i}\right) \times \frac{f_{n+1,j}}{\partial x_i} (W_{n+1,j} ·H_n +b_{n+1,j})$ 其中$\frac{\partial H_n}{\partial x_i}$可以通过递归方式求解，其余参数已知。因此对于深度学习模型$F$和给定输入$X$而言，前向导数矩阵为： $J_F(x) = \frac{\partial F(X)}{\partial X} = \left[\frac{\partial F_j(X)}{\partial x_i} \right]_{i \in 1,\dots,M;j \in 1,\dots,N}$ step2.计算对抗性显著图 用来表示输入空间中哪些特征对输出结构影响最大。为了达到将对抗样本分为$t$类的目的，$F_t(X)$的概率必须不断增大，且当$j \neq t$时，$F_j(X)$应该不断减小，直到$$t= \rm arg \max_j F_j(X)$$文章给出了一种显著图的计算方式：$$\begin{equation}S(X,t)[i]=\begin{cases}0: \rm if \frac{\partial F_t(X)}{\partial X_i}0\\\left(\frac{\partial F_t(X)}{\partial X_i}\right)\left| \sum\limits_{j\neq t} \frac{\partial F_j(X)}{\partial X_i}\right|: \mbox{otherwise}\end{cases}\end{equation}$$其中$i$表示输入空间的第$i$个特征，$J_{ij}(x)$表示前向导数矩阵的元素。通过计算显著图，显著图数值$ S(X,t)[i]$ 越大，表示会增加分类器将其分为对应target类的概率或者减少分类为其他类别的概率。 step3.样本修改 通过对抗性显著图的确定需要修改的输入特征后，对应的特征(像素点)修改值，文章引入参数$\theta$表示输入特征的修改量。在MNIST实验中，参数$\theta = +1$。 MNIST数据集上的实验结果实验主要从三方面评估JSMA攻击方法的效果: 是否可以利用JSMA攻击任何样本？ 如何确定样本被攻击的难易程度? 与DNN相比，人类是否可以正确认知JSMA攻击样本？ 问题一： 作者分别从MNIST数据中training、validation和Testing数据中选择10000个样本，每个样本产生对应的9个targeted对抗样本，从而总共产生270000个对抗样本。实验结果如下图所示： 实验表明，JSMA攻击对任何样本能够以97%以上的成功概率产生对抗样本，并且修改的输入特征平均百分比在4%左右。意味着，一个样本784个像素中平均只要32个像素点被修改就能达到生成对抗样本的目的。 问题二： 作者定义了Hardness measure $H(source,target)$来度量原类别source被扰动成target类的难易程度；定义Adversarial distance $A(X,target)$表示样本X到类别 target之间的距离，A距离越大表示越难将X误分类成target类。 问题三： 作者利用Mechanical Turk在线众包系统，分析人类判断JSMA对抗样本的结果。试验发现，人类不能发现无法发现JSMA对抗样本中添加的扰动，也就是满足了对抗样本“被DNN误分类，但是能被人类正确分类”的第二个条件。 原文链接 Paper2_Explaining and harnessing adversarial examples这篇文章主要提出与之前论文不同的线性假设来解释对抗样本的存在性。同时，论文提出了一种简单的对抗样本生成方法-FGSM，并且再利用该攻击方法产生的对抗样本进行对抗训练。这篇文章主要说明的对抗样本的三个方面：1.存在性、2.攻击方法、3.防御方法。 摘要一些机器学习模型，包括神经网络，总是对来自数据集的反例输入进行错误的分类，这些反例输入是通过对数据集中的示例应用小的但有意为之的最坏情况的扰动而形成的，这样，扰动的输入导致模型以很高的可信度输出错误的答案。早期对这一现象的解释主要集中在非线性和过拟合上。相反，我们认为神经网络易受对抗性扰动的主要原因是它们的线性特性。 这种解释得到了新的定量结果的支持，同时给出了关于它们最有趣的事实的第一个解释:它们在体系结构和训练集之间的泛化。此外，该视图提供了一种生成对抗性示例的简单而快速的方法。利用该方法为对抗训练提供了实例，减少了MNIST数据集上maxout网络的测试集误差。 介绍当前机器学习已经被广泛应用在日常生活各个领域，但是Szegedy等人于2014年首先发现当前的机器学习模型包括神经网络等模型容易受到对抗样本(Adversarial Examples)的攻击。所谓对抗样本，即攻击者通过轻微地扰动正常样本产生对抗样本，在且保证该攻击不影响人眼的识别的情况下，达到误导分类器的目的。 在当前的研究中，对抗样本的原因产生的原因仍是一个谜。之前很多假设推测对抗样本的产生是因为深度神经网络的极度非线性，可能还结合了监督学习中正则化和模型均化不足等原因。但是本文的作者认为，这种非线性(Nonlinear)的推测解释没有必要，高维空间的线性(Linear Behavior)足够产生对抗样本。根据这个观点，作者设计了一种新的快速产生对抗样本的方法，并且使得对抗学习(Adversarial Training)更实用。这种对抗学习方法提供除了传统正则化方法(dropout, pre-training, model averaging等)外另外一种”正则化方法”。 对抗样本的线性解释因为样本输入特征(input feature)的精度有限(一般图像的每个像素是8bits, 样本中所有低于1/255的信息都会被丢弃)，所以当样本 $x$ 中每个元素值添加的扰动值 $\eta$ 小于样本输入特征精度时，分类器无法将样本$ x$ 和对抗样本 $\tilde{x} = x + \eta$ 区分开。也就是对一个分类良好的分类器而言，如果$ \epsilon $是一个足够小以至于被舍弃掉的值，那么只要$ ||\eta||_{\infty} &lt; \epsilon$ ，分类器将认为 $x$ 和$ \tilde{x} $属于同一个类。 下面考虑权重向量$ \omega^{T} $和对抗样本 $\tilde{x} $的点积为$\omega^{T}\tilde{x} =\omega^{T}(x +\eta) = \omega^{T}x + \omega^{T}\eta$。可以看出，对抗扰动使得activation增加了 $ \omega^{T}\eta$，作者提出让$ \eta = sign(\omega) $从而使$ \omega^{T}\eta$ 最大化。假设权重向量 $\omega$有$ n $个维度，且权重向量中元素的平均量值是$ m$，那么activation将增加$\epsilon_{ m n} (\Rightarrow\omega^{T}\eta \le n\times m \times \epsilon )$。虽然$||\eta||_{\infty} $不会随着维度$n$的变化而变化，但是由$\eta$导致的activation的增加量$\epsilon_{ m n}$会随着维度$n$线性增长。那么对于一个高维度的问题，一个样本中大量维度的无限小的干扰加在一起就可以对输出造成很大的变化。 所以对抗样本的线性解释表明，对线性模型而言，如果其输入样本有足够大的维度，那么线性模型也容易受到对抗样本的攻击。 非线性模型的线性扰动作者利用对抗样本的线性解释提出了一个快速产生对抗样本的方式，也即Fast Gradient Sign Method(FGSM)方法。 假设模型的参数值为$\theta$，模型的输入是$ x， y $是模型对应的label值， $J(\theta,x,y)$是训练神经网络的损失函数。对某个特定的模型参数 $\theta$ 而言，FGSM方法将损失函数近似线性化，从而获得保证无穷范数限制的最优的扰动(即$||\eta||_{\infty} &lt; \epsilon$ )，扰动值具体为： $ \eta = \epsilon · \rm sgn(\nabla_xJ(\theta,x,y))$ FGSM攻击算法在ImageNet数据上的展示如下图所示： 实验表明： FGSM这种简单的算法确实可以产生误分类的对抗样本，从而证明了作者假设的对抗样本的产生原因是由于模型的线性特性。同时，这种算法也可作为一种加速对抗训练的方法。 线性模型的对抗训练考虑最简单的逻辑回归模型上运用FGSM方法，理解如何在一个简单的设置中生成对抗样本。 假设要训练一个逻辑回归的模型来识别标签$y \in \{-1,1\}$，预测函数为： $P(y=1) = \sigma(w^Tx+b)$ 即：$P(y=-1) = 1-\sigma(w^Tx+b)$。其中$\sigma$表示sigmoid函数。样本的损失函数为： $\mathbb{E}_{x,y \sim p_{data}} \varsigma(-y(w^Tx+b))$ 其中：$\varsigma(z) = \log (1+\exp(z))$是softplus函数。 对该模型使用FSGM方法，扰动量为： $\eta = \epsilon· \rm sgn(\nabla_xJ(\theta,x,y))\ = \epsilon· \rm sgn(\nabla_x\varsigma(-y(w^Tx+b)))\\ = \epsilon · \rm sgn(-w^T * \sigma(-(w^Tx+b))) \\ = \epsilon · \rm sgn(-w) \\ = - \epsilon · \rm sgn(w)$ 且$\rm w ^T \rm sgn(w) = ||w||_1$。逻辑回归的对抗形式为： $\mathbb{E}_{x,y \sim p_{data}} \varsigma(-y(w^T\tilde{x}+b)) = \mathbb{E}_{x,y \sim p_{data}} \varsigma(y(\epsilon||w||_1-w^Tx-b))$ 其中： $\tilde{x} = x+ \eta$ $\eta = \epsilon -sgn(w)$ 深度网络的对抗训练相比于线性模型，深度网络至少可以在训练网络过程中来抵御对抗扰动攻击。文章给一种利用FGSM进行对抗训练的方法： $\tilde{J}(\theta,x,y) = \alpha J(\theta,x,y)+(1-\alpha)J(\theta,x+\epsilon \rm sign(\nabla_xJ(\theta,x,y)))$ 这种对抗训练的方法意味着在训练过程中不断更新对抗样本，从而使得当前模型可以抵御对抗样本。但是作者表示在训练集上对抗训练的错误率error rate没有达到过0%，作者主要从以下两个方面解决： 增大模型，即使用1600个unit代替240个unit 在validation set上也使用early stopping算法 文章表明，在不进行对抗训练的情况下，模型识别FGSM攻击方法生成样本的错误率是89.4%，但是通过对抗训练，同样的模型识别对抗样本的错误率下降到17.9%。 对抗样本泛化原因对抗样本具有Transferability。具体来说，在一个特定模型上产生的对抗样本通常也容易被其他模型误分类，即使这些模型的结构不同或者模型在不同的训练集上训练。甚至，不同的模型对对抗样本误分类的结果相同！作者表明，非线性或者过拟合的假设不能解释上述的现象，即， 为什么拥有无限能力的极度非线性模型会以相同的方式标注数据分布点？ 在本文提出的线性解释下，作者认为对抗样本在广泛的子空间存在。 上图表明，在不同的 $\epsilon$ 下，可以看到FGSM可以在一维的连续子空间内产生对抗样本，而不是特定的区域。这就解释了为什么对抗样本特别多，以及对抗样本transferability存在的原因。 另外，为了解释为什么不同的分类器将对抗样本误分类到同一个类，作者假设目前的方法训练神经网络都类似于在同一个训练集上学习的线性分类器。由于机器学习算法的泛化能力，所以线性分类器可以在训练集的不同子集上训练出大致相同的分类权重。底层分类权重的稳定性反过来又会导致对抗样本中的稳定性。 对抗样本存在性的其他假设这一部分，作者通过实验及分析，反驳了其他两种对抗样本存在性的假设。 假设1：生成训练可以在训练过程中提供更多的限制，或者是的模型学习如何分辨”real”或者”fake”的数据，并且对”real”的数据更加自信。 文章表明，某些生成训练并不能达到假设的效果，但是不否认可能有其他形式的生成模型可以抵御攻击，但是确定的是生成训练的本身并不足够。 假设2：对抗样本存在于单个奇怪的模型(models with strange quirks)，因此多个模型的平均可以使得模型防御性更好。 文章通过实验说明，模型融合对于对抗样本的防御能力非常有限。 原文链接 Paper3_Practical Black-Box Attacks Against Machine Learning本文提供了一个生成对抗样本的黑箱攻击方式，以前的对抗样本的生成是基于白盒的，即攻击必须要知道受攻击的模型的详细信息（包括模型结构参数，训练样本集等），但在实际应用中，这种理想的条件是几乎不存在的，攻击者几乎不可能的到模型的详细信息。本文不需要知道这些信息，只需要可以接触到受攻击模型的判别label即可完成对抗攻击。论文的作者提出了一种基于黑盒攻击的方式，训练一个跟想要攻击的目标模型完成同样任务的替代模型，基于当前的模型去生成对抗样本，这些对抗样本最终被用于攻击原目标模型。 摘要机器学习模型，如DNNs，很容易受到反面例子的攻击：恶意的输入被修改以产生错误的输出，而在人类观察者看来没有被修改。潜在的攻击包括恶意软件等恶意内容识别为合法或控制车辆的问题。 现在所有描述对抗示例攻击都需要了解内部模式或其训练数据。我们将介绍攻击者在不了解此类知识的情况下控制远程托管的DNN的第一个实际演示。实际上，我们的黑盒对手的唯一能力是观察DNN给所选输入的标签。我们的攻击策略包括训练一个局部模型来代替目标DNN，使用由目标DNN综合生成并标记的输入。我们使用局部替代的方法来构造对抗性的例子，发现它们被目标DNN错误分类。 为了执行一个真实的、完全盲的评估，我们攻击了一个由MetaMind(一个在线深度学习API)托管的DNN。我们发现，他们的DNN错误分类了84.24%的与我们的替代品制作的对抗性的例子。我们使用逻辑回归替代方法对Amazon和谷歌承载的模型进行相同的攻击，从而演示了我们的策略对许多ML技术的一般适用性。 攻击限制在不知道受攻击模型的网络结构，参数设置以及训练样本的情况下，来生成对抗样本使得受攻击的模型误判。 以往大多数的方法是针对模型，猜测探索模型的误判方向，找到更容易产生偏差的方向，在这个方向上做扰动，从而达到迷惑模型的目的。现在除了知道接受到的输入打的标签，无法基于梯度的方法，有针对的探索模型特性。 基本思路无法接触到受攻击模型的信息，采用生成替代模型，来模拟近似受攻击模型的某些决策边界。替代模型不是用来学习判别最优的，而是学习能够模仿受攻击模型的决策边界的替代能力。 替代模型训练 训练一个替代模型$F$来近似原始模型oracle O。存在以下问题： 在不知道原始模型的结构信息的条件下为替代模型选择结构 为了保证这个方法容易处理，需要限制向原始模型询问(输入输出)的次数，因为全部遍历真实输入是不可能的，query太多容易被封。 作者给出一种合成数据生成技术(synthetic data generation technique)，这个方法是基于雅可比行列式对数据集进行扩充。 样本合成Jacobian-Based Dataset Augmentation: 雅可比矩阵$\frac{\partial F(x,\theta)}{\partial x}$描述了输出$F$对输入$x$的敏感程度，即很小的输入会导致很大的输出值变化幅度。这些方向上，更能描述模型的分类边界的样本情况。如下图，梯度和边界样本： 根据前面的一批合成样本，来生成后续的样本集，想要更有效的合成数据集，就要强化这些方向上的数据合成（即基于雅可比矩阵的数据集合成）。 新样本合成如下： $S: = \{x+ \lambda · sgn(J_F[\tilde{O}(\vec{x})]):\vec{x} \in S_{\rho}\} \cup S_{\rho}$ 其中：$\tilde{O}(\vec{x})$表示$x$在受攻击模型下的label，$J_F= \frac{\partial F}{\partial x_i}$。$\lambda$是扩充的参数，定义由雅可比矩阵确定的方向上采取的步长大小。 合成新数据多少条，就要query多少次受攻击模型。 利用合成数据训练替代模型，学习到原始模型的决策边界，然后利用已经学习了分类边界识别能力的模型做迁移对抗。 整个训练流程如下： Initial Collection 收集输入数据: 攻击者收集代表输入域（input domain）的非常小的集合S_0。例如，原始模型用于分类手写体数字，则攻击者对从0-9的每个数字收集10张图片，并且，这个集合的分布不必与目标模型的训练集分布一致。 Architecture Selection 选择模型的结构: 为替代模型F选择一个结构。值得一提的是，模型的类型，层数，大小对攻击的成功与否影响很小。（当然前提是替代模型是work的，总不能说拿一个准确率30%的模型去替代一个准确率95%的模型） Substitute Training 训练替代模型: 迭代的去训练替代模型F_ρ来提高其准确率（ρ是迭代轮数） Labeling 标数据：将初始的替代输入集合Sρ输入到模型O中，并将输出的结果作为样本的标签 Training 训练模型F：用打好标签的替代训练集训练模型F_ρ Augmentation 数据集扩充：用之前提到的数据集扩充技术对当前的数据集$S_ρ$进行扩充，得到$S_ρ+1$，这个新的集合能够更好的代表模型的决策边界。用集合S_ρ+1重复上述的Labeling和Training过程。 整个过程被重复多次，以提高$F$的准确率并让其决策边界和模型$O$更相似。 对抗样本生成根据正常的样本$x$生成对抗样本$x^{}$，使得模型误判$F(x^{}) = y^{*} \neq y$ 对抗生成方法很多，大多数是基于梯度的，本文作者提及了两种方法。 （1）GoodFellow方法(Fast Gradient Sign Method) $\delta_x = \epsilon·sgn\left[ \frac{\partial Cost(F,x,y)}{\partial x}\right] \rightarrow x^{*} = x+ \delta_x$ 主要想法是：代价函数在输入的某方向上变化率很大，那么也是最容易使得代价变化的方向，代价函数是用来指导优化的(这是以正确率为目标的)，也即是最容易使得误判发生。 （2）Papernot方法：适合指定误导类别的对抗生成 将样本$x$误导为类别:$$t= \rm arg \max_j F_j(X),t \neq label(x)$$定义显著图（saliency map）：$$\begin{equation}S(X,t)[i]=\begin{cases}0: \rm if \frac{\partial F_t(X)}{\partial X_i}0\\\left(\frac{\partial F_t(X)}{\partial X_i}\right)\left| \sum\limits_{j\neq t} \frac{\partial F_j(X)}{\partial X_i}\right|: \mbox{otherwise}\end{cases}\end{equation}$$选择$\frac{\partial F_t(X)}{\partial X_i}0$的输入维度，做扰动更新来生成对抗样本。 主要想法是：雅可比矩阵$\frac{\partial F(x,\theta)}{\partial x}$表明了函数$F(x,\theta)$对$x$的敏感度，其绝对值越大的点，越更可能是分类的临界点，因此，根据雅可比矩阵值，可以缩小查找输入扰动有效点的范围。 这两种方法：GoodFellow的算法生成快速但是干扰大，易被发现；Papernot算法干扰小但是计算代价大。在该论文中选择了选择FGSM（即Goodfellow et al. algorithm）。 黑盒攻击MetaMind作者在MetaMind网站上注册了改模型的API并上传了MNIST的50,000个样本作为训练集。作者强调整个过程都是自动的，他们无法得到训练的算法，模型结构和参数。 在实验的过程中，作者选取了两组替代训练集： MNIST subset: 作者在MNIST数据集中选取了150个样本，并且这些样本和输入到MetaMind中的样本都不一样。 Handcrafted set: 为了保证结果不是因为替代模型的训练样本和原始模型的训练样本很相似，作者还手工制作了另一组初始替代训练集——用笔记本触控板手工制作了100个样本（数字有10个样本）。 然后，将它们调整为28x28灰度像素的MNIST格式。 一些样本如下所示： 模型$F$的训练结果： 经过6轮训练，两个模型分别在MNIST数据集和Handcraft数据集上使得原始模型错误分类的成功率达到了81.2%和67%。 在不同的$\epsilon$参数【FSGM方法，扰动量为：$ \eta = \epsilon· \rm sgn(\nabla_xJ(\theta,x,y))$】下，基于两个替代模型生成的对抗样本让原始模型错误分类的成功率和对抗样本的可移植性如下图所示： 原文链接 Paper4_A Two-Pronged Defense against Adversarial Examples人工智能逐渐成为攻击者的目标。其中非常重要的一类攻击称为“对抗型攻击”，即攻击者通过精心地轻微扰动正常样本产生对抗样本，该攻击样本并不影响人眼识别，但是却可以导致学习系统的误分类，从而达到攻击深度学习系统的目的。在本文中，作者总结了当前针对神经网络分类的攻击与防御方法，发现当前防御方法都依赖对抗样本或者对抗样本的生成过程，泛化能力非常差。针对这一问题，文章提出了一种与攻击无关的（attack-independent）的防御框架，不依赖于对抗样本及其生成过程，而仅仅利用数据本身的特征。 深度学习中的“对抗样本”深度学习是当前最热的一个研究方向，在很多领域都表现出非常好的效果，比如图像识别、自然语言处理，自动驾驶，人机交互等。但是，当前很多研究研究表明，通过向深度学习系统中输入精心构造的数据，很容易达到攻击的目的。具体来说，攻击者通过精心地轻微扰动正常样本产生对抗样本，该攻击样本并不影响人眼识别，但是却可以导致学习系统的误分类，从而达到攻击深度学习系统的目的。 对抗样本 V.S. 正常样本 正常样本 对分类任务来说，正常样本$x$是“自然出现的”样本。也就是说，分类任务的物理过程是以不可忽略不计的概率生成正常样本$x$。 不可忽略不计(non-negligible)对应的是可忽略不计(negligible) ；可忽略不计的意义: 一个以可忽略不计的概率发生的事件几乎不可能发生，即使将这个实验重复进行多项式次数。 举例来说，如果分类任务针对的是手写数字，那么数据的生成过程几乎不可能生成老虎的图像。 对抗样本 对分类任务来说，对抗样本$y$是”非自然出现的“样本，并且分类器对样本$y$的判断结果不同于一般人的判断。 接下来，作者正式形式化地定义了对抗样本、防御方法及其评估标准。 形式化定义： 对抗样本、防御方法及其评估标准 首先定义如下集合： $S$: 样本空间内所有样本的集合; $C_t$: 对分类任务$t$的互斥类别的集合，例如如果$t$是手写字体分类模型，那么: $C=\{0,1,2,\dots,9\}$ $N_t$:$N_t = \{x| x \in S 并且x是针对分类任务t的自然产生的\}$ 每个分类任务$t$都假定有一个数据产生过程，该过程中每个样本$x \in S$都是以概率函数$p(x)$生成的。如果该概率函数$p(x)$是不可忽略不计的(non-negaligible)，那么生成的$x$则是在分类任务$t$下自然产生的。通常研究认为：$N_t$构建的manifold的维度远远低于$S$。这是因为我们无法知道数据的真实生成过程，所以通常对$t$的自然数据集的联合来近似表示$N_t$。比如应用在图像识别的CIFAR和MNIST数据集。 定义1: 分类任务$t$的一个分类器表示为函数 $f_t: S \rightarrow C_t$; 定义2: 分类任务$t$的ground-truth分类器，表示的是一般人的判断标准。该ground-truth分类器可以表示为函数$g_t: S \rightarrow C_t \cup \{ \bot\} $，其中$\{\bot\}$对输入$x$的不同于任务$t$数据生成过程的其它分类判定，换言之，$\{\bot\}$表示针对对抗样本的分类判定结果。 定义3: 对分类任务$t$和分类器$f_t$而言的一个对抗样本$x$需要满足以下两个条件： （1）$f_t(x) \neq g_t(x)$ （2）$x \in S/N_t$差集 第一个条件表明，对对抗样本来说分类器的判断结果通常会出错。但这个并非唯一原因，因为任何的分类器都不是完美的，分类器对正常样本也会产生误差；第二个条件将对抗样本限定为仅仅由攻击者人为产生的用来欺骗分类器的样本。 定义4: 针对分类器$f_t$中对抗样本的防御方法可以表示为函数$d_{f_t}: S \rightarrow C_t \cup\{\bot\}$ 那么，如何评估防御方法的效果呢？防御的目标是用来提升分类器在对抗样本上而非正常样本上的准确率。根据该想法，作者提出如下评估标准。 定义5: 如果以下任意一种情况满足，则表明防御方法$d_{f_t}$对样本$x$做出正确判断: (1) 当$x$是正常样本时，$d_{f_t}$于ground-truth分类器$g_t$的判断结果一致，即对任意$x \in N_t$，有$d_{f_t}(x) = g_t(x)$; (2) 当$x$是对抗样本时，或者有$d_{f_t}$直接判断$x$为对抗样本，或者$d_{f_t}$于$g_t$的判断结果一致，即对任意$x\in S/N_t$，有$d_{f_t} = \{\bot\}$或者$d_{f_t}(x) = g_t(x)$。 深度学习的攻击与防御在本文中，作者主要总结了神经网络分类方法的攻击方法，即寻找符合条件且最合适的“对抗样本”，并且表明，所有的攻击最终都转化为优化问题的求解。 常见攻击类型：产生对抗样本的方法 攻击类型1: Fast gradient sign method (FGSM) 对抗样本$x’$采用迭代求解方法：$x’=x+ \epsilon·sgn(\nabla_x Loss(x,l_x))$，其中$Loss(x,l_x)$函数表示将样本$x$分类成$l_x$的代价，并将其代价最大化。 攻击类型2: Iterative gradient sign method 该攻击是对FGSM攻击的改进，确保每一步更新的图像与原图像的距离在$\epsilon $范围内，可以增加FGSM的攻击成功率，即： $x_{i+1}’ = clip_{\epsilon ,x}(x_i’ + \alpha ·sgn((\nabla_x Loss(x,l_x)))$。 攻击类型3: DeepFool 该攻击的基本思路是：在图像空间中，找到距离正常样本$x$最接近的决策边界，然后跨过该边界去欺骗分类器，通常使用线性迭代的优化方法。 攻击类型4: Carlini Attack 该攻击是当前最有效且最小扰动的产生对抗样本的方法，可以用如下优化问题来表示: $\min\limits_{\delta} ||\delta||_2 + c \cdot f( x + \delta), 使得 x + \delta \in [0,1]^n$ 防御方法总结 作者根据当前常见的针对对抗样本的防御方法，总结了如下三种常见的防御方法。 防御方法1: Adversarial training 对抗训练 一个非常直接的想法是在训练过程中包含对抗样本的信息，从而构建一个鲁棒性更好的分类器，该方法即称为对抗训练方法。该方法直观上有效，但是有个非常严重的问题是，对抗训练很难推断出攻击者的攻击方式，即对抗训练无法得知攻击者产生的对抗样本的模式、每种对抗样本的比重，从而很难在对抗训练中使用。 防御方法2: Detecting adversarial example 该方法主要是训练一个分类器来区分对抗样本与正常样本，与对抗训练方法不同的是：对抗训练只是改善了原有分类器的正确性，而该方法是额外增加一个分类器。 防御方法3: Defensive distillation 该方法在训练过程中通过各种方式隐藏梯度信息，从而使得攻击很难通过梯度求解方法攻击目标训练器，但是该防御方法很容易被各种攻击技巧绕过。 然而这些防御方法都有局限性。方法1和方法2都需要对抗样本来训练防御方法，因此这些防御方法都是特定于产生这些对抗样本的过程，而对其他对抗样本不具有泛化能力；于方法3而言，Carlini等人表明distillation 方法并不能增加神经网络的鲁棒性，而且这种防御方法需要改变并重新训练目标分类器，增加了工程上的复杂性。 “对抗样本”及本文的解决思路 误分类的原因 —— 根据深度学习的流形假设(Manifold Hypothesis)，对大多数的AI任务来说，样本空间是一个非常高维的空间，但是我们所能掌握的有效样本其实在一个维度远远低于高维度样本空间的一个流行(Manifold)空间内。 因此，作者总结了一个分类器错分类对抗样本的原因，有如下两条： 对抗样本远远地远离该任务的流行Manifold的边界。比如说，如果该任务是手写数字图像分类，那么不含有数字的其他图像都是对抗样本。因为分类器必须要对该图像分类，从而产生一个类标签，所以一定会导致错误分类； 对抗样本非常接近于该任务的流行边界，如果此时该分类方法的泛化能力不好，那么也会发生误分类。 本文的解决思路 —— 本文中，作者设计实现了一个双管齐下的防御系统，针对上面两个误分类的原因，提出了对应的Detector和Reformer方法。针对第一个原因，Detector用来判断一个样本是否远远远离Manifold边界，如果是的话直接删除；接着针对第二个原因，对于一个样例，努力找到一个接近或者就在该任务Manifold之上的样例，其中是的近似表示，然后再交由分类器。 MagNet防御方案在本文中，作者提出了一个用于防御深度学习分类任务中对抗样本攻击的框架: MagNet。MagNet中包括一个或者多个Detector模块和一个Reformer模块，详细如下图所示。 DetectorDetector是一个判断样本是否是对抗样本的函数，即$d: S\rightarrow \{0,1\}$事实上，针对这个问题，当前主要做法是训练一个分类器来区分正常样本和对抗样本。但是因为这类方法都需要对抗样本数据集或者需要了解对抗样本的生成过程，对其他类型的对抗样本没有泛化能力，所以这类方法非常容易被稍微改动的攻击绕过。本文中，作者提出了两种新的检测对抗样本的方法：基于重构误差的检测和基于概率分歧的检测方法。 1. 基于重构误差（reconstruction error）的Detector 为了避免在检测的过程中需要对抗样本的数据集，Detector中的检测方法是通过对正常样本建模，估算对抗样本与正常样本Manifold边界的距离，从而达到检测对抗样本的目的。在本文中，作者称这类方法为基于重构误差的检测方法，其具体实现中主要利用的是Autoencoder。 如下图2所示，一个Autoencoder可以表示为:$ae = d \circ e$ , 主要包括两部分: 一个encoder $e: S \rightarrow H$ 和 一个decoder$d: H \rightarrow S$其中$S$是输出空间，$H$是隐藏空间。 对于所有正常样本的训练集合，作者训练了一个autoencoder使得这个训练集的损失函数最小，其损失函数定义为： $L(X_{train}) = \frac{1}{X_{train}} \sum_{x \in X_{train}} ||x - ae(x)||_2$ 一旦训练得到一个autoencoder的模型，那么对于任意待测试的样本$x$，其重构误差为: $E(x) = ||x - ae(x)||_p$ 作者认为如果待测试的样本是正常样本，因为该样本与autoencoder的训练数据的生成过程相同，所以其重构误差很小，反之，如果带检测的样本是对抗样本，那么该样本的重构误差会很大。所以，在本文中作者用重构误差来衡量待检测样本与正常样本Manifold间的距离。但是，由于该距离是连续值，所以在本文中作者还利用验证集确定一个阈值 $r_{re}$，用来判断是否为对抗样本。 2.基于概率分歧/发散（probability divergence）的Detector 基于重构误差的Detector的适用于当样本的误差很大的时候，那么如果重构误差很小的时候呢？在本文中作者的解决方式是利用目标分类器来进一步检测。 大多数的神经网络分类器在最后一层实现softmax函数，即: $softmax(l)_i = \frac{exp(l_i)}{\sum_{j=1}^n exp(l_j)}$ 本文定义 $f(x)$ 为神经网络softmax层对于输入 $x$ 的输出值， $ae(x)$ 为autoencoder $ae$ 接受输入 $x$ 时候产生的输出值，为输入 $x$ 先经过autoencoder然后再softmax函数后的输出值。在本文中作者认为，如果$x$是正常样本，$ae(x)$一般与$x$非常接近，那么$f(x)$与$f(ae(x))$的概率函数则非常接近；如果$x’$是对抗样本，$ae(x’)$一般与$x’$差别非常大，尽管两者的重构误差可能很小，$f(x)$与$f(ae(x))$的概率函数的差别也会很大。 因此，作者利用 $f(x)$与$f(ae(x))$间的分歧来判断样本$x$是否为正常样本，分歧程度用Jensen-Shannon散度来衡量： $JSD(P||Q) = \frac{1}{2} D_{KL}(P||M) +\frac{1}{2} D_{KL}(Q||M)$其中 $D_{KL}(P||Q) = \sum_i P(i)log\frac{P(i)}{Q(i)}$，且 $M = \frac{1}{2}(P+Q)$ ReformerReformer函数 $r: S \rightarrow N_t$用来重构测试输入样本，理想的reformer需要满足如下两个条件： 不应该改变正常样本的分类结构； 应该充分重构对抗样本，使得重构后的样本接近于正常样本。 基于上述思路，作者主要利用autoencoder来实现Reformer。通过在正常样本上训练得到一个autoencoder，然后利用该autoencoder，不论是对抗样本还是正常样本，都会输出一个于正常样本Manifold接近的样本，从而达到了理想reformer的要求。本文中作者展示了Reformer的效果如图3所示。 MagNet的实验评估文章中，作者主要基于MINST和CIFAR-10数据集，实验评估了MagNet防御方法针对常见的四种攻击(FGSM, Iterative, DeepFool和Carlini’s)的防御效果。另外，针对这四种攻击模型，作者总结了三种攻击场景： 黑盒攻击：攻击者不知道防御方法的任何参数、神经网络结构、训练集等； 灰盒攻击：攻击了除了不知道防御方法的参数外，其他一切信息都了解； 白盒攻击：攻击者了解防御方法的一切参数、神经网络结构、训练集等。 作者表明，针对白盒攻击，当前几乎没有任何防御方法，MagNet的防御效果也不理想。所以在本文中，作者主要展示了MagNet在黑盒攻击和灰盒攻击中的防御效果。 MagNet对黑盒攻击的性能 针对MINST和CIFAR-10数据集，文章显示了MagNet防御方法对黑盒攻击的防御效果，具体如下图4与图5所示。 以最后一行为例，上述两图表明使用的Carlini攻击对两个数据集都能达到100%攻击成功效果（0%的分类成功），但是在MagNet的防御下，该分类准确度分别为92%和77.5%，表明攻击成功率只有8%和22.5%。所以，黑盒攻击下，MagNet的防御效果非常好！ 如何防御灰盒攻击？ 在灰盒攻击攻击下，攻击者不知道防御的具体参数，但是知道防御的具体结构、训练集合等，这样会极大地影响MagNet防御方法的效果。在本文中，作者认为即使我们不能完全消除对抗样本，我们也可能增加攻击者发现对抗样本的难度，从而增强训练器的鲁棒性。 本文利用了密码学中随机性的思想，在防御过程中创建了不同的autoencoder作为候选的Detector和Reformer。MagNet对每一个场景、每一个测试集、甚至每个测试样本都随机选择一个autoencoder。因此，可以通过增加autoencoder集合的大小和多样性，从而使得攻击更难，到达更好的防御效果。图6显示了针对CIFAR-10数据集Carlini攻击产生的对抗样本在MagNet防御下的分类正确性。 对角线上数据表明，当Carlini攻击中的autoencoder与MagNet在测试过程中使用的autoencoder相同时，MagNet的分类准确性大部分下降到0。但是当这两个autoencoder不同时，分类的准确度一般都在90%以上。最后一行表明，当攻击者在训练过程中随机选择一个autoencoder，并且MagNet在测试过程中也随机酸则一个autoencoder时候，MagNet的分类准确度保持在80%以上。 总结本文提出一个针对神经网络中“对抗样本”的防御框架 — MagNet。MagNet框架中使用两种方法处理神经网络中的不可信的输入数据。该框架中首先使用Detector来检测所有样本中扰动量大的对抗样本，直接删除，然后针对扰动量小的对抗样本，努力找到一个接近正常样本的Manifold上的对应样本。这两种方法双管齐下共同提高神经网络的分类准确度。此外，通过使用autoencoder作为detector检测方法，MagNet在不需要任何对抗样本或者对抗样本产生过程知识的情况下达到检测对抗样本的目的，该检测方法具有更强的泛化能力。实验表明，MagNet可以有效地防御当前最新的对抗样本攻击手段。在攻击者知道MagNet训练样本的情况下，作者介绍了一个新的灰盒攻击模型，并利用随机化多样性来有效地抵御这种攻击。 另外，文章表明针对对抗样本的防御方法应该是attack-independent的。也就是说，我们不能从特定的生成过程找到对抗样本的属性，而应该是通过在所有对抗样本的生成过程中找到内在的共同特性来使得防御方法的泛化能力更强。作者同时表明，MagNet知识解决该问题第一步，并证明了其良好的性能。 对抗攻击论文总结综述机器学习的安全性《Can machine learning be secure?》 对机器学习安全性的影响可以通过两方面：一是在学习过程中调节训练数据《Poisoning attacks against support vector machines》，二是在预测过程中操控输入数据《In Machine Learning and Knowledge Discovery in Databases）。 Attack对抗问题最先由2013《Evasion attacks against machine learning at test time》提出，对抗攻击（Adversarial Attacks）的提出2014《Intriguing properties of neural networks》提出。 对抗攻击产生的原因，分为两个方面：深度神经网络模型的非线性加上不充分的模型平均和不充分的正则化导致的过拟合；《Explaining And Harnessing Adversarial Examples》提到高维空间中的线性性就足以造成对抗样本，深度模型对对抗样本的无力最主要的还是由于其线性部分的存在。 针对对抗攻击在黑盒攻击中的应用《Practical black-box attacks against deep learning systems using adversarial examples》，（增强学习中的黑盒攻击）《Adversarial Attacks on Neural Network Policies》，《Universal adversarial perturbations》中的构造样本方法也能在黑盒前提下发挥作用。通常的黑盒攻击都是在对抗攻击的transferability特性的情况下发挥作用，《Machine Learning as an Adversarial Service:Learning Black-Box Adversarial Examples》提出了直接采用对抗样本的方法进行黑盒攻击。 对抗攻击的transferability特性《Intriguing properties of neural networks》（多种神经网络结构），《Adversarial Attacks on Neural Network Policies》（在多种增强学习策略下）， 构建对抗样本的算法（优化算法）：（围绕两个问题：difficult to find new methods that are both effective in jeopardizing a model and computationally affordable）1.传统的梯度下降，牛顿法，BFGS，L-BFGS2.Jacobian saliency map attack (JSMA) ：《The limitations of deep learning in adversarial settings2.5 FGSM：《Explaining And Harnessing Adversarial Examples》iterative version of FGSM：《Adversarial examples in the physical world》(smaller perturbation)3.RP2： 《Robust Physical-World Attacks on Machine Learning Models》4.Papernot Method：《 Adversarial perturbations against deep neural networks for malware classification》5.Universal Perturbations （extend of DeepFool method）：《Analysis of universal adversarial perturbations》《Universal adversarial perturbations》（思考：能否根据universal原理将神经网络中的输入都加上同一个扰动，让对应的模型分类效果更好，从而得出置信度更高的分类结果）6.DeepFool：《Deepfool: a simple and accurate method to fool deep neural networks. 》the first method to compute and apply the minimal perturbation necessary for misclassification under the L2 norm.（the approximation is more accurate than FGSM and faster than JSMA）（still computationally expensive）7.《Towards evaluating the robustness of neural networks》（The authors cast the formulation of Szegedy et al. into a more efficient optimization problem, which allows them to craft efficient adversarial samples with low distortion.）（also very expensive）\8. Virtual adversarial examples：《Virtual adversarial training: a regularization method for supervised and semi-supervised learning》 对抗攻击应用在物理目标上：（面部识别）《Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition》，（拍照图片）《Adversarial examples in the physical world》，（路标）《Robust Physical-World Attacks on Machine Learning Models》，（自动汽车）《Concrete Problems for Autonomous Vehicle Safety:Advantages of Bayesian Deep Learning》，（恶意软件分类）《Adversarial Perturbations Against Deep Neural Networks for Malware Classification》 在增强学习上的应用《Vulnerability of deep reinforcement learning to policy induction attacks.》，《Adversarial Attacks on Neural Network Policies》 增强攻击效果的方法：《Adversarial Attacks on Image Recognition》提到可以通过PCA降维处理数据 Defencedefence的两个方向： 《Adversarial Attacks on Neural Network Policies》（作为future work提到）\1. 将对抗样本加入到训练集中。即是可以手动生成对抗样本，并加入到训练集中。（但是生成对抗样本的代价比较大）\2. 在测试模型时增加一个探测对抗输入的模块，判断输入是否有对抗攻击Defence的方法： Adversarial Training（augmenting the training data with perturbed examples）：《Intriguing properties of neural networks》（either feeding a model with both true and adversarial examples or learning it using the following modified objective function:J ˆ(θ, x, y) = αJ(θ, x, y) + (1 − α)J(θ, x + ∆x, y)） Defensive distillation：《Distillation as a defense to adversarial perturbations against deep neural networks》–hardens the model in two steps: first, a classification model is trained and its softmax layer is smoothed by division with a constant T ; then, a second model is trained using the same inputs, but instead of feeding it the original labels, the probability vectors from the last layer of the first model are used as soft targets. （《Adversarial perturbations of deep neural networks》对这种方法进行了改动，只需要一步即可构成攻击） Feature squeezing：《Feature squeezing: Detecting adversarial examples in deep neural networks》《Feature squeezing mitigates and detects carlini/wagner adversarial examples》 Detection systems:performe statistical tests:《On the (statistical) detection of adversarial examples》use an additional model for detection:《Adversarial and clean data are not twins》《On detecting adversarial perturbations》apply dropout at test time:《Detecting adversarial samples from artifacts》 PCA whitening ：《Early methods for detecting adversarial images》 研究点： 各种攻击方式的优化 defence策略的构建 针对各种特定应用场景的对抗 利用攻击和defence策略优化现有模型结构，增强机器学习模型的效果 探究对抗攻击深层原理，理解其背后的数学本质，实际上就是理解深度神经网络的工作原理以及对抗攻击为什么能有作用。思考：对抗攻击的问题，好像神经网络泛化性能的限制问题，因为增加的很小的扰动就能让模型错误率比较高，说明模型的泛化能力不够强，针对其他样本的效果也不是很好，因此，增强神经网络模型的泛化能力也是defence策略的一个方向，尤其是在黑盒攻击方面 意义 (citing from ) Able to handle massive volumes of data Works at machine speed to thwart attacks Does not rely on signatures Can stop known and unknown malware Stops malware pre-execution Higher detection retes, lower false positives]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>对抗攻击</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet and DenseNet网络模型]]></title>
    <url>%2Fblog%2F2019%2F04%2F20%2FResNet%20and%20DenseNet%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[ResNet深度残差网络（Deep residual network） 参考文献1：deep residual learning for image recognition 网络深度增加，可以进行更加复杂的特征模式的提取，但会出现深度网络的退化问题（Degradation problem）：网络深度增加时，网络准确度出现饱和，甚至下降。56层网络比20层网络效果要差，这不是过拟合问题，56层网络训练误差同样高。 深层网络存在着梯度消失或者爆炸问题，使得深度学习模型很难训练。 残差学习现在有一个浅层的网络，想通过向上堆积新层来建立深层网络，一个极端情况是这些新加的层什么都不学习，仅仅复制浅层网络的特征，即这样的新层是恒等映射（identity mapping）。这样，深层网络至少和浅层网络一样，不应该出现退化现象。 Kaiming He提出用残差学习来解决退化问题。对于一个堆积层结构（几层堆积而成），当输入为$x$时其学习到的特征记作$H(x)$。学习残差$F(x) = H(x) -x$，即原始的学习特征为$F(x) +x$。 残差的学习相比原始特征直接学习更加容易，当残差为0时，此时堆积层仅仅做了恒等映射，至少网络的性能不会下降，实际上残差不会为0，这会使得堆积层在输入特征基础上学习新的特征，而拥有 更好的性能。 残差学习的结构如图所示：（类似电路短路，是一种短路连接 （shortcut connection）） 残差学习相对更容易，直观上是残差学习需要学习的内容少，残差一般较小，学习难度小。残差单元可以表示为：$$y_l = h(x_l) + F(x_l,W_l)$$ $$x_{l+1} = f(y_l)$$ 其中，$x_l$和$x_{l+1}$分别表示的是第$l$个残差单元的输入和输出，每一个残差单元一般包含多层结构。 $F$是残差函数，表示学习到的残差 $h(x_l) = x_l$表示恒等映射 $f$表示ReLU激活函数。 简要推导： $x_L = x_l + \sum\limits_{i=l}^{L-1} F(x_i,W_i)$，表示从浅层$l$到深层$L$的学习特征。 利用链式规则，可以求得反向过程的梯度：$$\frac{\partial Loss}{\partial x_l} = \frac{\partial Loss}{\partial x_L}·\frac{\partial Loss}{\partial x_l}= \frac{\partial Loss}{\partial x_L}·\left( 1+ \frac{\partial}{\partial x_L}\sum\limits_{i=l}^{L-1}F(x_i,W_i) \right)$$式中，$\frac{\partial Loss}{\partial x_L}$表示损失函数到达$L$的梯度，小括号中的1表明短路机制可以无损地传播梯度，另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。 残差梯度不会那么巧全为-1，而且比较小，有1的存在不会导致梯度消失，残差学习更容易。 ResNet网络结构参考的VGG19网络，在其基础上修改，并通过短路机制加入残差单元。 主要变化是在 ResNet直接采用stride=2的卷积做下采样，并用global average pool层替换全连接层。 ResNet的一个重要设计原则：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。 ResNet相比普通网络每两层间增加了短路机制，形成了残差学习，其中虚线表示feature map数量发生了改变。 从表中可见，18-layer-ResNet和34-layer-ResNet，其进行的是两层间的残差学习，当网络更深时，其进行的三层间的残差学习，三层的卷积核分别是1×1，3×3和1×1。 其中，隐含层的feature map的数量是比较小的，并且是输出feature map数量的1/4。 残差单元ResNet使用的是两种残差单元。 分别对应浅层网络（左图）和深层网络（右图）。 对于短路连接，当输入和输出维度不一致时，可以直接将输入加到输出上，但是当维度不一致时（对应维度增加一倍），不能直接相加。 有两种策略：1、采用zero-padding增加维度，此时一般要先做一个downsamp，可以采用stride=2的pooling，这样不会增加参数； 2、采用新的映射（projection shortcut），一般采用1×1的卷积，这样会增加参数，也会增加计算量。 短路连接除了可以直接使用恒等映射，也可以采用projection shortcut。 改进后的残差单元及效果 改进的前后一个明显变化是采用pre-activation，BN和ReLU都提前了，推荐短路连接采用恒等变换，保证短路连接不会有阻碍。[^参考文献2] [^参考文献2]:identity mappings in deep residual networks ResNet的Tensorflow实现Github地址 DenseNet模型ResNet可以训练出更深的CNN模型，从而实现更高的准确度，其核心是通过建立前面层与后面层之间的“短路连接”（shortcuts,skip connection），有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。 DenseNet的基本思路与ResNet一致，但是建立的是前面所有层与后面层的密集连接（dense connection）。 DenseNet的另一特点是：通过特征在channel上的连接来实现特征重用（feature reuse）。使得DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能。 DenseNet提出的密集连接机制：互相连接所有的层。每个层都会接受其前面的所有层作为其额外的输入。 ResNet的短路连接方式是通过元素级相加。每个层与前面的某层（一般2-3层）短路连接在一起。 DenseNet的每个层都会与前面所有层在channel维度上连接（concat）在一起，各个层的特征图大小是相同的，并且作为下一层的输入。 对于一个L层的网络，DenseNet共包含$\frac{L(L+1)}{2}$个连接，是一种密集连接。此外，DenseNet是直接concat来自不同的层的特征图，实现特征重用。 传统的网络在$l$层的输出为：$$x_l = H_l(x_l-1)$$对于ResNet，增加了来自上一层输入的identity函数：$$x_l = H_l(x_l-1) +x_{l-1}$$在DesNet中，会连接前面所有层作为输入：$$x_l = H_l([x_0,x_1,\dots,x_{l-1}])$$其中$H_l(·)$代表非线性转化函数（non-linear transformation），是一个组合操作，可能包括一系列的BN（Batch Normalization），ReLU，Pooling及Conv操作，$l$和$l-1$层之间可能包括多个卷积层。 上图可见，$h_3$的输入不仅包括来自$h_2$的$x_2$，还包括前面两层的$x_1$和$x_0$，它们是在channel维度上连接在一起的。 CNN网络一般要经过Pooling或者stride&gt;1的Conv来降低特征图的大小，DenseNet的密集连接方式需要特征图大小保持一致。 解决方法是：使用DenseNet +Transition结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。 使用Pytorch实现DenseNet DenseNet的优势主要体现在以下几个方面： 由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“deep supervision”； 参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的； 由于特征复用，最后的分类器使用了低级特征。 要注意的一点是，如果实现方式不当的话，DenseNet可能耗费很多GPU显存，一种高效的实现如图10所示，更多细节可以见这篇论文Memory-Efficient Implementation of DenseNets。不过我们下面使用Pytorch框架可以自动实现这种优化。 参考资料]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web前端简介]]></title>
    <url>%2Fblog%2F2019%2F04%2F20%2Fweb%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[WEB前端学习路线 web前端学习路线 HTML基础 基础技术：HTML、CSS、JavaScript语言。 1、HTML是网页内容的载体。内容就是网页制作者放在页面上想要让用户浏览的信息，可以包含文字、图片、视频等。 2、CSS样式是表现。网页的外衣。如标题字体、颜色变化，或为标题加入背景图片、边框等。所有这些用来改变内容外观的东西称之为表现。 3、JavaScript是用来实现网页上的特效效果。如：鼠标滑过弹出下拉菜单，或鼠标滑过表格的背景颜色改变，焦点新闻图片的轮换，有动画、交互的一般都是用JavaScript来实现的。 HTML是一种超文本标记语言（markup language），是一套标记标签（markup tag）。 HTML使用标记标签来描述网页。 HTML的标签是由尖括号包围的关键词，如&lt;html&gt; HTML的标签通常是成对出现的，第一个是开始标签，第二个是结束标签，中间是标签内容。 标签与标签之间是可以嵌套的，但先后顺序必须保持一致，如： 1&lt;div&gt;&lt;p&gt;内容 。 &lt;/p&gt;&lt;/div&gt; HTML文件固定结构1234&lt;html&gt; 根标签，所有的网页标签都在其中&lt;head&gt;…&lt;/head&gt; 用于定义文档的头部，是所有头部元素的容器。 &lt;body&gt;…&lt;/body&gt; 之间的内容是网页的主要内容&lt;/html&gt; 代码讲解： &lt;html&gt;&lt;/html&gt;称为根标签，所有的网页标签都在&lt;html&gt;&lt;/html&gt;中。 &lt;head&gt; 标签用于定义文档的头部，它是所有头部元素的容器。头部元素有​&lt;title&gt;​、​&lt;script&gt;、&lt;style&gt;、&lt;link&gt;、 &lt;meta&gt;等标签，头部标签在之后会详细介绍。 在&lt;body&gt;和&lt;/body&gt;标签之间的内容是网页的主要内容，如&lt;h1&gt;、&lt;p&gt;、&lt;a&gt;、&lt;img&gt;等网页内容标签，在这里的标签中的内容会在浏览器中显示出来。 在head中设置网页标题和字符集编码，即： 1234&lt;head&gt; &lt;title&gt;这里是标题&lt;/title&gt; &lt;meta charset = "utf-8"/&gt;&lt;/head&gt; HTML段落标签想在网页上显示文章，需要&lt;p&gt;标签，把文章的段落放在&lt;p&gt;标签中。其默认样式，段前和段后都会有空白。 123&lt;p&gt; 段落&lt;/p&gt; HTML换行标签若想要在不产生一个新段落的情况下进行换行（新行），使用&lt;br/&gt;标签。 123&lt;p&gt; this is &lt;br/&gt; a para &lt;br/&gt; graph with line breaks&lt;/p&gt; HTML标题一共有6个，h1,h2,h3……h6。分别为一级标题至六级标题。并且依据重要性递减，&lt;h1&gt;是最高等级。 1&lt;hx&gt; 标题文本 &lt;/hx&gt; (x=1-6) HTML水平线使用&lt;hr/&gt;标签在HTML页面创建，用于分隔内容。 123456789101112131415&lt;p&gt; this is a paragraph&lt;/p&gt;&lt;hr/&gt;&lt;p&gt; this is a paragraph&lt;/p&gt;&lt;hr/&gt;&lt;p&gt; this is a paragraph&lt;/p&gt; HTML注释1&lt;!--注释文字 --&gt; HTML的&lt;span&gt;标签 为结合CSS设置单独的样式用，本身无任何效果。 HTML列表标签（1）无序列表 使用ul-li标签来完成，没有前后顺序的信息列表。 123456&lt;ul&gt; &lt;li&gt;……&lt;/li&gt; &lt;li&gt;……&lt;/li&gt; …… &lt;/ul&gt; （2）有序列表 使用&lt;ol&gt;标签制作有序列表来展示。 12345&lt;ol&gt; &lt;li&gt;信息&lt;/li&gt; &lt;li&gt;信息&lt;/li&gt; ……&lt;/ol&gt; HTML表格123456789101112131415161718192021222324252627282930&lt;table&gt; &lt;caption&gt;标题文本&lt;/caption&gt; 表格标题，显示在表格上方 &lt;tr&gt; &lt;th&gt;产品名称&lt;/th&gt; &lt;th&gt;品牌&lt;/th&gt; &lt;th&gt;库存量&lt;/th&gt; &lt;th&gt;入库时间&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 耳机&lt;/td&gt; &lt;td&gt;联想&lt;/td&gt; &lt;td&gt;500&lt;/td&gt; &lt;td&gt;2013-1-2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;u盘&lt;/td&gt; &lt;td&gt;金士顿&lt;/td&gt; &lt;td&gt;120&lt;/td&gt; &lt;td&gt;2013-8-10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;u盘&lt;/td&gt; &lt;td&gt;金士顿&lt;/td&gt; &lt;td&gt;120&lt;/td&gt; &lt;td&gt;2013-8-10&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 创建表格的四个元素： table、tr、th、td 1、&lt;table&gt;…&lt;/table&gt;：整个表格以&lt;table&gt;标记开始、&lt;/table&gt;标记结束。 2、&lt;tr&gt;…&lt;/tr&gt;：表格的一行，所以有几对tr 表格就有几行。 3、&lt;td&gt;…&lt;/td&gt;：表格的一个单元格，一行中包含几对&lt;td&gt;...&lt;/td&gt;，说明一行中就有几列。 4、&lt;th&gt;…&lt;/th&gt;：表格的头部的一个单元格，表格表头。 5、表格中列的个数，取决于一行中数据单元格的个数 6、合并行使用rowspan属性，合并列使用colspan属性。 HTML超链接使用&lt;a&gt;标签可实现超链接。 1&lt;a href="目标网址" title="鼠标滑过显示的文本"&gt;链接显示的文本 &lt;/a&gt; 如： 1&lt;a href="http://www.google.com" title="点击进入谷歌"&gt;click here! &lt;/a&gt; 单击click here！文字，网页链接到 http://www.google.com title的属性的作用是：鼠标滑过链接文字时会显示这个属性的文本内容，这个属性方便搜索引擎了解链接地址的内容（语义化更友好）。 &lt;a&gt;标签是在默认情况下，链接的网页是在当前浏览器窗口中打开，若需要在新的浏览器窗口中打开，有： 1&lt;a href = "目标网址" target= "_blank"&gt;click here!&lt;/a&gt; HTML图片使用&lt;img&gt;标签插入图片。 1&lt;img src = "图片地址" alt = "下载失败时的替换文本" title = "提示文本"/&gt; 1&lt;img src = "myimage.gif" alt = "my image" title = "my image"/&gt; 1、src：标识图像的位置； 2、alt：指定图像的描述性文本，当图像不可见时（下载不成功时），可看到该属性指定的文本； 3、title：提供在图像可见时对图像的描述(鼠标滑过图片时显示的文本)； 4、图像可以是GIF，PNG，JPEG格式的图像文件。 HTML表单网站和用户之间使用表单（form）进行交互，表单可以把浏览器输入的数据传送到服务器端，服务器端程序就可以处理表单传过来的数据。 123&lt;form method="传送方式" action="服务器文件"&gt; &lt;/form&gt; 1.&lt;form&gt; ：&lt;form&gt;标签是成对出现的，以&lt;form&gt;开始，以&lt;/form&gt;结束。 2.action ：浏览者输入的数据被传送到的地方,比如一个PHP页面(save.php)。 3.method ： 数据传送的方式（get/post）。 123456&lt;form&gt; method="post" action="save.php" &lt;label for="username"&gt;用户名：&lt;/label&gt; &lt;input type="text" name="username" /&gt; &lt;label for ="pass"&gt;密码：&lt;/label&gt; &lt;input type="password" name="pass" /&gt;&lt;/form&gt; 注意: 1、所有表单控件（文本框、文本域、按钮、单选框、复选框等）都必须放在 &lt;form&gt;&lt;/form&gt;标签之间（否则用户输入的信息可提交不到服务器上哦！）。 2、method : post/get 表单元素-文本、密码框123&lt;form&gt; &lt;input type="text/password" name="名称" value="文本" /&gt;&lt;/form&gt; 1、type： 当type=”text“时，输入框为文本输入框; 当type=”password“时, 输入框为密码输入框。 2、name：为文本框命名，以备后台程序ASP 、PHP使用。 3、value：为文本输入框设置默认值。(一般起到提示作用) 如： 1234567&lt;form&gt; 姓名： &lt;input type="text" name="myname" &gt; &lt;br/&gt; 密码： &lt;input type="password" name="pass" &gt;&lt;/form&gt; 表单元素-多行文本框用于在表单中输入大段文字时，需要用到文本输入域。 123&lt;textarea rows="行数" cols="列数"&gt;在这里输入文本……&lt;/textarea&gt; 1、&lt;textarea&gt;标签是成对出现的，以&lt;textarea&gt;开始，以&lt;/textarea&gt;结束。 2、cols ：多行输入域的列数。 3、rows ：多行输入域的行数。 4、在&lt;textarea&gt;&lt;/textarea&gt;标签之间可以输入默认值。 举例： 1234567&lt;form method="post" action="save.php"&gt; &lt;label&gt;联系我们&lt;/label&gt; &lt;textarea rows="10" cols="10"&gt; 在这里输入文本内容…… &lt;/textarea&gt;&lt;/form&gt; 表单元素-选择框HTML有两种选择框，单选框和复选框。 1&lt;input type="radio/checkbox" value="值" name="名称" checked="checked" /&gt; 1、type: 当 type=”radio” 时，控件为单选框 当 type=”checkbox” 时，控件为复选框 2、value：提交数据到服务器的值（后台程序PHP使用） 3、name：为控件命名，以备后台程序 ASP、PHP 使用 4、checked：当设置 checked=”checked” 时，该选项被默认选中 注意：同一组的单选按钮，name取值一定要一致，这样同一组的单选按钮才可以起到单选的作用。 在浏览器中显示为： 表单元素-下拉选择框节省网页空间，可以单选或多选。 1、value &lt;option value =&#39;提交值&#39; &gt;选项&lt;/option&gt; 其中提交值是向服务器提交的值，选项是显示的值。 2、selected = “selected”：表示该选项就被默认选中。 表单元素-按钮有包括：提交按钮、重置。 123&lt;input type = "submit" value ="提交"&gt;type :只有当type值设置为submit时，按钮才有提交作用value:按钮上显示的文字 HTML-div网页制作过程中，可以把一些独立的逻辑部分划分出来，放在一个&lt;div&gt;标签中，这个标签相当于一个容器。 123&lt;div&gt; ……&lt;/div&gt; 逻辑部分是页面上相互关联的一组元素，如网页中独立的栏目版块，图中用红色边框标出的部分就是一个逻辑部分。使用&lt;div&gt;标签作为一个容器。 几大主流的前端框架（UI/JS）框架 主流的前端框架]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
</search>
